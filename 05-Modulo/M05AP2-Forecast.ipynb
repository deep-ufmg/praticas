{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S05A01_2 - Forecast.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sk3TpLb8DhT",
        "colab_type": "text"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports básicos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heM-v_e475r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Basic imports.\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xONfLuDyDvSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 500,     # Number of epochs.\n",
        "    'lr': 1e-3,           # Learning rate.\n",
        "    'weight_decay': 5e-4, # L2 penalty.\n",
        "    'momentum': 0.9,      # Momentum.\n",
        "    'num_workers': 6,     # Number of workers on data loader.\n",
        "    'batch_size': 10,     # Mini-batch size.\n",
        "    'num_samples': 150,   # Number of samples generated\n",
        "    'sample_size': 500,   # Length of each sample\n",
        "    'window': 80,         # Input sequence length  ###\n",
        "    'horizon': 30,        # Output sequence length ###\n",
        "    'clip_norm': 6.0,     # Upper limit on gradient L2 norm ###\n",
        "    \n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKm1fiTZA2Wj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random initialization for weights and biases.\n",
        "def initialize_weights(*models):\n",
        "    for model in models:\n",
        "        for k, module in enumerate(model.modules()):\n",
        "            if isinstance(module, nn.RNN):\n",
        "                for name, param in module.named_parameters():\n",
        "                  if 'weight' in name:\n",
        "                    nn.init.xavier_normal_(param.data)\n",
        "                  elif 'bias' in name:\n",
        "                    nn.init.constant_(param.data, 0)\n",
        "            elif isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_normal_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "            elif isinstance(module, nn.BatchNorm2d):\n",
        "                module.weight.data.fill_(1)\n",
        "                module.bias.data.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX_VzbGqpMon",
        "colab_type": "text"
      },
      "source": [
        "# Forecast de séries temporais\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gObDnyHvRa2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Dados \n",
        "\n",
        "Na aula de hoje vamos criar um modelo de sequência na sua forma mais simples, com entradas e saídas de **tamanho fixo**.\n",
        "\n",
        "Os dados de entrada são senoides com diferentes frequências que treinarão um único modelo. O processo de treinamento terá como entrada um crop aleatório de cada sequência de treino, com o objetivo de prever os pontos num intervalo futuro definido. No dicionário de argumentos foram definidos o tamanho do crop de entrada em **```args['window']```**, e o intervalo futuro a ser predito em **```args['horizon']```**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAaQiClPU9r0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "xtime = np.arange(0, args['sample_size'])\n",
        "\n",
        "X = [ np.sin(i * xtime) for i in  np.linspace(0.05, 0.2, args['num_samples']) ]\n",
        "\n",
        "plt.figure(figsize=(16,4))\n",
        "for i in [0, args['num_samples']//2, -1]:  \n",
        "  x = X[i]\n",
        "  x = x - min(x) / (max(x) - min(x))\n",
        "  \n",
        "  plt.plot(x)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXfiIif2GuV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceData(data.Dataset):\n",
        "  \n",
        "  def __init__(self, num_samples, sample_size, window, horizon):\n",
        "    \n",
        "    self.window  = window\n",
        "    self.horizon = horizon\n",
        "    self.sample_size = sample_size\n",
        "    self.num_samples = num_samples\n",
        "    self.time = torch.arange(0, self.sample_size, dtype=torch.float)\n",
        "    \n",
        "    self.data  = [ torch.sin(i * self.time) for i in  np.linspace(0.05, 0.2, self.num_samples) ]\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    \n",
        "    seq = self.data[idx]\n",
        "    seq = (seq - min(seq)) / (max(seq) - min(seq))\n",
        "    \n",
        "    crop_idx = np.random.choice(np.arange(len(seq) - self.window - self.horizon))\n",
        "    \n",
        "    x = seq[crop_idx : crop_idx+self.window]\n",
        "    y = seq[crop_idx+self.window : crop_idx+self.window+self.horizon]\n",
        "    \n",
        "    return x.unsqueeze(-1), y\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "    \n",
        "\n",
        "# Setting datasets.\n",
        "train_data = SequenceData(args['num_samples'], args['sample_size'], args['window'], args['horizon'])\n",
        "test_data  = SequenceData(args['num_samples'], args['sample_size'], args['window'], args['horizon'])\n",
        "\n",
        "# Randomizing train/test splits\n",
        "torch.manual_seed(1) # For reproducibility\n",
        "indices = torch.randperm(len(train_data)).tolist()\n",
        "dataset = torch.utils.data.Subset(train_data, indices[:-200])\n",
        "dataset_test = torch.utils.data.Subset(test_data, indices[-200:])\n",
        "\n",
        "# Setting dataloaders.\n",
        "train_loader = DataLoader(train_data, batch_size=args['batch_size'], shuffle=True, num_workers=args['num_workers'])    \n",
        "test_loader = DataLoader(test_data, batch_size=args['batch_size'], shuffle=True, num_workers=args['num_workers'])    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDryOyGJBaJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot single sample\n",
        "for k, (inps, labs) in enumerate(train_loader):\n",
        "  if k == 3: break\n",
        "  \n",
        "  plt.figure()\n",
        "  \n",
        "  len_inp = len(inps.data.numpy()[0])\n",
        "  len_lab = len(labs.data.numpy()[0])\n",
        "  \n",
        "  plt.plot(inps.data.numpy()[0], label='Window')\n",
        "  plt.plot(np.arange(len_inp, len_inp+len_lab),  labs.data.numpy()[0], label='Horizon')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5e36FuV6D0_",
        "colab_type": "text"
      },
      "source": [
        "## Tipos de RNN\n",
        "\n",
        "Relembrando: redes recorrentes podem compor diferentes tipos de modelos. [Um post feito pelo Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) sumariza em 5 diferentes modelagens, apresentadas na imagem a seguir.\n",
        "\n",
        "- **One to One**: Redes feed forward tradicionais, com uma entrada e uma saída.\n",
        "- **One to Many**: Uma entrada e múltiplas saídas (ex: Image captioning).\n",
        "- **Many to One**: Entrada sequencial e saída única (ex: análise de sentimentos).\n",
        "- **Many to Many**: Sequence to sequence não sincronizado (ex: tradução de texto)\n",
        "- **Many to Many sync**: Sequence to sequence sincronizado, cada elemento da entrada mapeia diretamente para a saída correspondente (ex: sequence labeling).\n",
        "\n",
        "![](http://karpathy.github.io/assets/rnn/diags.jpeg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INxTK5aDHE2N",
        "colab_type": "text"
      },
      "source": [
        "## Backpropagation Through Time (BPTT)\n",
        "\n",
        "Para explicar a ideia geral da BPTT usaremos a referência [desse post](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/), altamente recomendado para mais detalhes sobre o assunto.\n",
        "\n",
        "Assuma $E = \\{E_0, E_1, ..., E_n\\}$ a sequência de erros das saídas produzidas em cada timestep $t = \\{0, 1, ..., n\\}$. O cálculo do gradiente é feito pelo acúmulo das derivadas de cada erro $E_t$ em relação aos pesos, i.e., $\\sum_t \\frac{\\partial E_t}{\\partial W_{hh}}$.  O exemplo a seguir apresenta isoladamente o cálculo de $\\frac{\\partial E_3}{\\partial W_{hh}}$, aplicando a regra da cadeia para derivar o erro $E_3$ em $t=3$ em relação a $W_{hh}$ em $t=0$.\n",
        "\n",
        "É importante compreender que modelos recorrentes são **profundos na dimensão do tempo**, de modo que o backpropagation na conexão recorrente tem alta propensão aos problemas de **vanishing** e **exploding gradient**. Diferente de redes feed-forward profundas, onde o gradiente só explode ou some nas camadas iniciais, uma única camada recorrente é profunda no tempo, e composta de uma única matriz de pesos. Portanto, **problemas com distribuições complexas e dependências de longo prazo oferecem um grande desafio até para modelos recorrentes de poucas camadas**.\n",
        "\n",
        "![](http://www.wildml.com/wp-content/uploads/2015/10/rnn-bptt-with-gradients.png)\n",
        "\n",
        "Nas aulas futuras aprenderemos como contornar o problema do vanishing gradient através do uso de unidades avançadas. Para o exploding gradient, um artifício muito comum é o uso de gradient clipping, apresentado a seguir.\n",
        "\n",
        "## Gradient Clipping\n",
        "\n",
        "O clip do gradient nada mais é do que uma norma L2 sobre o gradiente da rede. Define-se uma norma limite na função ```clip_grad_norm_``` do pytorch, que por sua vez já modifica os gradientes caso sua norma total extrapole esse limite. \n",
        "\n",
        "Documentação Pytorch: https://www.pydoc.io/pypi/torch.raspi-0.4.0/autoapi/nn/utils/clip_grad/index.html\n",
        "\n",
        "É importante chamar a função de clipping exatamente após o cálculo dos gradientes, e antes do passo de otimização, como apresentado a seguir.\n",
        "\n",
        "```python\n",
        "# Computing backpropagation.\n",
        "loss.backward()\n",
        "\n",
        "# Clipping Gradient\n",
        "torch.nn.utils.clip_grad_norm_(net.parameters(), args['clip_norm'])\n",
        "\n",
        "# Weight update\n",
        "optimizer.step()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u70akMOj_IN-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Atividade Prática\n",
        "\n",
        "Como o nosso problema possui uma saída de tamanho fixo, podemos modelá-lo como Many to One. Para isso, basta adicionar uma cada Linear, que recebe o hidden state mais significativo (o último da sequência). Essa camada irá realizar a **regressão** do intervalo futuro que queremos prever. A figura a seguir apresenta uma representação visual do que implementaremos a seguir.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1yzWthOHPX5jeEgi1OXO81fHBWdXKU6Wq)\n",
        "\n",
        "Os passos da atividade consistem em:\n",
        "\n",
        "1.   Implemente a arquitetura proposta, com uma RNN de duas camadas seguida de uma camada Linear.\n",
        "\n",
        "*  RNN  **Hidden size = 64**\n",
        "*   Defina **```batch_first=True```** na RNN, visto que a saída do Dataloader organiza o batch na forma ```(batch_size, seq_len, feat_size)```, que não é o padrão da camada RNN.\n",
        "*   A ativação não linear da RNN é definida internamente. Não há necessidade de camadas de ativação após camadas recorrentes. \n",
        "*   A saída da camada Linear corresponde ao argumento **```args['horizon']```** que define o intervalo futuro que se deseja prever.\n",
        "*   Após a camada Linear, defina uma camada de ativação **sigmóide**, visto que estamos lidando com um problema de regressão.\n",
        "\n",
        "\n",
        "2.   Implemente o forward da sua rede.\n",
        "*   Lembre-se que é necessário inicalizar o hidden state a cada forward. \n",
        "*   Atenção também para a transição entre a camada recorrente e a Linear, deve-se passar **apenas o último hidden state da sequência** que agrega toda a memória da sequência.\n",
        "\n",
        "3.   Defina o otimizador e a função de perda de regressão (sugestão: L1). \n",
        "\n",
        "4.   Modifique as funções de treino e teste, realizando experimentos sem o gradient clipping, e com diferentes hiperparâmetros de clipping (verifique dicionário de argumentos).\n",
        "*   Plote os gradientes ao longo das iterações para entender o impacto do clipping.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCWF9_ZTjwxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Implement RNN Class\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afNlL9XPCG6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting optimizer.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kes3eEF2COy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting loss.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEZYI-b7GsXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ave_grads, grad_norm, total_norm, trloss = [], [], [], []\n",
        "\n",
        "def save_grad_flow(named_parameters):\n",
        "\n",
        "    average = []\n",
        "    param_norms = []\n",
        "    \n",
        "    norm = 0.\n",
        "    for n, p in named_parameters:\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\n",
        "            average.append(p.grad.abs().mean())\n",
        "            param_norms.append(p.grad.data.norm(2))\n",
        "            \n",
        "            norm += param_norms[-1].item() ** 2\n",
        "    norm = norm ** (1. / 2)\n",
        "    \n",
        "    total_norm.append(norm)\n",
        "    grad_norm.append(param_norms)\n",
        "    ave_grads.append(average)\n",
        "\n",
        "def train(train_loader, net, criterion, optimizer, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for training mode.\n",
        "    net.train()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    train_loss = []\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "\n",
        "        # TODO: Obtaining images, labels and paths for batch.\n",
        "        \n",
        "        \n",
        "        # TODO: Casting to cuda variables.\n",
        "        \n",
        "        # TODO: Clears the gradients of optimizer.\n",
        "        \n",
        "        # TODO: Forwarding.\n",
        "        \n",
        "        # TODO: Computing loss.\n",
        "        \n",
        "        # TODO: Computing backpropagation.\n",
        "        \n",
        "        # TODO: Clipping Gradient\n",
        "        \n",
        "        # Saving gradient norm\n",
        "        save_grad_flow(net.named_parameters())\n",
        "        \n",
        "        # TODO: Weight update (optimizer step)\n",
        "        \n",
        "        # Updating lists.\n",
        "        train_loss.append(loss.data.item())\n",
        "    \n",
        "    toc = time.time()\n",
        "    \n",
        "    train_loss = np.asarray(train_loss)\n",
        "    trloss.append(train_loss.mean())\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "    if epoch % 5 == 0:\n",
        "      print('--------------------------------------------------------------------')\n",
        "      print('[epoch %d], [train loss %.4f +/- %.4f], [training time %.2f]' % (\n",
        "          epoch, train_loss.mean(), train_loss.std(), (toc - tic)))\n",
        "      print('--------------------------------------------------------------------')\n",
        "    \n",
        "def test(test_loader, net, criterion, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for evaluation mode (not computing gradients).\n",
        "    net.eval()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    test_loss = []\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(test_loader):\n",
        "\n",
        "        # TODO: Obtaining images, labels and paths for batch.\n",
        "        inps, labs = batch_data\n",
        "\n",
        "        # TODO: Casting to cuda variables.\n",
        "        \n",
        "        # TODO: Forwarding.\n",
        "        \n",
        "        # TODO: Computing loss.\n",
        "                \n",
        "        # Updating lists.\n",
        "        test_loss.append(loss.data.item())\n",
        "        lab_list.append(labs.detach().cpu().numpy())\n",
        "        \n",
        "        if i < 2:\n",
        "            plt.figure(figsize=(10,4))\n",
        "            \n",
        "            inp = inps.cpu().data.numpy()[0]\n",
        "            lab = labs.cpu().data.numpy()[0]\n",
        "            out = outs.cpu().data.numpy()[0]\n",
        "            \n",
        "            plt.plot(inp, label='Input')\n",
        "            plt.plot(np.arange(len(inp), len(inp)+len(lab)), lab, label='Label')\n",
        "            plt.plot(np.arange(len(inp), len(inp)+len(lab)), out, label='Output')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "    \n",
        "    toc = time.time()\n",
        "    plt.close('all')\n",
        "    \n",
        "    test_loss = np.asarray(test_loss)\n",
        "    tsloss.append(test_loss.mean())\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "   \n",
        "    print('********************************************************************')\n",
        "    print('[epoch %d], [test loss %.4f +/- %.4f], [testing time %.2f]' % (\n",
        "        epoch, test_loss.mean(), test_loss.std(), (toc - tic)))\n",
        "    print('********************************************************************')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1w7vE2VWHgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Iterating over epochs.\n",
        "for epoch in range(1, args['epoch_num'] + 1):\n",
        "\n",
        "    # Training function.\n",
        "    train(train_loader, net, criterion, optimizer, epoch)\n",
        "\n",
        "    # Computing test loss and metrics.\n",
        "    if epoch % 50 == 0:\n",
        "      test(test_loader, net, criterion, epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh0UVL44Ii_t",
        "colab_type": "text"
      },
      "source": [
        "## Visualizando a Norma do Gradiente\n",
        "Plot da norma do gradiente ao longo do treinamento, junto com o gráfico de convergência do modelo no treino e no teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwzPfxnkdeks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot total norm and loss convergence\n",
        "plt.figure(figsize=(20,4))\n",
        "plt.ylabel('Grad Norm')\n",
        "plt.xlabel('Iterations')\n",
        "plt.plot(total_norm)\n",
        "plt.figure(figsize=(20,4))\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.plot(trloss, label='Train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CdHwyIRdlCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot gradient norm per weight\n",
        "layers = []\n",
        "for n, p in net.named_parameters():\n",
        "  layers.append(n)\n",
        "\n",
        "fig, axs = plt.subplots(nrows=5, ncols=1, figsize=(15, 25))\n",
        "grad_norm = np.array(grad_norm)\n",
        "for i in range(len(grad_norm[0])):\n",
        "  axs[i].plot(grad_norm[:,i])\n",
        "  axs[i].set_ylabel('Grad Norm')\n",
        "  axs[i].set_xlabel('Epochs')\n",
        "  axs[i].set_title(layers[i])\n",
        "  \n",
        "  axs[i].set_ylim( np.min(grad_norm).cpu().data.numpy(), np.max(grad_norm).cpu().data.numpy()*1.1 )\n",
        "  \n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8wYKMdQJ6pA",
        "colab_type": "text"
      },
      "source": [
        "## Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTAeGIH3PeUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_vis = DataLoader(test_data, batch_size=args['batch_size'], shuffle=False, num_workers=1)    \n",
        "\n",
        "losses = []\n",
        "for inps, labs in test_vis:\n",
        "    inps = inps.to(args['device'])\n",
        "    labs = labs.to(args['device'])\n",
        "    \n",
        "    outs = net(inps)\n",
        "    \n",
        "    for b in range(0,len(inps),3):\n",
        "      inp = inps.cpu().data.numpy()[b]\n",
        "      lab = labs.cpu().data.numpy()[b]\n",
        "      out = outs.cpu().data.numpy()[b]\n",
        "\n",
        "      plt.figure(figsize=(12,4))\n",
        "      plt.plot(inp)\n",
        "      plt.plot(np.arange(len(inp), len(inp)+len(lab)), lab)\n",
        "      plt.plot(np.arange(len(inp), len(inp)+len(lab)), out)\n",
        "      plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
