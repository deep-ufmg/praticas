{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S07A01 - Seq2Seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tCy34_fj_E8",
        "colab_type": "text"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports básicos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeSdMJtajy_z",
        "colab_type": "code",
        "outputId": "d5ec9b3a-67a7-4bc5-b94f-a12f218ef1ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "# Basic imports.\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torchvision import models\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "import spacy\n",
        "! python -m spacy download en\n",
        "! python -m spacy download fr\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: fr_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.1.0/fr_core_news_sm-2.1.0.tar.gz#egg=fr_core_news_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/fr_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/fr\n",
            "You can now load the model via spacy.load('fr')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f47b221cd30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQWXOc1GkDTQ",
        "colab_type": "code",
        "outputId": "33ef7f13-af60-4ee5-a3df-47d674851d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 100,       # Number of epochs.\n",
        "    'lr': 1e-3,           # Learning rate.\n",
        "    'weight_decay': 5e-4, # L2 penalty.\n",
        "    'momentum': 0.9,      # Momentum.\n",
        "    'num_workers': 6,     # Number of workers on data loader.\n",
        "    'batch_size': 10,     # Mini-batch size.\n",
        "    'max_length': 50,    # Maximun length of predicted sentence\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHsYj_w9H6TL",
        "colab_type": "text"
      },
      "source": [
        "# Generating Sequences\n",
        "\n",
        "\n",
        "Dentre os tipos de problemas solucionáveis com modelos recorrentes, dois deles são baseados em geração de sequências: Os problemas One-to-Many,  e os Many-to-Many não sincronizados. <br>\n",
        "\n",
        "Tipicamente os modelos de geração de sequências são baseados em arquiteturas **Encoder-Decoder**, onde a entrada é codificada para uma forma fixa, e então decodificada passo a passo em uma sequência.\n",
        "\n",
        "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0K-eTs1VAso",
        "colab_type": "text"
      },
      "source": [
        "## One-to-Many: Image Captioning\n",
        "\n",
        "O problema de legendar (ou descrever) imagens, se encaixa na categoria One-to-Many, pois cada imagem é considerada uma unidade atômica, ou seja, não é modelada como uma sequência. Já a saída é uma sequência de caracteres semanticamente relacionados com a imagem. Na figura a seguir vemos uma representação superficial do problema.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=10YhOB7pvnhXUhqu08JNJ-YcgoyPp8c3K\" height=\"350\">\n",
        "\n",
        "Em termos de modelagem de solução, temos que a imagem deve ser mapeada para um espaço latente que provê um vetor de características de contexto (**context feature**).  Essa etapa consiste na codificação da sua entrada (**encoder**), destacando características semanticamente relevantes para a etapa de decodificação (**decoder**).\n",
        "\n",
        "Sendo a saída uma sequência de palavras, é comum que o decoder seja composto por camadas recorrentes, que na primeira iteração recebem como entrada:\n",
        "*  Hidden state inicial ($h_0$): context feature, ou seja, a saída produzida pelo encoder\n",
        "*  Input inicial ($x_0$): Token especial de início de sequência (**```<sos>```** - start of sequence)\n",
        "\n",
        "As iterações seguintes recebem como entrada os resultados produzidos na iteração anterior, ou seja:\n",
        "*  Hidden state inicial ($h_t$): $h_{t-1}$\n",
        "*  Input inicial ($x_t$): $y_{t-1}$\n",
        "\n",
        "O fim das iterações é determinado pela geração do token **```<eos>```** indicando o término da sequência (end of sequence). A figura a seguir apresenta uma representação visual desse pipeline. Em azul é apresentada a entrada única do decoder (```<sos>```) e em vermelho as saídas do modelo.<br><br>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1j7SUTfGIHi7XIPv8YRKF1XLTTmJfvQrd\" width=\"750\"><br><br>\n",
        "\n",
        "\n",
        "O código a seguir apresenta uma ilustração de pequeno porte de um modelo de Image Captioning, sem adição de transformações ou modelos de atenção. Para problemas do mundo real, aplicar o conceito de Atenção melhora significativemente a qualidade de modelos, mas a princípio vamos concentrar na arquitetura recorrente Encoder-Decoder. <br>\n",
        "Para quem quiser saber mais, recomendo esse tutorial de Image Captioning: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning\n",
        "\n",
        "Perceba no código a seguir a implementação de dois tipos de ```forward()```:\n",
        "* ```forward_inference()```: A forma mais direta de decodificar a entrada em uma sequência de tokens, usando a saída do timestep anterior como entrada do timestep seguinte. Requer a implementação de um **loop explícito**, diferente do forward encapsulado que vínhamos utilizando.\n",
        "* ```forward()```: Em tempo de treino, é possível realizar o forward encapsulado, alimentando a sequência target como entrada do modelo recorrente. Nesse caso o comprimento das sequências é conhecido, e o loop encapsulado pode ser interrompido ao final da sequência target. **Na prática recomenda-se fazer parte do treinamento com os targets e outra parte sem**. A figura a seguir apresenta a diferença sutil que permite o encapsulamento do forward de batches. Novamente em azul são apresentadas as entradas fornecidas ao modelo, e em vermelho as saídas. <br><br>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1XBnnnbAWx2y63JiZ9G8zBtxxll0YpV_F\" width=\"750\"> <br><br>\n",
        "\n",
        "Vale explicitar que **cada iteração no decoder é composta pelas camadas de embedding, RNN e Linear**. Em problemas que precisam fazer classifcação em múltiplas iterações, usa-se de um artifício para permitir o forward encapsulado. Dada uma saída recorrente com shape ```(seq_len, batch_size, hidden_size)```, o forward na camada linear é realizado redimensionando a saída para a forma  ```(seq_len * batch_size, hidden_size)```, de modo que a camada linear interprete como um grande batch de características. Como a dependência temporal é modelada internamente pela GRU, as amostras podem ser alimentadas individualmente para a camada Linear.\n",
        "\n",
        "```python\n",
        "outputs_rnn, hidden = self.gru(inputs, hidden)\n",
        "\n",
        "input_linear = output_rnn.view(output_rnn.size(0)*output_rnn.size(1), output_rnn.size(2))\n",
        "output = self.softmax(self.out(input_linear))\n",
        "\n",
        "```\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1eFDgd6gn_l-lcKcQIuQA7ooI_ZkvWYrp\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14vqCWN4SQ70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, output_size, max_length, dropout_p=0.1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        self.embed_size  = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Word embedding\n",
        "        self.embedding = nn.Embedding(self.output_size, self.embed_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        \n",
        "        # Recurrent feature\n",
        "        self.gru = nn.GRU(self.embed_size, self.hidden_size)\n",
        "        \n",
        "        # Classify next word\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, hidden, target=None, lengths=None):\n",
        "\n",
        "        if target is None:\n",
        "          return self.forward_inference(hidden)\n",
        "      \n",
        "        embedded = self.dropout(self.embedding(target))\n",
        "\n",
        "        packed_inputs = nn.utils.rnn.pack_padded_sequence(embedded, lengths)        \n",
        "        packed_outputs, hidden = self.gru(packed_inputs, hidden)\n",
        "        output_rnn, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
        "        \n",
        "        output = output_rnn.view(output_rnn.size(0)*output_rnn.size(1), output_rnn.size(2))\n",
        "        output = self.softmax(self.out(output))\n",
        "        output = output.view(output_rnn.size(0), output_rnn.size(1), -1)\n",
        "        \n",
        "        return output\n",
        "\n",
        "      \n",
        "    def forward_inference(self, hidden):\n",
        "      \n",
        "        # Start inference with <sos> token\n",
        "        input = torch.tensor(TEXT.vocab.stoi[\"<sos>\"]).to(args['device'])\n",
        "        \n",
        "        outputs = []\n",
        "        # Iterate to a maximum length\n",
        "        for i in range(self.max_length):\n",
        "          \n",
        "          # Forward single sample\n",
        "          embedded = self.embedding(input).view(1,1,-1)\n",
        "          output_rnn, hidden = self.gru(embedded, hidden)\n",
        "          output = self.softmax(self.out(output_rnn[0]))\n",
        "          \n",
        "          outputs.append(output.detach())\n",
        "          \n",
        "          # Current output feeds future input\n",
        "          topv, topi = output.topk(1)\n",
        "          input = topi.squeeze().detach()  \n",
        "\n",
        "          # Finish inference when <eos> generated\n",
        "          if input == TEXT.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "            \n",
        "        # Return sequence of tokens produced \n",
        "        # Either interrupted by producing <eos>\n",
        "        # Or interrupted by max_length\n",
        "        return torch.stack(outputs)\n",
        "          \n",
        "        \n",
        "      \n",
        "\n",
        "####### Build encoder ###########\n",
        "# encoder = models.resnet18(pretrained=True)\n",
        "encoder = models.resnet18()\n",
        "num_features = encoder.fc.in_features\n",
        "\n",
        "# Remove linear layers\n",
        "modules = list(encoder.children())[:-2]\n",
        "encoder = nn.Sequential(*modules)\n",
        "#################################\n",
        "\n",
        "\n",
        "####### Build decoder ###########\n",
        "vocab_size   = len(TEXT.vocab)\n",
        "embed_size   = 100\n",
        "hidden_size  = num_features\n",
        "max_length   = args['max_length'] \n",
        "\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, max_length).to(args['device'])\n",
        "#################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGMQRCmguT4A",
        "colab_type": "text"
      },
      "source": [
        "## Sequence-to-sequence models (Seq2Seq)\n",
        "\n",
        "Modelos Sequence-to-Sequence (Seq2Seq) partem do mesmo princípio do Image Captioning, porém a entrada também é sequencial, de modo que a codificação também é realizada por um modelo recorrente. \n",
        "\n",
        "A atividade de hoje é no contexto de Neural Machine Translation (NMT), cujo pipeline é representado de forma simplificada a seguir. Note que o idioma source (francês) necessita apenas do token de finalização de sentença, enquanto o idioma target precisa de ambos os inicializadores e os finalizadores (```<sos>```, ```<eos>```), visto que a entrada da rede precisa do token de inicialização, mas a saída, através da qual será calculada a loss, é produzida apenas com a finalização.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
        "\n",
        "Imagem retirada do tutorial de NMT do Pytorch: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\n",
        "Encontre modelos de linguagem pré-treinados e arquiteturas implementadas em: http://opennmt.net/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVIb6Cw3kEMr",
        "colab_type": "code",
        "outputId": "1684eedb-f367-4808-c122-63705ec74aef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        }
      },
      "source": [
        "# Baixando Dataset\n",
        "!wget https://www.dropbox.com/s/gq36ksk347d36ln/translation_data.zip\n",
        "!unzip translation_data.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-04 14:29:17--  https://www.dropbox.com/s/gq36ksk347d36ln/translation_data.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.1, 2620:100:601f:1::a27d:901\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/gq36ksk347d36ln/translation_data.zip [following]\n",
            "--2019-08-04 14:29:17--  https://www.dropbox.com/s/raw/gq36ksk347d36ln/translation_data.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc67850fb7241dcbf7a43578f5d7.dl.dropboxusercontent.com/cd/0/inline/Al8iFFgogPo_KxLMaaiD-EtddFZqTv7OjfIzlC16Wgi63CH-yiBJBn-SRUYGuLFxwtLbj4WnbFsKOAJDvRK0hHaAtS5yMWtWQbdnBedcrAL5WhTBJNMsaW1wTGT1cHZjGLQ/file# [following]\n",
            "--2019-08-04 14:29:17--  https://uc67850fb7241dcbf7a43578f5d7.dl.dropboxusercontent.com/cd/0/inline/Al8iFFgogPo_KxLMaaiD-EtddFZqTv7OjfIzlC16Wgi63CH-yiBJBn-SRUYGuLFxwtLbj4WnbFsKOAJDvRK0hHaAtS5yMWtWQbdnBedcrAL5WhTBJNMsaW1wTGT1cHZjGLQ/file\n",
            "Resolving uc67850fb7241dcbf7a43578f5d7.dl.dropboxusercontent.com (uc67850fb7241dcbf7a43578f5d7.dl.dropboxusercontent.com)... 162.125.9.6, 2620:100:601f:6::a27d:906\n",
            "Connecting to uc67850fb7241dcbf7a43578f5d7.dl.dropboxusercontent.com (uc67850fb7241dcbf7a43578f5d7.dl.dropboxusercontent.com)|162.125.9.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/Al_xsc3Oo5gT5XI4any406kNEcr1jo7wx60ZLbVqPK98lkzqSCTIkIlNwynBsNV_Upxtlk1qIWc1n1K8XTBKMR-2hUNXEWCQnJNLKZpW-Xyy6yG_u7mKr39WH7elbaJKU6W6SBOMIiFF8aiVqgRynXIAM5Uedj6owHmSjWMKhN9pxfdzQ6sPwDXGWZF8n1T-bGtT-gVZYXHBKtIya60f9LC0ESVBtr8Wsl7TWC5SY6uDZzh5PYhFoBQshhbGUtnjfzcmeKqptM5eYMykU2_vGXDYcI2wzgqdJIC741z_8PDAftDM7U-5xSErH2asr44LlNOZno2bCIoBfW1sUlzCSbsoLtLEokcUAANXpyWO2cIVWw/file [following]\n",
            "--2019-08-04 14:29:18--  https://uc67850fb7241dcbf7a43578f5d7.dl.dropboxusercontent.com/cd/0/inline2/Al_xsc3Oo5gT5XI4any406kNEcr1jo7wx60ZLbVqPK98lkzqSCTIkIlNwynBsNV_Upxtlk1qIWc1n1K8XTBKMR-2hUNXEWCQnJNLKZpW-Xyy6yG_u7mKr39WH7elbaJKU6W6SBOMIiFF8aiVqgRynXIAM5Uedj6owHmSjWMKhN9pxfdzQ6sPwDXGWZF8n1T-bGtT-gVZYXHBKtIya60f9LC0ESVBtr8Wsl7TWC5SY6uDZzh5PYhFoBQshhbGUtnjfzcmeKqptM5eYMykU2_vGXDYcI2wzgqdJIC741z_8PDAftDM7U-5xSErH2asr44LlNOZno2bCIoBfW1sUlzCSbsoLtLEokcUAANXpyWO2cIVWw/file\n",
            "Reusing existing connection to uc67850fb7241dcbf7a43578f5d7.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘translation_data.zip’\n",
            "\n",
            "translation_data.zi 100%[===================>]   2.75M  18.3MB/s    in 0.2s    \n",
            "\n",
            "2019-08-04 14:29:18 (18.3 MB/s) - ‘translation_data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  translation_data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ7knpeZs2fv",
        "colab_type": "code",
        "outputId": "ce65bf67-495c-45be-ec5e-eafd8397ade1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "# Criando CSV de treino e teste para carregar com o TabularDataset\n",
        "\n",
        "translation_path = 'data/eng-fra.txt'\n",
        "\n",
        "samples = open(translation_path).read().split('\\n')\n",
        "  \n",
        "# Write txt to csv\n",
        "lines = (line.split(\"\\t\") for line in samples)\n",
        "with open('translation_data.csv', 'w') as out_file:\n",
        "    writer = csv.writer(out_file)\n",
        "    writer.writerow(('English', 'French'))\n",
        "    writer.writerows(lines)\n",
        "    \n",
        "df = pd.read_csv('translation_data.csv')\n",
        "\n",
        "# Reducing data (throwing out samples)\n",
        "train, _ = train_test_split(df, test_size=0.6)\n",
        "\n",
        "# Split train and test set \n",
        "train, test = train_test_split(train, test_size=0.02)\n",
        "\n",
        "train.to_csv('train.csv', index=False)\n",
        "test.to_csv('test.csv', index=False)\n",
        "\n",
        "\n",
        "df = pd.read_csv('test.csv')\n",
        "df.tail()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>French</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1082</th>\n",
              "      <td>Are you sure you're warm enough?</td>\n",
              "      <td>Êtes-vous sûre d'avoir assez chaud ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1083</th>\n",
              "      <td>Sea turtles have a long lifespan.</td>\n",
              "      <td>Les tortues marines ont une haute espérance de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1084</th>\n",
              "      <td>What prevented him from coming?</td>\n",
              "      <td>Qu'est-ce qui l'a empêché de venir ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085</th>\n",
              "      <td>You will need a bodyguard.</td>\n",
              "      <td>Il te faudra une garde du corps.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1086</th>\n",
              "      <td>He will be delighted to see you.</td>\n",
              "      <td>Il sera ravi de vous voir.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                English                                             French\n",
              "1082   Are you sure you're warm enough?               Êtes-vous sûre d'avoir assez chaud ?\n",
              "1083  Sea turtles have a long lifespan.  Les tortues marines ont une haute espérance de...\n",
              "1084    What prevented him from coming?               Qu'est-ce qui l'a empêché de venir ?\n",
              "1085         You will need a bodyguard.                   Il te faudra une garde du corps.\n",
              "1086   He will be delighted to see you.                         Il sera ravi de vous voir."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnMn_u9wNc5-",
        "colab_type": "code",
        "outputId": "717fb08a-1616-4049-99b2-6a43e2461d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Preparação do dataset:\n",
        "# Tokenização e inclusão dos tokens especiais (<eos>, <sos>, <pad>, <unk>)\n",
        "TEXT_FR = data.Field(tokenize = 'spacy', include_lengths=True, eos_token = \"<eos>\")\n",
        "TEXT_EN = data.Field(tokenize = 'spacy', include_lengths=True, init_token = \"<sos>\", eos_token = \"<eos>\")\n",
        "\n",
        "fields = [('text_en', TEXT_EN), ('text_fr', TEXT_FR)]\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        "                                  path = '.',\n",
        "                                  train = 'train.csv',\n",
        "                                  test = 'test.csv',\n",
        "                                  format = 'csv',\n",
        "                                  fields = fields,\n",
        "                                  skip_header = True)\n",
        "\n",
        "for sample in train_data:\n",
        "  print(sample.text_fr)\n",
        "  print(sample.text_en)\n",
        "  break\n",
        "  \n",
        "print(len(train_data), len(test_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ma', 'sœur', \"m'a\", 'demandé', 'de', 'lui', 'prêter', 'le', 'dictionnaire', '.']\n",
            "['My', 'sister', 'asked', 'me', 'to', 'lend', 'her', 'the', 'dictionary', '.']\n",
            "53250 1087\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzHUADPwPJLz",
        "colab_type": "code",
        "outputId": "05be4730-3bd1-45b9-e9a3-17190b075f3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "source": [
        "# Criando vocabulário\n",
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "TEXT_EN.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE)\n",
        "\n",
        "TEXT_FR.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE)\n",
        "\n",
        "\n",
        "# Instanciando bucket iterator\n",
        "# Note que a ordenação é definida pelo \n",
        "# comprimento do par ** (en, fr) **\n",
        "# Precisaremos empacotar ambas as sequências \n",
        "# para realizar o forward encapsulado.\n",
        "train_iterator = data.BucketIterator(\n",
        "    train_data, \n",
        "    batch_size = args['batch_size'],\n",
        "    sort_key = lambda x:(len(x.text_fr), len(x.text_en)),\n",
        "    sort_within_batch = True,\n",
        "    device = args['device'])\n",
        "\n",
        "\n",
        "test_iterator = data.BucketIterator(\n",
        "    test_data, \n",
        "    batch_size = 1,\n",
        "    sort_within_batch = False,\n",
        "    device = args['device'])\n",
        "\n",
        "for k, batch in enumerate(train_iterator):\n",
        "  text_fr, lengths_fr = batch.text_fr\n",
        "  text_en, lengths_en = batch.text_en\n",
        "  \n",
        "  print(text_fr)\n",
        "  print(text_en)\n",
        "  print(lengths_fr)\n",
        "  print(lengths_en)\n",
        "  break\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1237,    22,   178,    22,    15,    33,    22,   212,    94,     6],\n",
            "        [ 1577,    12,   285,   739, 13315,    58,    20,  3545,    12,    10],\n",
            "        [   29,   114,  1566,     7,   173,    49,    80,   100,    10,   127],\n",
            "        [  995,     7,    16,   273, 13922,    36,     9,  4450, 16412,    80],\n",
            "        [   16,    10,    23,     4,     4,   151,    10,   214,     5,     9],\n",
            "        [  909,   462,    83,   419, 15317,   604,   424,    14,    10,   142],\n",
            "        [ 6880,    25,   120,   246,    39,   213,   941,  2072,     7,    25],\n",
            "        [   29,   483,    11,    82,    14,    14,   210,     4,   141,   403],\n",
            "        [  907,   171,    90,   661,  1220,  2170,  1818,   708,  1044,   473],\n",
            "        [    3,     3,     3,     3,     3,     3,     3,     3,     8,     3],\n",
            "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2]],\n",
            "       device='cuda:0')\n",
            "tensor([[   2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
            "        [ 572,   19,    5,   19,   16,   31,   19,  159,  101,    5],\n",
            "        [ 149,   57,   23,   64, 8250,  464,  165,  555,   13,  150],\n",
            "        [  42,   10,  250,   88,   48,   46,    6,  461,   10,    6],\n",
            "        [1187,  196,    7,  272, 3781,   13,   71,   75,    6,    5],\n",
            "        [  24,    6,  142,  120,   15,   51,   85,  236,  447,   23],\n",
            "        [ 747,   94,   20,  583, 8330,  496,   53,    8,   26,   11],\n",
            "        [ 898,  292,   30,  110,   20,  173, 1249, 1427,  999,  495],\n",
            "        [ 774,  108, 9805,   25, 1083,  368,    4,    4,    9,    4],\n",
            "        [   4,    4,    4,    4,    4,    4,    3,    3,    3,    3],\n",
            "        [   3,    3,    3,    3,    3,    3,    1,    1,    1,    1]],\n",
            "       device='cuda:0')\n",
            "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11], device='cuda:0')\n",
            "tensor([11, 11, 11, 11, 11, 11, 10, 10, 10, 10], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSkInJB2lAs4",
        "colab_type": "text"
      },
      "source": [
        "# Atividade Prática\n",
        "\n",
        "Implemente a arquitetura apresentada na figura abaixo, criando **indivudalmente** os blocos encoder e decoder. Para o nosso exercício, eles não serão agrupados em uma única classe.\n",
        "\n",
        "### EncoderRNN\n",
        "\n",
        "Implemente a classe **EncoderRNN** composta de um passo de representação de palavras e um passo de caracterização de sequência, ou seja, implemente as seguintes camadas:\n",
        "*  Embedding: como não usaremos vetores pré-treinados, a dimensão de saída dessa camada é um hiperparâmetro livre. Sugestão de tamanho: ```100```. Sua entrada é definida pelo tamanho do dicionário do idioma source (nesse caso o francês).\n",
        "*  Dropout: ```0.1``` <br><br>\n",
        "*  GRU: Defina ```hidden_size = 128``` para o encoder\n",
        "\n",
        "### DecoderRNN\n",
        "\n",
        "Implemente a classe **DecoderRNN**. Novamente é necessário uma camada de representação de palavras, seguida de uma camada de caracterização de sequências. Além disso, o decoder também deve possuir uma camada Linear de classificação, que transformará a representação de cada timestep (saída da RNN) em uma predição da próxima palavra.\n",
        "\n",
        "* Embedding: A entrada definida pelo vocabulário do idioma target (inglês), saída é um hiperparâmetro livre (sugestão: ```100```).\n",
        "* Dropout: ```0.1``` <br><br>\n",
        "* GRU: Seus hiperparâmetros são inferíveis a partir das outras informações. **Lembre-se que a inicialização do hidden state é dada pelo último hidden state do encoder** (veja na função train). <br><br>\n",
        "* Linear: Parâmetros inferíveis pelas outras informações. Quantas classes tem a predição de palavras em inglês?\n",
        "* LogSoftmax: ativação da classificação.\n",
        "\n",
        "No decoder, **implemente ambos os forward** para treinamento (encapsulado em batches) e para inferência (loop explícito sem targets).\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1j8aLVymyvhGtM0lpfON0aDyPvUf4700U\" width=\"850\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCV4MQ9WuaD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    # TODO: Implemente o encoder\n",
        "      \n",
        "      \n",
        "class DecoderRNN(nn.Module):\n",
        "    # TODO: Implemente o decoder\n",
        "        \n",
        "      \n",
        "\n",
        "# TODO: Instancie ambos encoder e decoder\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbtjvr-PM9GM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting optimizer.\n",
        "# encoder_optimizer = ...\n",
        "# decoder_optimizer = ...\n",
        "\n",
        "# Setting loss.\n",
        "# NLLLoss usada em par com a ativação LogSoftmax\n",
        "criterion = nn.NLLLoss().to(args['device'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC7WBLNIUwFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, criterion, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for training mode.\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    train_loss = []\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "\n",
        "        # Obtaining images, labels and paths for batch.\n",
        "        text, text_lengths = batch_data.text_fr\n",
        "        labs, labs_lengths = batch_data.text_en\n",
        "        \n",
        "        # Ignorando batches não ordenados para acelerar o treinamento\n",
        "        if sorted(labs_lengths, reverse=True) != list(labs_lengths.data):\n",
        "          continue\n",
        "\n",
        "        # Clears the gradients of optimizer.\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        # Forwarding.\n",
        "        enc, enc_hidden = encoder(text, text_lengths)\n",
        "        outs = decoder(enc_hidden, labs[:-1], labs_lengths-1)\n",
        "        \n",
        "        # Computing loss.\n",
        "        loss = 0.\n",
        "        for k, out in enumerate(outs):\n",
        "          loss += criterion(out, labs[k+1])\n",
        "        loss = loss.mean()\n",
        "        \n",
        "        # Computing backpropagation.\n",
        "        loss.backward()\n",
        "        \n",
        "        # Weight update\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "        \n",
        "        # Updating lists.\n",
        "        train_loss.append(loss.data.item())\n",
        "    \n",
        "    toc = time.time()\n",
        "    \n",
        "    train_loss = np.asarray(train_loss)\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('[epoch %d], [train loss %.4f +/- %.4f], [training time %.2f]' % (\n",
        "        epoch, train_loss.mean(), train_loss.std(), (toc - tic)))\n",
        "    print('--------------------------------------------------------------------')\n",
        "\n",
        "def test(test_loader, criterion, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for evaluation mode (not computing gradients).\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    test_loss = []\n",
        "    \n",
        "    print('********************************************************************')\n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(test_loader):\n",
        "\n",
        "        # Obtaining images, labels and paths for batch.\n",
        "        text, text_lengths = batch_data.text_fr\n",
        "        labs, labs_lengths = batch_data.text_en\n",
        "        \n",
        "        \n",
        "        # Forwarding.\n",
        "        enc, enc_hidden  = encoder(text, text_lengths)\n",
        "        outs = decoder(enc_hidden)\n",
        "\n",
        "        if i < 2:\n",
        "          print('Input:',  [TEXT_FR.vocab.itos[t] for t in text])\n",
        "          print('Label:',  [TEXT_EN.vocab.itos[t] for t in labs[1:]])\n",
        "          print('Output:', [TEXT_EN.vocab.itos[np.argmax(t.cpu().data)] for t in outs], '\\n')\n",
        "        \n",
        "        \n",
        "        # Computing approximate loss \n",
        "        labs = labs[1:]\n",
        "        minlen = min(len(labs), len(outs))\n",
        "        \n",
        "        loss = 0.\n",
        "        for k in range(minlen):\n",
        "          loss += criterion(outs[k], labs[k])\n",
        "        loss = loss.mean()\n",
        "                \n",
        "        # Updating lists.\n",
        "        test_loss.append(loss.data.item())\n",
        "    \n",
        "    toc = time.time()\n",
        "\n",
        "    test_loss = np.asarray(test_loss)\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "   \n",
        "    print('[epoch %d], [test loss %.4f +/- %.4f], [testing time %.2f]' % (\n",
        "        epoch, test_loss.mean(), test_loss.std(), (toc - tic)))\n",
        "    print('********************************************************************')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7scMkTCQhADF",
        "colab_type": "code",
        "outputId": "bb4984f5-65b0-41dc-91cb-d0df34f288c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Iterating over epochs.\n",
        "for epoch in range(1, args['epoch_num'] + 1):\n",
        "\n",
        "    # Training function.\n",
        "    train(train_iterator, criterion, epoch)\n",
        "\n",
        "    # Computing test loss and metrics.\n",
        "    test(test_iterator, criterion, epoch)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------\n",
            "[epoch 1], [train loss 31.6066 +/- 13.2043], [training time 60.28]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Je', 'conduirai', '.', '<eos>']\n",
            "Label: ['I', \"'ll\", 'drive', '.', '<eos>']\n",
            "Output: ['I', \"'m\", 'going', 'to', '.', '<eos>'] \n",
            "\n",
            "Input: [\"J'aime\", 'prendre', 'mon', 'café', 'sans', 'sucre', '.', '<eos>']\n",
            "Label: ['I', 'like', 'my', 'coffee', 'without', 'sugar', '.', '<eos>']\n",
            "Output: ['I', 'like', 'my', 'car', 'to', 'go', '.', '<eos>'] \n",
            "\n",
            "[epoch 1], [test loss 45.5876 +/- 22.4559], [testing time 8.31]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 2], [train loss 24.3321 +/- 10.1345], [training time 60.38]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Tom', 'est', 'un', 'meilleur', 'menteur', 'que', 'toi', '.', '<eos>']\n",
            "Label: ['Tom', 'is', 'a', 'much', 'better', 'liar', 'than', 'you', '.', '<eos>']\n",
            "Output: ['Tom', 'is', 'a', 'good', 'idea', '.', '<eos>'] \n",
            "\n",
            "Input: ['Je', 'préfère', 'voyager', 'en', 'train', 'plutôt', 'que', 'par', 'avion', '.', '<eos>']\n",
            "Label: ['I', 'prefer', 'traveling', 'by', 'train', 'to', 'flying', '.', '<eos>']\n",
            "Output: ['I', 'like', 'to', 'go', 'to', 'the', 'door', '.', '<eos>'] \n",
            "\n",
            "[epoch 2], [test loss 42.6239 +/- 24.4330], [testing time 8.01]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 3], [train loss 20.9720 +/- 8.9125], [training time 60.46]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Ce', \"n'est\", 'pas', 'de', 'votre', 'faute', '.', '<eos>']\n",
            "Label: ['That', \"'s\", 'not', 'your', 'fault', '.', '<eos>']\n",
            "Output: ['That', \"'s\", 'not', 'your', 'way', '.', '<eos>'] \n",
            "\n",
            "Input: ['Cette', 'réponse', 'le', 'mit', 'en', 'colère', '.', '<eos>']\n",
            "Label: ['This', 'answer', 'made', 'him', 'angry', '.', '<eos>']\n",
            "Output: ['This', 'book', 'is', 'very', 'angry', '.', '<eos>'] \n",
            "\n",
            "[epoch 3], [test loss 41.2179 +/- 25.9506], [testing time 8.23]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 4], [train loss 18.5392 +/- 7.9668], [training time 60.55]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Faire', 'de', \"l'argent\", \"n'est\", 'pas', 'le', 'seul', 'but', 'de', 'la', 'vie', '.', '<eos>']\n",
            "Label: ['Making', 'money', 'is', 'not', 'the', 'only', 'goal', 'in', 'life', '.', '<eos>']\n",
            "Output: ['At', 'the', 'problem', ',', 'the', 'not', 'do', \"n't\", 'have', 'the', 'problem', '.', '<eos>'] \n",
            "\n",
            "Input: [\"N'est\", '-', 'elle', 'pas', 'médecin', '?', '<eos>']\n",
            "Label: ['Is', \"n't\", 'she', 'a', 'doctor', '?', '<eos>']\n",
            "Output: ['Is', \"n't\", 'Tom', 'a', 'doctor', '?', '<eos>'] \n",
            "\n",
            "[epoch 4], [test loss 39.4560 +/- 26.2112], [testing time 8.18]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 5], [train loss 17.0325 +/- 7.5424], [training time 60.52]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Fais', '-', 'moi', 'savoir', 'aussitôt', \"qu'il\", 'arrive', '!', '<eos>']\n",
            "Label: ['Let', 'me', 'know', 'as', 'soon', 'as', 'he', 'comes', '.', '<eos>']\n",
            "Output: ['Do', 'whatever', 'he', 'will', 'be', 'seen', '.', '<eos>'] \n",
            "\n",
            "Input: ['Pourrais', '-', 'je', 'avoir', 'une', 'bière', 'supplémentaire', ',', \"s'il\", 'vous', 'plait', '?', '<eos>']\n",
            "Label: ['Could', 'I', 'get', 'one', 'more', 'beer', ',', 'please', '?', '<eos>']\n",
            "Output: ['Could', 'I', 'give', 'you', 'some', 'help', '?', '<eos>'] \n",
            "\n",
            "[epoch 5], [test loss 38.9842 +/- 27.3100], [testing time 8.36]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 6], [train loss 15.9489 +/- 7.1622], [training time 60.39]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Je', 'me', 'réjouis', 'de', 'te', 'revoir', '.', '<eos>']\n",
            "Label: ['I', \"'m\", 'glad', 'to', 'see', 'you', 'again', '.', '<eos>']\n",
            "Output: ['I', \"'m\", 'glad', 'to', 'see', 'you', 'with', 'you', '.', '<eos>'] \n",
            "\n",
            "Input: ['Pouvez', '-', 'vous', 'attendre', 'un', 'peu', 'plus', 'longtemps', '\\u202f', '?', '<eos>']\n",
            "Label: ['Can', 'you', 'hold', 'on', 'a', 'little', 'longer', '?', '<eos>']\n",
            "Output: ['Could', 'you', 'stay', 'a', 'little', 'longer', '?', '<eos>'] \n",
            "\n",
            "[epoch 6], [test loss 37.8811 +/- 26.7128], [testing time 8.27]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 7], [train loss 15.2487 +/- 7.0682], [training time 60.22]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Personne', 'ne', 'le', '<unk>', '.', '<eos>']\n",
            "Label: ['Nobody', 'can', '<unk>', 'him', '.', '<eos>']\n",
            "Output: ['Nobody', \"'s\", 'going', 'to', 'him', '.', '<eos>'] \n",
            "\n",
            "Input: ['Ça', 'me', 'serait', 'égal', '.', '<eos>']\n",
            "Label: ['I', 'would', \"n't\", 'care', '.', '<eos>']\n",
            "Output: ['I', 'would', 'be', 'happy', '.', '<eos>'] \n",
            "\n",
            "[epoch 7], [test loss 37.2919 +/- 26.6330], [testing time 7.92]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 8], [train loss 14.6672 +/- 6.9102], [training time 60.58]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Peux', '-', 'tu', 'déterminer', 'ce', 'qui', \"s'est\", 'passé', '?', '<eos>']\n",
            "Label: ['Can', 'you', 'determine', 'what', 'happened', '?', '<eos>']\n",
            "Output: ['Can', 'you', 'understand', 'what', 'happened', '?', '<eos>'] \n",
            "\n",
            "Input: ['Est', '-', 'ce', 'que', 'le', 'prénom', 'Tom', 'est', 'un', 'prénom', 'courant', 'dans', 'votre', 'pays', '?', '<eos>']\n",
            "Label: ['Is', 'Tom', 'a', 'common', 'name', 'in', 'your', 'country', '?', '<eos>']\n",
            "Output: ['Is', 'this', 'your', 'father', \"'s\", 'name', 'in', 'the', 'garden', '?', '<eos>'] \n",
            "\n",
            "[epoch 8], [test loss 38.0210 +/- 27.9196], [testing time 8.49]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 9], [train loss 14.2600 +/- 6.7980], [training time 60.75]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Je', 'fait', 'maintenant', 'officiellement', 'partie', 'de', 'ce', 'groupe', '.', '<eos>']\n",
            "Label: ['I', \"'m\", 'now', 'officially', 'part', 'of', 'this', 'group', '.', '<eos>']\n",
            "Output: ['I', 'am', 'going', 'to', 'get', 'married', 'in', 'the', 'summer', '.', '<eos>'] \n",
            "\n",
            "Input: ['Tom', 'a', 'peut', '-', 'être', 'été', 'à', 'Boston', 'avec', 'Marie', 'la', 'semaine', 'dernière', '.', '<eos>']\n",
            "Label: ['Tom', 'may', 'have', 'been', 'in', 'Boston', 'with', 'Mary', 'last', 'week', '.', '<eos>']\n",
            "Output: ['Tom', 'could', 'be', 'a', 'week', 'with', 'Mary', \"'s\", 'last', 'week', '.', '<eos>'] \n",
            "\n",
            "[epoch 9], [test loss 36.5717 +/- 26.6071], [testing time 8.21]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 10], [train loss 13.8508 +/- 6.5916], [training time 60.44]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['As', '-', 'tu', 'déjà', 'entendu', 'pareille', 'histoire', '?', '<eos>']\n",
            "Label: ['Have', 'you', 'ever', 'heard', 'such', 'a', 'story', '?', '<eos>']\n",
            "Output: ['Have', 'you', 'ever', 'heard', 'that', 'story', '?', '<eos>'] \n",
            "\n",
            "Input: ['Il', 'te', 'prêtera', 'sûrement', 'son', 'livre', '.', '<eos>']\n",
            "Label: ['He', \"'ll\", 'lend', 'you', 'his', 'book', '.', '<eos>']\n",
            "Output: ['He', 'will', 'read', 'his', 'book', 'to', 'you', '.', '<eos>'] \n",
            "\n",
            "[epoch 10], [test loss 35.7199 +/- 27.2777], [testing time 8.09]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 11], [train loss 13.6837 +/- 6.8215], [training time 60.04]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Il', 'travailla', 'dur', 'à', '<unk>', 'la', 'paix', '.', '<eos>']\n",
            "Label: ['He', 'worked', 'hard', 'to', '<unk>', 'peace', '.', '<eos>']\n",
            "Output: ['He', 'worked', 'hard', 'to', 'go', 'to', 'the', 'world', '.', '<eos>'] \n",
            "\n",
            "Input: ['Il', 'a', 'pris', 'le', 'premier', 'train', 'et', 'est', 'arrivé', 'juste', 'à', 'temps', '.', '<eos>']\n",
            "Label: ['He', 'caught', 'the', 'first', 'train', 'and', 'got', 'there', 'just', 'in', 'time', '.', '<eos>']\n",
            "Output: ['He', 'took', 'the', 'first', 'time', 'to', 'be', 'time', '.', '<eos>'] \n",
            "\n",
            "[epoch 11], [test loss 36.3899 +/- 27.8086], [testing time 7.94]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 12], [train loss 13.3629 +/- 6.4981], [training time 59.74]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: [\"J'ai\", 'besoin', \"d'aide\", 'pour', 'peindre', 'la', 'palissade', '.', '<eos>']\n",
            "Label: ['I', 'need', 'help', 'painting', 'the', 'fence', '.', '<eos>']\n",
            "Output: ['I', 'need', 'to', 'take', 'the', 'medicine', '.', '<eos>'] \n",
            "\n",
            "Input: [\"J'ai\", 'été', 'ému', 'par', 'ses', 'larmes', '.', '<eos>']\n",
            "Label: ['I', 'was', 'moved', 'by', 'his', 'tears', '.', '<eos>']\n",
            "Output: ['I', 'was', 'surprised', 'by', 'his', 'tears', '.', '<eos>'] \n",
            "\n",
            "[epoch 12], [test loss 35.5238 +/- 27.4901], [testing time 8.03]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 13], [train loss 13.2890 +/- 6.7401], [training time 61.41]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Vous', 'auriez', 'pu', 'me', 'le', 'dire', '.', '<eos>']\n",
            "Label: ['You', 'could', 'have', 'told', 'me', '.', '<eos>']\n",
            "Output: ['You', 'could', 'tell', 'me', '.', '<eos>'] \n",
            "\n",
            "Input: [\"T'es\", 'un', 'mec', 'bien', '.', '<eos>']\n",
            "Label: ['You', 'are', 'a', 'good', 'person', '.', '<eos>']\n",
            "Output: ['You', \"'re\", 'a', 'good', 'guy', '.', '<eos>'] \n",
            "\n",
            "[epoch 13], [test loss 35.2275 +/- 27.8427], [testing time 8.14]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 14], [train loss 12.9764 +/- 6.3780], [training time 61.07]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: [\"J'ai\", 'déjà', 'fini', 'le', 'travail', '.', '<eos>']\n",
            "Label: ['I', 'have', 'already', 'finished', 'the', 'job', '.', '<eos>']\n",
            "Output: ['I', \"'ve\", 'already', 'finished', 'the', 'work', '.', '<eos>'] \n",
            "\n",
            "Input: ['Je', 'viendrai', 'dans', 'ton', 'pays', 'un', 'de', 'ces', 'jours', '.', '<eos>']\n",
            "Label: ['I', 'will', 'come', 'to', 'your', 'country', 'some', 'day', '.', '<eos>']\n",
            "Output: ['I', \"'ll\", 'stay', 'in', 'a', 'table', 'for', 'your', 'country', '.', '<eos>'] \n",
            "\n",
            "[epoch 14], [test loss 35.6733 +/- 28.0581], [testing time 8.49]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 15], [train loss 12.8477 +/- 6.4991], [training time 60.91]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Les', '<unk>', 'peuvent', 'être', 'dangereux', '.', '<eos>']\n",
            "Label: ['<unk>', 'can', 'be', 'dangerous', '.', '<eos>']\n",
            "Output: ['The', 'can', 'be', 'how', 'to', 'be', 'careful', '.', '<eos>'] \n",
            "\n",
            "Input: ['Comment', \"l'ont\", '-', 'ils', 'découvert', '?', '<eos>']\n",
            "Label: ['How', 'did', 'they', 'find', 'out', '?', '<eos>']\n",
            "Output: ['How', 'did', 'they', 'get', 'out', 'of', 'this', '?', '<eos>'] \n",
            "\n",
            "[epoch 15], [test loss 35.6749 +/- 28.4616], [testing time 8.33]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 16], [train loss 12.7707 +/- 6.5459], [training time 61.16]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Je', 'suis', 'tellement', 'désolé', 'de', 'vous', 'avoir', 'fait', 'attendre', '.', '<eos>']\n",
            "Label: ['I', 'am', 'so', 'sorry', 'to', 'have', 'kept', 'you', 'waiting', '.', '<eos>']\n",
            "Output: ['I', \"'m\", 'sorry', 'if', 'I', 'could', 'have', 'a', 'friend', 'of', 'you', '.', '<eos>'] \n",
            "\n",
            "Input: ['Je', 'ne', 'peux', 'pas', 'supporter', 'les', 'enfants', '.', '<eos>']\n",
            "Label: ['I', 'ca', \"n't\", 'stand', 'kids', '.', '<eos>']\n",
            "Output: ['I', 'ca', \"n't\", 'stand', 'children', '.', '<eos>'] \n",
            "\n",
            "[epoch 16], [test loss 35.0722 +/- 27.8030], [testing time 8.51]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 17], [train loss 12.6803 +/- 6.6295], [training time 60.30]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Mon', 'verre', 'est', 'plein', '.', '<eos>']\n",
            "Label: ['My', 'glass', 'is', 'full', '.', '<eos>']\n",
            "Output: ['My', 'glass', 'is', 'very', 'much', '.', '<eos>'] \n",
            "\n",
            "Input: ['Je', 'veux', 'que', 'vous', '<unk>', 'selon', 'les', 'ordres', '.', '<eos>']\n",
            "Label: ['I', 'want', 'you', 'to', 'follow', 'orders', '.', '<eos>']\n",
            "Output: ['I', 'want', 'you', 'to', 'love', 'your', 'dog', '.', '<eos>'] \n",
            "\n",
            "[epoch 17], [test loss 35.8418 +/- 28.7743], [testing time 8.26]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 18], [train loss 12.5366 +/- 6.5020], [training time 60.68]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Tout', 'en', 'valait', 'la', 'peine', '.', '<eos>']\n",
            "Label: ['It', 'was', 'all', 'worth', 'it', '.', '<eos>']\n",
            "Output: ['All', 'the', 'whole', 'was', '.', '<eos>'] \n",
            "\n",
            "Input: ['Je', 'me', 'dois', 'de', 'présenter', 'mes', 'excuses', '.', '<eos>']\n",
            "Label: ['I', 'have', 'an', 'apology', 'to', 'make', '.', '<eos>']\n",
            "Output: ['I', 'owe', 'him', 'to', 'apologize', '.', '<eos>'] \n",
            "\n",
            "[epoch 18], [test loss 35.5440 +/- 28.4795], [testing time 8.48]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 19], [train loss 12.4548 +/- 6.4021], [training time 61.10]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Elle', 'cherche', 'un', 'meilleur', 'emploi', '.', '<eos>']\n",
            "Label: ['She', 'is', 'after', 'a', 'better', 'job', '.', '<eos>']\n",
            "Output: ['She', 'is', 'looking', 'for', 'a', 'job', '.', '<eos>'] \n",
            "\n",
            "Input: ['Nous', 'verrons', 'ce', 'que', 'le', 'juge', 'a', 'à', 'dire', 'à', 'ce', 'sujet', '.', '<eos>']\n",
            "Label: ['We', \"'ll\", 'see', 'what', 'the', 'judge', 'has', 'to', 'say', 'about', 'that', '.', '<eos>']\n",
            "Output: ['We', 'all', 'the', 'way', 'to', 'think', 'of', 'that', '.', '<eos>'] \n",
            "\n",
            "[epoch 19], [test loss 34.8681 +/- 28.7507], [testing time 8.37]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 20], [train loss 12.3413 +/- 6.5408], [training time 61.10]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Vous', 'êtes', 'en', 'retard', '.', '<eos>']\n",
            "Label: ['You', 'are', 'late', '.', '<eos>']\n",
            "Output: ['You', \"'re\", 'late', '.', '<eos>'] \n",
            "\n",
            "Input: ['Ça', 'grouille', '<unk>', '.', '<eos>']\n",
            "Label: ['It', \"'s\", 'crawling', 'with', 'spiders', '.', '<eos>']\n",
            "Output: ['It', \"'s\", 'getting', 'cold', '.', '<eos>'] \n",
            "\n",
            "[epoch 20], [test loss 35.8125 +/- 29.1979], [testing time 8.47]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 21], [train loss 12.2125 +/- 6.2221], [training time 61.18]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Tom', 'commença', 'à', 'mettre', 'ses', 'chaussures', '.', '<eos>']\n",
            "Label: ['Tom', 'began', 'putting', 'on', 'his', 'shoes', '.', '<eos>']\n",
            "Output: ['Tom', 'began', 'to', 'put', 'his', 'shoes', '.', '<eos>'] \n",
            "\n",
            "Input: ['Il', 'a', 'eu', 'de', 'la', 'chance', '.', '<eos>']\n",
            "Label: ['He', 'was', 'lucky', '.', '<eos>']\n",
            "Output: ['He', 'was', 'lucky', 'to', 'worry', '.', '<eos>'] \n",
            "\n",
            "[epoch 21], [test loss 34.5301 +/- 28.2499], [testing time 8.37]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 22], [train loss 12.1804 +/- 6.3027], [training time 61.19]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: [\"L'homme\", 'est', 'mortel', '.', '<eos>']\n",
            "Label: ['Man', 'is', '<unk>', '.', '<eos>']\n",
            "Output: ['The', 'man', 'is', '.', '<eos>'] \n",
            "\n",
            "Input: ['Quel', 'est', 'votre', 'prénom', '\\u202f', '?', '<eos>']\n",
            "Label: ['What', 'is', 'your', 'first', 'name', '?', '<eos>']\n",
            "Output: ['What', \"'s\", 'your', 'name', '?', '<eos>'] \n",
            "\n",
            "[epoch 22], [test loss 34.8737 +/- 27.9495], [testing time 8.53]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 23], [train loss 12.0607 +/- 6.1721], [training time 60.78]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: [\"J'aime\", 'écouter', 'la', 'musique', 'classique', '.', '<eos>']\n",
            "Label: ['I', 'like', 'to', 'listen', 'to', 'classical', 'music', '.', '<eos>']\n",
            "Output: ['I', 'like', 'to', 'like', 'music', '.', '<eos>'] \n",
            "\n",
            "Input: ['Il', 'sera', 'mon', '<unk>', 'pendant', 'que', 'je', 'serai', 'parti', '.', '<eos>']\n",
            "Label: ['He', 'will', 'be', 'my', '<unk>', 'while', 'I', 'am', 'away', '.', '<eos>']\n",
            "Output: ['He', 'will', 'be', 'asleep', 'while', 'I', \"'m\", 'waiting', 'for', 'a', 'while', 'I', 'get', 'back', '.', '<eos>'] \n",
            "\n",
            "[epoch 23], [test loss 34.5739 +/- 27.9791], [testing time 8.18]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 24], [train loss 12.0161 +/- 6.1613], [training time 60.63]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Où', '<unk>', 'vous', 'a', '-', 't', '-', 'elle', '<unk>', '?', '<eos>']\n",
            "Label: ['Where', 'did', 'the', 'bee', 'sting', 'you', '?', '<eos>']\n",
            "Output: ['Where', 'did', 'you', 'speak', 'her', '?', '<eos>'] \n",
            "\n",
            "Input: [\"J'espère\", 'que', 'vous', 'vous', 'amusez', 'bien', '.', '<eos>']\n",
            "Label: ['I', 'hope', 'you', \"'re\", 'having', 'fun', '.', '<eos>']\n",
            "Output: ['I', 'hope', 'you', \"'re\", 'making', 'fun', '.', '<eos>'] \n",
            "\n",
            "[epoch 24], [test loss 36.3948 +/- 31.5023], [testing time 8.70]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 25], [train loss 12.0525 +/- 6.2940], [training time 60.92]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Votre', 'réponse', 'est', 'erronée', '.', '<eos>']\n",
            "Label: ['Your', 'answer', 'is', 'wrong', '.', '<eos>']\n",
            "Output: ['Your', 'answer', 'is', 'wrong', '.', '<eos>'] \n",
            "\n",
            "Input: ['Tout', 'espoir', 'a', 'disparu', '.', '<eos>']\n",
            "Label: ['All', 'hope', 'is', 'gone', '.', '<eos>']\n",
            "Output: ['All', 'we', 'should', 'be', 'back', '.', '<eos>'] \n",
            "\n",
            "[epoch 25], [test loss 34.7695 +/- 29.2561], [testing time 8.27]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 26], [train loss 12.0082 +/- 6.5533], [training time 60.95]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Où', 'te', 'fais', '-', 'tu', 'coiffer', '?', '<eos>']\n",
            "Label: ['Where', 'do', 'you', 'get', 'your', 'hair', 'done', '?', '<eos>']\n",
            "Output: ['Where', 'do', 'you', 'get', 'your', 'car', '?', '<eos>'] \n",
            "\n",
            "Input: ['Tom', '<unk>', '.', '<eos>']\n",
            "Label: ['Tom', 'hesitated', '.', '<eos>']\n",
            "Output: ['Tom', 'is', 'getting', 'drunk', '.', '<eos>'] \n",
            "\n",
            "[epoch 26], [test loss 34.7251 +/- 28.5333], [testing time 8.30]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 27], [train loss 11.8850 +/- 6.2869], [training time 60.60]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Il', 'est', 'plein', \"d'énergie\", '.', '<eos>']\n",
            "Label: ['He', 'is', 'full', 'of', 'energy', '.', '<eos>']\n",
            "Output: ['He', \"'s\", 'very', 'special', '.', '<eos>'] \n",
            "\n",
            "Input: ['Je', 'me', '<unk>', 'très', 'seul', 'si', 'tu', 'venais', 'à', 'disparaître', '.', '<eos>']\n",
            "Label: ['When', 'you', 'go', ',', 'I', \"'ll\", 'miss', 'you', 'very', 'much', '.', '<eos>']\n",
            "Output: ['I', \"'m\", 'very', 'glad', 'to', 'be', 'the', 'only', 'one', 'to', 'keep', 'you', 'alone', '.', '<eos>'] \n",
            "\n",
            "[epoch 27], [test loss 34.4899 +/- 29.2277], [testing time 8.25]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 28], [train loss 11.8182 +/- 6.2804], [training time 60.38]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Laissez', '-', 'nous', 'cela', '!', '<eos>']\n",
            "Label: ['Leave', 'it', 'to', 'us', '.', '<eos>']\n",
            "Output: ['Let', \"'s\", 'do', 'this', '.', '<eos>'] \n",
            "\n",
            "Input: ['Je', 'suis', 'prêt', 'pour', 'mon', 'prochain', 'défi', '.', '<eos>']\n",
            "Label: ['I', \"'m\", 'ready', 'for', 'my', 'next', 'challenge', '.', '<eos>']\n",
            "Output: ['I', \"'m\", 'ready', 'to', 'meet', 'my', 'mother', '.', '<eos>'] \n",
            "\n",
            "[epoch 28], [test loss 33.8927 +/- 28.7601], [testing time 8.32]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 29], [train loss 11.8294 +/- 6.1725], [training time 60.80]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Je', 'me', 'sens', 'mieux', ',', 'maintenant', '.', '<eos>']\n",
            "Label: ['I', 'feel', 'better', 'now', '.', '<eos>']\n",
            "Output: ['I', 'feel', 'better', 'right', 'now', '.', '<eos>'] \n",
            "\n",
            "Input: ['Je', 'ne', 'suis', 'pas', 'extravertie', '.', '<eos>']\n",
            "Label: ['I', \"'m\", 'not', 'outgoing', '.', '<eos>']\n",
            "Output: ['I', \"'m\", 'not', 'allowed', '.', '<eos>'] \n",
            "\n",
            "[epoch 29], [test loss 33.6261 +/- 28.3520], [testing time 8.09]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 30], [train loss 11.7504 +/- 6.0837], [training time 60.66]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Les', 'tortues', '<unk>', 'ont', 'une', 'haute', '<unk>', 'de', 'vie', '.', '<eos>']\n",
            "Label: ['Sea', 'turtles', 'have', 'a', 'long', 'lifespan', '.', '<eos>']\n",
            "Output: ['The', 'many', 'countries', 'have', 'a', 'large', 'life', '.', '<eos>'] \n",
            "\n",
            "Input: ['Êtes', '-', 'vous', 'sûre', \"d'avoir\", 'assez', 'chaud', '?', '<eos>']\n",
            "Label: ['Are', 'you', 'sure', 'you', \"'re\", 'warm', 'enough', '?', '<eos>']\n",
            "Output: ['Are', 'you', 'sure', 'you', \"'re\", 'warm', 'enough', '?', '<eos>'] \n",
            "\n",
            "[epoch 30], [test loss 34.7698 +/- 28.5215], [testing time 8.45]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 31], [train loss 11.7224 +/- 6.0927], [training time 60.83]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Elle', 'sauva', 'la', 'vie', 'de', 'son', 'bébé', 'au', 'risque', 'de', 'perdre', 'la', 'sienne', '.', '<eos>']\n",
            "Label: ['She', 'saved', 'her', 'baby', \"'s\", 'life', 'at', 'the', 'risk', 'of', 'losing', 'her', 'own', '.', '<eos>']\n",
            "Output: ['She', 'met', 'her', 'life', 'to', 'be', 'dead', 'in', 'the', 'garden', '.', '<eos>'] \n",
            "\n",
            "Input: ['Nous', \"n'avons\", 'pas', 'encore', 'très', 'faim', '.', '<eos>']\n",
            "Label: ['We', 'are', \"n't\", 'very', 'hungry', 'yet', '.', '<eos>']\n",
            "Output: ['We', \"'re\", 'not', 'really', 'hungry', '.', '<eos>'] \n",
            "\n",
            "[epoch 31], [test loss 33.6197 +/- 28.3057], [testing time 8.32]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 32], [train loss 11.6611 +/- 6.1385], [training time 60.52]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Êtes', '-', 'vous', 'sûre', \"d'avoir\", 'assez', 'chaud', '?', '<eos>']\n",
            "Label: ['Are', 'you', 'sure', 'you', \"'re\", 'warm', 'enough', '?', '<eos>']\n",
            "Output: ['Are', 'you', 'sure', 'you', \"'re\", 'hot', 'enough', '?', '<eos>'] \n",
            "\n",
            "Input: ['Le', 'jour', 'où', 'nous', 'arrivâmes', 'était', 'férié', '.', '<eos>']\n",
            "Label: ['The', 'day', 'we', 'arrived', 'was', 'a', 'holiday', '.', '<eos>']\n",
            "Output: ['The', 'day', 'we', 'were', 'going', 'abroad', '.', '<eos>'] \n",
            "\n",
            "[epoch 32], [test loss 34.0354 +/- 28.3915], [testing time 8.33]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 33], [train loss 11.6185 +/- 6.1215], [training time 60.35]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Voulez', '-', 'vous', 'me', 'prêter', 'votre', 'vélo', '?', '<eos>']\n",
            "Label: ['Will', 'you', 'lend', 'me', 'your', 'bicycle', '?', '<eos>']\n",
            "Output: ['Do', 'you', 'want', 'to', 'give', 'me', 'a', 'bicycle', '?', '<eos>'] \n",
            "\n",
            "Input: [\"J'ai\", '<unk>', 'le', 'piano', 'tous', 'les', 'jours', 'pendant', '15', 'ans', '.', '<eos>']\n",
            "Label: ['I', 'have', 'practiced', 'piano', 'every', 'day', 'for', 'fifteen', 'years', '.', '<eos>']\n",
            "Output: ['I', 'have', 'the', 'last', 'train', 'to', 'study', 'today', '.', '<eos>'] \n",
            "\n",
            "[epoch 33], [test loss 33.6589 +/- 28.3280], [testing time 8.07]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 34], [train loss 11.5518 +/- 5.9513], [training time 60.38]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Tom', 'a', 'été', 'surpris', 'par', 'ce', \"qu'il\", 'a', 'vu', '.', '<eos>']\n",
            "Label: ['Tom', 'was', 'surprised', 'by', 'what', 'he', 'saw', '.', '<eos>']\n",
            "Output: ['Tom', 'was', 'surprised', 'at', 'what', 'he', 'saw', '.', '<eos>'] \n",
            "\n",
            "Input: ['Vous', 'êtes', 'plus', 'grande', 'que', 'moi', '.', '<eos>']\n",
            "Label: ['You', \"'re\", 'taller', 'than', 'I', 'am', '.', '<eos>']\n",
            "Output: ['You', \"'re\", 'taller', 'than', 'me', '.', '<eos>'] \n",
            "\n",
            "[epoch 34], [test loss 34.3459 +/- 28.2900], [testing time 8.25]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 35], [train loss 11.5873 +/- 6.1464], [training time 60.17]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Comment', \"l'avez\", '-', 'vous', 'perdue', '?', '<eos>']\n",
            "Label: ['How', 'did', 'you', 'lose', 'it', '?', '<eos>']\n",
            "Output: ['How', 'did', 'you', 'get', 'lost', '?', '<eos>'] \n",
            "\n",
            "Input: ['Il', 'a', 'travaillé', 'toute', 'la', 'journée', '.', '<eos>']\n",
            "Label: ['He', \"'s\", 'been', 'working', 'all', 'day', 'long', '.', '<eos>']\n",
            "Output: ['He', 'worked', 'all', 'day', '.', '<eos>'] \n",
            "\n",
            "[epoch 35], [test loss 34.6806 +/- 28.7067], [testing time 8.26]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 36], [train loss 11.5337 +/- 6.0927], [training time 59.96]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Je', 'pense', 'que', 'Tom', 'peut', 'faire', 'la', 'différence', '.', '<eos>']\n",
            "Label: ['I', 'think', 'Tom', 'could', 'make', 'a', 'difference', '.', '<eos>']\n",
            "Output: ['I', 'think', 'Tom', 'can', 'make', 'them', '.', '<eos>'] \n",
            "\n",
            "Input: ['Tom', 'est', 'mon', 'frère', '.', '<eos>']\n",
            "Label: ['Tom', 'is', 'my', 'brother', '.', '<eos>']\n",
            "Output: ['Tom', 'is', 'my', 'brother', '.', '<eos>'] \n",
            "\n",
            "[epoch 36], [test loss 34.1459 +/- 28.8306], [testing time 8.23]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 37], [train loss 11.5422 +/- 6.1336], [training time 60.52]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Tu', 'ne', \"m'as\", 'jamais', 'dit', 'ça', '.', '<eos>']\n",
            "Label: ['You', 'never', 'told', 'me', 'that', '.', '<eos>']\n",
            "Output: ['You', 'never', 'told', 'me', 'that', '.', '<eos>'] \n",
            "\n",
            "Input: ['Ça', 'grouille', '<unk>', '.', '<eos>']\n",
            "Label: ['It', \"'s\", 'crawling', 'with', 'spiders', '.', '<eos>']\n",
            "Output: ['It', \"'s\", 'getting', 'cold', '.', '<eos>'] \n",
            "\n",
            "[epoch 37], [test loss 34.2112 +/- 28.1123], [testing time 8.31]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 38], [train loss 11.5328 +/- 6.1891], [training time 60.74]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: [\"J'aimerais\", 'oublier', 'que', 'tout', 'ça', \"s'est\", 'produit', '.', '<eos>']\n",
            "Label: ['I', \"'d\", 'like', 'to', 'forget', 'the', 'whole', 'thing', 'ever', 'happened', '.', '<eos>']\n",
            "Output: ['I', \"'d\", 'like', 'to', 'know', 'what', 'happened', '.', '<eos>'] \n",
            "\n",
            "Input: ['Je', 'suppose', 'que', \"c'est\", 'permis', '.', '<eos>']\n",
            "Label: ['I', 'suppose', 'that', \"'s\", 'allowed', '.', '<eos>']\n",
            "Output: ['I', 'guess', 'that', \"'s\", 'an', 'emergency', '.', '<eos>'] \n",
            "\n",
            "[epoch 38], [test loss 33.2835 +/- 27.7976], [testing time 8.21]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 39], [train loss 11.4360 +/- 5.9576], [training time 60.74]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Il', 'te', 'faudra', 'une', 'garde', 'du', 'corps', '.', '<eos>']\n",
            "Label: ['You', 'will', 'need', 'a', 'bodyguard', '.', '<eos>']\n",
            "Output: ['You', \"'ll\", 'need', 'a', 'bodyguard', '.', '<eos>'] \n",
            "\n",
            "Input: ['Il', 'avait', \"l'air\", 'déconcerté', '.', '<eos>']\n",
            "Label: ['He', 'looked', '<unk>', '.', '<eos>']\n",
            "Output: ['He', 'looked', 'looked', '.', '<eos>'] \n",
            "\n",
            "[epoch 39], [test loss 33.9879 +/- 28.4217], [testing time 8.34]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 40], [train loss 11.4502 +/- 6.2317], [training time 60.84]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Je', \"n'ai\", 'pas', 'de', 'tatouage', '.', '<eos>']\n",
            "Label: ['I', 'do', \"n't\", 'have', 'a', 'tattoo', '.', '<eos>']\n",
            "Output: ['I', 'do', \"n't\", 'have', 'to', 'sell', '.', '<eos>'] \n",
            "\n",
            "Input: ['Les', 'conducteurs', 'doivent', 'faire', 'attention', 'aux', 'enfants', 'qui', 'traversent', 'la', 'route', '.', '<eos>']\n",
            "Label: ['Drivers', 'must', 'look', 'out', 'for', 'children', 'crossing', 'the', 'road', '.', '<eos>']\n",
            "Output: ['The', 'world', 'must', 'be', 'careful', 'with', 'the', 'beach', '.', '<eos>'] \n",
            "\n",
            "[epoch 40], [test loss 33.9795 +/- 28.1646], [testing time 8.30]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 41], [train loss 11.3304 +/- 5.8532], [training time 60.39]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: [\"C'est\", 'de', 'votre', 'responsabilité', '.', '<eos>']\n",
            "Label: ['That', \"'s\", 'your', 'responsibility', '.', '<eos>']\n",
            "Output: ['It', \"'s\", 'your', 'responsibility', '.', '<eos>'] \n",
            "\n",
            "Input: ['Y', 'a', '-', 't', '-', 'il', 'un', 'problème', '\\u202f', '?', '<eos>']\n",
            "Label: ['What', \"'s\", 'your', 'problem', '?', '<eos>']\n",
            "Output: ['Is', 'there', 'a', 'problem', '?', '<eos>'] \n",
            "\n",
            "[epoch 41], [test loss 33.7133 +/- 28.0731], [testing time 8.29]\n",
            "********************************************************************\n",
            "--------------------------------------------------------------------\n",
            "[epoch 42], [train loss 11.4051 +/- 6.0812], [training time 60.72]\n",
            "--------------------------------------------------------------------\n",
            "********************************************************************\n",
            "Input: ['Il', 'était', 'petit', 'mais', 'fort', '.', '<eos>']\n",
            "Label: ['He', 'was', 'small', ',', 'but', 'strong', '.', '<eos>']\n",
            "Output: ['He', 'was', 'tired', ',', 'but', 'I', 'could', \"n't\", 'have', 'told', '.', '<eos>'] \n",
            "\n",
            "Input: ['Nous', 'voulons', 'que', 'tu', 'la', 'prennes', '.', '<eos>']\n",
            "Label: ['We', 'want', 'you', 'to', 'take', 'it', '.', '<eos>']\n",
            "Output: ['We', 'want', 'you', 'to', 'take', 'it', '.', '<eos>'] \n",
            "\n",
            "[epoch 42], [test loss 33.7877 +/- 28.5964], [testing time 8.21]\n",
            "********************************************************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-32d4c37e7a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Training function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Computing test loss and metrics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-116-fdb81b711cfd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, criterion, epoch)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Ignorando batches não ordenados para acelerar o treinamento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabs_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabs_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m           \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}