{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S04A01_3 - ResNet in Pytorch (Gabarito).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG-mVsVuE0if",
        "colab_type": "text"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports, funções, downloads e instalação do Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ65_lev3RXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reinstalling torch with the right CUDA bindings.\n",
        "!pip3 install -U https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install -U https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEHmMCjR4PJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic imports.\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from skimage import io\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwhRUUlc4j23",
        "colab_type": "code",
        "outputId": "4e83f2ff-386e-4a56-e51b-4e97a03199ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 50,      # Number of epochs.\n",
        "    'n_classes': 10,      # Number of classes.\n",
        "    'lr': 1e-4,           # Learning rate.\n",
        "    'weight_decay': 5e-4, # L2 penalty.\n",
        "    'momentum': 0.9,      # Momentum.\n",
        "    'num_workers': 3,     # Number of workers on data loader.\n",
        "    'batch_size': 200,    # Mini-batch size.\n",
        "    'w_size': 224,        # Width size for image resizing.\n",
        "    'h_size': 224,        # Height size for image resizing.\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20kc9tHQ59ba",
        "colab_type": "text"
      },
      "source": [
        "# O pacote Torchvision\n",
        "\n",
        "Assim como no MXNet, o subpacote [datasets](https://pytorch.org/docs/stable/torchvision/datasets.html) do [torchvision](https://pytorch.org/docs/stable/torchvision/index.html) do Pytorch possui dataloaders para vários datasets conhecidos da literatura de Visão Computacional. O CIFAR10, já usado previamente no curso, é um desses dataset clássicos de classificação que possuem implementações que podem ser carregados por meio de uma única linha de código. Outros datasets clássicos presentes no torchvision são: MNIST (classificação), Fasion-MNIST (classificação), ImageNet (classificação), VOC (segmentação e detecção), COCO (detecção e captioning), etc. No bloco de código abaixo são carregados os datasets de treino e teste do CIFAR10 e seus respectivos dataloaders.\n",
        "\n",
        "O torchvision também possui outras funções de apoio ao treino de modelos para Deep Learning em imagens, como a definição de modelos (pré-treinados ou não) no subpacote [models](https://pytorch.org/docs/stable/torchvision/models.html) e a possibilidade de realizar pré-processamentos em imagens (incluindo Data Augmentation) por meio de transformações comuns em imagens no subpacote [transforms](https://pytorch.org/docs/stable/torchvision/transforms.html). Essas transformações também são demonstradas no código abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ura-BNFMcoTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Root directory for the dataset (to be downloaded).\n",
        "root = './'\n",
        "\n",
        "# Data Augmentation transforms.\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((100, 100)),\n",
        "    transforms.RandomCrop((75, 75)),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Setting datasets and dataloaders.\n",
        "train_set = datasets.CIFAR10(root,\n",
        "                             train=True,\n",
        "                             download=True,\n",
        "                             transform=data_transform)\n",
        "test_set = datasets.CIFAR10(root,\n",
        "                            train=False,\n",
        "                            download=False,\n",
        "                            transform=data_transform)\n",
        "    \n",
        "for iters in range(5):\n",
        "    \n",
        "    fig, ax = plt.subplots(1, 5, figsize=(20, 4))\n",
        "\n",
        "    for i, test_data in enumerate(test_set):\n",
        "\n",
        "        if i >= 5:\n",
        "            break\n",
        "    \n",
        "        test_img, _ = test_data\n",
        "\n",
        "        ax[i].imshow(test_img.numpy().transpose(1, 2, 0))\n",
        "        ax[i].set_yticks([])\n",
        "        ax[i].set_xticks([])\n",
        "        ax[i].set_title('Image ' + str(i))\n",
        "        \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi3Zh8fQ4X_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Root directory for the dataset (to be downloaded).\n",
        "root = './'\n",
        "\n",
        "# Setting transforms (resizing, conversion to tensor and normalizing).\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((args['h_size'], args['w_size'])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Setting datasets and dataloaders.\n",
        "train_set = datasets.CIFAR10(root,\n",
        "                             train=True,\n",
        "                             download=True,\n",
        "                             transform=data_transform)\n",
        "test_set = datasets.CIFAR10(root,\n",
        "                            train=False,\n",
        "                            download=False,\n",
        "                            transform=data_transform)\n",
        "\n",
        "train_loader = DataLoader(train_set,\n",
        "                          args['batch_size'],\n",
        "                          num_workers=args['num_workers'],\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(test_set,\n",
        "                         args['batch_size'],\n",
        "                         num_workers=args['num_workers'],\n",
        "                         shuffle=False)\n",
        "\n",
        "# Printing training and testing dataset sizes.\n",
        "print('Size of training set: ' + str(len(train_set)) + ' samples')\n",
        "print('Size of test set: ' + str(len(test_set)) + ' samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drOsx-32Ifo1",
        "colab_type": "text"
      },
      "source": [
        "# Programação Orientada a Objetos\n",
        "\n",
        "Faz-se necessário o uso de  [Programação Orientada a Objetos](https://www.tutorialspoint.com/python/python_classes_objects.htm) (POO) à medida que se começa a lidar com tarefas mais específicas tratadas por algoritmos de Deep Learning. Algumas vantagens que POO trazem para a programação de algoritmos de Deep Learning são:\n",
        "\n",
        "1.   Organização do código. Criar uma arquitetura com centenas de camadas podendo conter várias *skip connections* no meio de um bloco de código, como vínhamos fazendo anteriormente, torna o nosso código desorganizado;\n",
        "2.   Reaproveitamento dos códigos básicos do framework (i.e. dataloaders, módulos/camadas de redes predefinidos, funções de perda, etc) para adaptá-los para novas tarefas minimizando a quantidade de código que necessita ser escrita;\n",
        "3.   Códigos escritos em formato POO podem ser reutilizados em outros pontos da aplicação. Arquiteturas como [DenseNets](https://arxiv.org/abs/1608.06993), [ResNets](https://arxiv.org/abs/1512.03385) ou [U-Nets](https://arxiv.org/abs/1505.04597) possuem blocos de código que se repetem, os quais podem ser simplificados com o uso de POO;\n",
        "4.   POO também permite uma melhor modularização do código, de forma que mudanças (i.e. loss function, arquitetura, dataloader, etc) possam ser feitas em módulos isolados sem afetar o resto do código.\n",
        "\n",
        "No caso da implementação de modelos usando classes no Pytorch, normalmente é preciso se preocupar apenas com a implementação de dois métodos: o *\\_\\_init\\_\\_()* e o *forward()*. O *\\_\\_init\\_\\_()* é conhecido como o construtor da classe e define a arquitetura da rede neural que é implementada pela classe, ou seja, é nesse método que se definem as camadas da rede neural. Já o *forward()* serve para definir o fluxo de dados que passará pelas camadas definidas no *\\_\\_init\\_\\_()*. O *forward()* recebe sempre como entrada os dados que passarão pelo modelo e deve retornar o output do modelo após processar esses dados de entrada ao longo de suas camadas. É notável que entre as camadas convolucionais e as camadas Fully Connected, faz-se necessário chamar o método [*view()*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) para linearizar os dados do tensor.\n",
        "\n",
        "Também é preciso lembrar de sempre definir a classe em que o modelo está sendo implementado como subclasse de *nn.Module*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg1Ci0ZQ4xIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VGG11 implementation.\n",
        "class VGG11(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_classes=10):\n",
        "\n",
        "        super(VGG11, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool2d(output_size=(7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=12544, out_features=4096, bias=True),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n",
        "        )       \n",
        "        self.initialize_weights()\n",
        "    \n",
        "    # Function for randomly initializing weights.\n",
        "    def initialize_weights(self):\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.features(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1) # Linearizing.\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Instantiating architecture.\n",
        "net = VGG11(args['n_classes']).to(args['device'])\n",
        "\n",
        "# Printing architecture.\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk4sjZ_HnjY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using predefined and pretrained model of VGG11 on torchvision.\n",
        "# net = models.vgg11(pretrained=True, progress=False).to(args['device'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWKYCVh5oFtL",
        "colab_type": "text"
      },
      "source": [
        "# Redes Residuais (ResNets)\n",
        "\n",
        "Entre 2012 e 2015 a comunidade de Visão Computacional percebeu que redes mais profundas conseguiam capturar características semânticas mais úteis dos dados para tarefas de reconhecimento de imagens (i.e. classificação, segmentação, detecção...). Porém, redes mais profundas que as maiores arquiteturas da época -- como a [VGG](https://arxiv.org/abs/1409.1556) e a [Inception](https://arxiv.org/abs/1409.4842) -- sofriam de um problema chamado **Vanishing Gradient**.\n",
        "\n",
        "![VGG](https://www.researchgate.net/profile/Clifford_Yang/publication/325137356/figure/fig2/AS:670371271413777@1536840374533/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means_W640.jpg)\n",
        "\n",
        "![Inception](https://miro.medium.com/max/700/1*uW81y16b-ptBDV8SIT1beQ.png)\n",
        "\n",
        "\n",
        "O **Vanishing Gradient** se torna mais problemático em redes mais profundas porque o gradiente dos erros precisa backpropagar desde a última camada até o começo da rede. Dessa forma, as últimas camadas conseguem ser treinadas de forma eficiente, mas o gradiente dos erros vai desaparecendo à medida em que backpropaga pela rede, praticamente impossibilitando o treinamento das primeiras camadas. Assim, foi constatado que uma rede com, por exemplo, 34 camadas acabava por conseguir resultados piores que uma rede com apenas 18 camadas.\n",
        "\n",
        "![Rede Não Residual](https://www.dropbox.com/s/pq190al5b3qv194/normal_18_vs_34_layers.png?dl=1)\n",
        "\n",
        "No final de 2015 foi proposta uma solução para o **Vanishing Gradient** na forma de **Blocos Residuais** que, juntos, formam **Redes Residuais** [**(ResNets)**](https://arxiv.org/abs/1512.03385). Esses blocos residuais recebem uma entrada $x$ e a alimentam para um bloco convolucional ($\\mathcal{F}(x)$) composto por:\n",
        "\n",
        "1.   Convolução 3x3;\n",
        "2.   Batch Normalization;\n",
        "3.   ReLU;\n",
        "4.   Convolução 3x3;\n",
        "5.   Batch Normalization.\n",
        "\n",
        "A saída $\\mathcal{F}(x)$ desse bloco, antes de ser passada por uma segunda ReLU, é passada em conjunto com a entrada $x$ para uma função identidade, que, no caso das **ResNets**, é uma simples soma. Dessa forma, a saída final de um **Bloco Residual** é dada por: $\\mathcal{F}(x) + x$. O esquema de um **Bloco Residual** pode ser visto na figura abaixo.\n",
        "\n",
        "![Bloco Residual](https://www.dropbox.com/s/ezydump33p95ohc/residual_block.png?dl=1)\n",
        "\n",
        "Como pode ser visto na imagem a seguir, com o uso de blocos residuais, uma arquitetura com 34 camadas consegue resultados melhores que uma arquitetura com apenas 18 camadas. Esses resultados evidenciam que o uso da soma como **identity function** de fato permite que o backward treine mais efetivamente as primeiras camadas das **ResNets**. **ResNets** permitiram que CNNs chegassem até a casa das 100 camadas. A maior **ResNet** usada na prática possui 152 camadas, o que a deixa impraticável de imprimir numa figura como é mostrado abaixo na ResNet34.\n",
        "\n",
        "![Rede Residual](https://www.dropbox.com/s/q4wcjwf8qj4xjrn/resnet_18_vs_34_layers.png?dl=1)\n",
        "\n",
        "Como pode ser visto nas imagens abaixo, ResNets (e outras arquiteturas modernas como a [VGG](https://arxiv.org/abs/1409.1556) e as [DenseNets](https://arxiv.org/abs/1608.06993) são compostas basicamente de convoluções com kernels de tamanho 3x3. Além disso, é notável na arquitetura residual (à direita) a presença dos \"atalhos\" para o gradiente  na forma das funções identidade que ajudam no treinamento das primeiras camadas durante o backpropagation.\n",
        "\n",
        "![VGG vs. Plain34 vs. ResNet34](https://www.dropbox.com/s/d2w3h7dlumgclx2/vgg_plain34_resnet34.png?dl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHotK0b3hqpG",
        "colab_type": "text"
      },
      "source": [
        "# Atividade Prática\n",
        "\n",
        "1.   Implementar a classe *ResidualBlock()*. Devem ser implementados os"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25PD5HF-8XxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Implementation of residual block to be reused.\n",
        "# class ResidualBlock(nn.Module):\n",
        "    \n",
        "#     def __init__(self, in_planes, out_planes):\n",
        "        \n",
        "#         super(ResidualBlock, self).__init__()\n",
        "        \n",
        "#         # First conv block.\n",
        "#         self.conv1 = nn.Conv2d(in_planes,\n",
        "#                                out_planes,\n",
        "#                                kernel_size=3,\n",
        "#                                padding=1)\n",
        "#         self.bn1 = nn.BatchNorm2d(out_planes)\n",
        "        \n",
        "#         # Second conv block.\n",
        "#         self.conv2 = nn.Conv2d(out_planes,\n",
        "#                                out_planes,\n",
        "#                                kernel_size=3,\n",
        "#                                padding=1)\n",
        "#         self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        \n",
        "#         # Activation for both blocks.\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "#         # If in_planes != out_planes, perform a 1x1 conv to match #channels.\n",
        "#         self.conv1x1 = None\n",
        "#         if in_planes != out_planes:\n",
        "#             self.conv1x1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, padding=0)\n",
        "\n",
        "#     def forward(self, x):\n",
        "        \n",
        "#         identity = x\n",
        "\n",
        "#         # First conv block.\n",
        "#         out = self.conv1(x)\n",
        "#         out = self.bn1(out)\n",
        "#         out = self.relu(out)\n",
        "\n",
        "#         # Second conv block.\n",
        "#         out = self.conv2(out)\n",
        "#         out = self.bn2(out)\n",
        "#         if self.conv1x1 is not None:\n",
        "#             identity = self.conv1x1(identity)\n",
        "#         out += identity # Sum of output and identity.\n",
        "#         out = self.relu(out)\n",
        "\n",
        "#         return out\n",
        "\n",
        "# # ResNet18 implementation.\n",
        "# class ResNet18(nn.Module):\n",
        "    \n",
        "#     def __init__(self, num_classes=10):\n",
        "\n",
        "#         super(ResNet18, self).__init__()\n",
        "\n",
        "#         self.conv1 = nn.Conv2d(3,\n",
        "#                                64,\n",
        "#                                kernel_size=7,\n",
        "#                                stride=2,\n",
        "#                                padding=3,\n",
        "#                                bias=False)\n",
        "#         self.bn1 = nn.BatchNorm2d(64)\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "#         self.maxpool1 = nn.MaxPool2d(kernel_size=3,\n",
        "#                                      stride=2,\n",
        "#                                      padding=1)\n",
        "        \n",
        "#         self.res_block1 = ResidualBlock(64, 64)\n",
        "#         self.maxpool2 = nn.MaxPool2d(kernel_size=3,\n",
        "#                                      stride=2,\n",
        "#                                      padding=1)\n",
        "#         self.res_block2 = ResidualBlock(64, 128)\n",
        "#         self.maxpool3 = nn.MaxPool2d(kernel_size=3,\n",
        "#                                      stride=2,\n",
        "#                                      padding=1)\n",
        "        \n",
        "#         self.adaptive_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "#         self.classifier = nn.Linear(128, num_classes)\n",
        "        \n",
        "#         self.initialize_weights()\n",
        "    \n",
        "#     # Function for randomly initializing weights.\n",
        "#     def initialize_weights(self):\n",
        "        \n",
        "#         for m in self.modules():\n",
        "#             if isinstance(m, nn.Conv2d):\n",
        "#                 nn.init.kaiming_normal_(m.weight,\n",
        "#                                         mode='fan_out',\n",
        "#                                         nonlinearity='relu')\n",
        "#                 if m.bias is not None:\n",
        "#                     nn.init.constant_(m.bias, 0)\n",
        "#             elif isinstance(m, nn.BatchNorm2d):\n",
        "#                 nn.init.constant_(m.weight, 1)\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#             elif isinstance(m, nn.Linear):\n",
        "#                 nn.init.normal_(m.weight, 0, 0.01)\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "                \n",
        "#     def forward(self, x):\n",
        "        \n",
        "#         # First convolution block.\n",
        "#         out = self.conv1(x)\n",
        "#         out = self.bn1(out)\n",
        "#         out = self.relu(out)\n",
        "#         out = self.maxpool1(out)\n",
        "        \n",
        "#         # First residual block.\n",
        "#         out = self.res_block1(out)\n",
        "#         out = self.maxpool2(out)\n",
        "        \n",
        "#         # Second residual block.\n",
        "#         out = self.res_block2(out)\n",
        "#         out = self.maxpool3(out)\n",
        "        \n",
        "#         # Adaptive Average Pooling for consistent output size of 1x1.\n",
        "#         out = self.adaptive_pool(out)\n",
        "        \n",
        "#         # Linearizing trailling dimensions.\n",
        "#         out = out.view(x.size(0), -1)\n",
        "        \n",
        "#         # Inference layer.\n",
        "#         out = self.classifier(out)\n",
        "        \n",
        "#         return out\n",
        "\n",
        "# # Instantiating architecture.\n",
        "# net = ResNet18(args['n_classes']).to(args['device'])\n",
        "\n",
        "# # Printing architecture.\n",
        "# print(net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpNtOtd6bHDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using predefined and pretrained model of ResNet18 on torchvision.\n",
        "# net = models.resnet18(pretrained=True, progress=False).to(args['device'])\n",
        "# net.fc = nn.Linear(in_features=512, out_features=10, bias=True).to(args['device'])\n",
        "\n",
        "# Shrunk version of ResNet18 to be used in class.\n",
        "# net = models.resnet18(pretrained=True, progress=False).to(args['device'])\n",
        "# net.layer3 = nn.Sequential().to(args['device']) # Uncomment to shrink ResNet.\n",
        "# net.layer4 = nn.Sequential().to(args['device']) # Uncomment to shrink ResNet.\n",
        "# net.fc = nn.Linear(in_features=128, out_features=10, bias=True).to(args['device'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS2l_pqAI0F2",
        "colab_type": "text"
      },
      "source": [
        "# Definindo o otimizador\n",
        "\n",
        "O Pytorch possui vários otimizadores prontos no subpacote [optim](https://pytorch.org/docs/stable/optim.html), desde o SGD básico a otimizadores mais complexos e com taxas de aprendizado por parâmetro como o Adagrad, RMSProp e Adam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_-RN1wH-4bB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(net.parameters(),\n",
        "                       lr=args['lr'],\n",
        "                       betas=(args['momentum'], 0.999),\n",
        "                       weight_decay=args['weight_decay'])\n",
        "\n",
        "for state in optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.to(args['device'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVhOWUkWKU4f",
        "colab_type": "text"
      },
      "source": [
        "# Definindo a loss\n",
        "\n",
        "O subpacote [nn](https://pytorch.org/docs/stable/nn.html) possui várias funções de perda para diferentes tarefas (i.e. Cross Entropy, Negative Log Likelihood, loss L1, MSE, Kullback Leibler Divergence, etc) implementadas por padrão.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX_bmN3__LIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().to(args['device'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXhZakGZK_kU",
        "colab_type": "text"
      },
      "source": [
        "# Criando funções para Treino e Teste\n",
        "\n",
        "Iterando sobre os datasets/dataloaders de treino e teste do CIFAR10. Abaixo são implementadas a função *train()* que itera sobre os batches do dataset de treino e atualiza o modelo e a função *test()* que apenas realiza o forward dos dados de teste no modelo e calcula a acurácia no dataset de teste para o modelo no estado atual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCU5Gx9D_6xW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, net, criterion, optimizer, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for training mode.\n",
        "    net.train()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    train_loss = []\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "\n",
        "        # Obtaining images, labels and paths for batch.\n",
        "        inps, labs = batch_data\n",
        "        \n",
        "        # Casting to cuda variables.\n",
        "        inps = inps.to(args['device'])\n",
        "        labs = labs.to(args['device'])\n",
        "        \n",
        "        # Clears the gradients of optimizer.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forwarding.\n",
        "        outs = net(inps)\n",
        "\n",
        "        # Computing loss.\n",
        "        loss = criterion(outs, labs)\n",
        "\n",
        "        # Computing backpropagation.\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Updating lists.\n",
        "        train_loss.append(loss.data.item())\n",
        "    \n",
        "    toc = time.time()\n",
        "    \n",
        "    train_loss = np.asarray(train_loss)\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('[epoch %d], [train loss %.4f +/- %.4f], [training time %.2f]' % (\n",
        "        epoch, train_loss.mean(), train_loss.std(), (toc - tic)))\n",
        "    print('--------------------------------------------------------------------')\n",
        "    \n",
        "def test(test_loader, net, criterion, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for evaluation mode (not computing gradients).\n",
        "    net.eval()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    test_loss = []\n",
        "    prd_list = []\n",
        "    lab_list = []\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "\n",
        "        # Obtaining images, labels and paths for batch.\n",
        "        inps, labs = batch_data\n",
        "\n",
        "        # Casting to cuda variables.\n",
        "        inps = inps.to(args['device'])\n",
        "        labs = labs.to(args['device'])\n",
        "\n",
        "        # Forwarding.\n",
        "        outs = net(inps)\n",
        "\n",
        "        # Computing loss.\n",
        "        loss = criterion(outs, labs)\n",
        "        \n",
        "        # Obtaining predictions.\n",
        "        prds = outs.data.max(dim=1)[1].cpu().numpy()\n",
        "        \n",
        "        # Updating lists.\n",
        "        test_loss.append(loss.data.item())\n",
        "        prd_list.append(prds)\n",
        "        lab_list.append(labs.detach().cpu().numpy())\n",
        "    \n",
        "    toc = time.time()\n",
        "    \n",
        "    # Computing accuracy.\n",
        "    acc = metrics.accuracy_score(np.asarray(lab_list).ravel(),\n",
        "                                 np.asarray(prd_list).ravel())\n",
        "    \n",
        "    test_loss = np.asarray(test_loss)\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('[epoch %d], [test loss %.4f +/- %.4f], [acc %.4f], [testing time %.2f]' % (\n",
        "        epoch, test_loss.mean(), test_loss.std(), acc, (toc - tic)))\n",
        "    print('--------------------------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijo07bsTMFMs",
        "colab_type": "text"
      },
      "source": [
        "# Iterando sobre epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU2aYIob_zTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Iterating over epochs.\n",
        "for epoch in range(1, args['epoch_num'] + 1):\n",
        "\n",
        "    # Training function.\n",
        "    train(train_loader, net, criterion, optimizer, epoch)\n",
        "\n",
        "    # Computing test loss and metrics.\n",
        "    test(test_loader, net, criterion, epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}