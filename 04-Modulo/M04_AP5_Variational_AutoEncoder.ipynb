{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M04_AP5 - Variational AutoEncoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG-mVsVuE0if",
        "colab_type": "text"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports, funções, downloads e instalação do Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEHmMCjR4PJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Basic imports.\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from skimage import io\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwhRUUlc4j23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 50,      # Number of epochs.\n",
        "    'lr': 5e-4,           # Learning rate.\n",
        "    'weight_decay': 1e-5, # L2 penalty.\n",
        "    'num_workers': 3,     # Number of workers on data loader.\n",
        "    'batch_size': 100,    # Mini-batch size.\n",
        "    'print_freq': 1,      # Printing frequency.\n",
        "    'lambda_var': 1.0,    # Variational multiplier in loss.\n",
        "    'num_gauss': 20,      # Number of gaussians in bottleneck of VAE.\n",
        "    'num_samples': 8,     # Number of samples to be generated in evaluation.\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20kc9tHQ59ba",
        "colab_type": "text"
      },
      "source": [
        "# Carregando o  MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi3Zh8fQ4X_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Root directory for the dataset (to be downloaded).\n",
        "root = './'\n",
        "\n",
        "# Transformations over the dataset.\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Setting datasets and dataloaders.\n",
        "train_set = datasets.MNIST(root,\n",
        "                           train=True,\n",
        "                           download=True,\n",
        "                           transform=data_transforms)\n",
        "test_set = datasets.MNIST(root,\n",
        "                          train=False,\n",
        "                          download=False,\n",
        "                          transform=data_transforms)\n",
        "\n",
        "# Setting dataloaders.\n",
        "train_loader = DataLoader(train_set,\n",
        "                          args['batch_size'],\n",
        "                          num_workers=args['num_workers'],\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(test_set,\n",
        "                         args['batch_size'],\n",
        "                         num_workers=args['num_workers'],\n",
        "                         shuffle=False)\n",
        "\n",
        "# Printing training and testing dataset sizes.\n",
        "print('Size of training set: ' + str(len(train_set)) + ' samples')\n",
        "print('Size of test set: ' + str(len(test_set)) + ' samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drOsx-32Ifo1",
        "colab_type": "text"
      },
      "source": [
        "# AutoEncoder Variacional\n",
        "\n",
        "Idealmente codificações compactas de dados redundantes (i.e. imagens) deveriam produzir representações latentes que fossem independentes uma da outra num nível semântico. Ou seja, cada bin num feature map latente $z$ de um autoencoder deveria codificar o máximo de informação possível (i.e. linhas verticais que compõem um '1', '7' ou '9'; ou círculos que compõem um '6', '8' ou '0') para a reconstrução dos dígitos do MNIST, por exemplo. A [inferência variacional](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf) provém uma forma mais simples de computarmos o Maximum a Posteriori (MAP) de distribuições estatísticas complexas como as que estamos lidando.\n",
        "\n",
        "![VAE Features](https://www.dropbox.com/s/fkvdn69tkh7tm1p/vae_gaussian.png?dl=1)\n",
        "\n",
        "Se tivermos controle sobre representações latentes em $z$ que codificam features de algo nível semântico, podemos utilizar o Decoder de um AE para geração de novas amostras. Usando o Encoder de um AE tradicional, conseguimos partir do vetor de entrada $x$ e chegar no vetor latente $z \\sim q(z ∣ x)$. Porém, como não temos controle sobre a distribuição $q$, não é possível fazer o caminho inverso, ou seja, a partir de $z$ modelar $x \\sim p(x | z)$. Essa é a motivação para um Variational AutoEncoder (VAE).\n",
        "\n",
        "![VAE x->z](https://www.dropbox.com/s/o8daaskdrhfav7r/VAE_Enc.png?dl=1)\n",
        "\n",
        "![VAE z->x](https://www.dropbox.com/s/wqi8nsak84i11mi/VAE_Dec.png?dl=1)\n",
        "\n",
        "Para podermos ter um controle maior sobre distribuição de cada bin de $z$, adicionamos uma \"regularização\" $\\mathcal{L}_{KL}(\\mu, \\sigma)$ à loss de regressão $\\mathcal{L}_{r}(x, \\hat{x})$ de um AE tradicional. Percebe-se que $\\mu$ e $\\sigma$ devem codificar a média e o desvio padrão de distribuições gaussianas multivariadas, o que permite realizarmos uma amostragem dessa distribuição. Não podemos, porém, backpropagar de nós na nossa rede que realizem amostragem de uma distribuição. Portanto, precisamos do truque da reparametrização mostrado abaixo para backpropagarmos apenas por $\\mu$ e $\\sigma$, mas não por $\\epsilon$.\n",
        "\n",
        "![Reparametrization](https://jaan.io/images/reparametrization.png)\n",
        "\n",
        "Assim, a arquitetura final de um VAE segue o esquema a seguir composto no bottleneck por um vetor $\\mu$, um vetor $\\sigma$ e um vetor $\\epsilon$, que formam a representação latente $z = \\mu + \\sigma * \\epsilon$.\n",
        "\n",
        "![VAE training](https://www.dropbox.com/s/719vkfnfsobimmd/VAE_training.png?dl=1)\n",
        "\n",
        "A ideia é que cada gaussiana codifique uma característica de alto nível nos dados, permitindo que utilizemos o modelo generativo do VAE para, de fato, gerar amostras novas verossímeis no domínio dos dados de treino.\n",
        "\n",
        "![VAE gif](https://media.giphy.com/media/26ufgj5LH3YKO1Zlu/giphy.gif)\n",
        "\n",
        "Para entender mais sobre \"disentangled representations\", ler o paper original do [VAE](https://arxiv.org/abs/1312.6114), o [$\\beta$-VAE](https://openreview.net/references/pdf?id=Sy2fzU9gl) e o paper que propõe as [InfoGANs](https://arxiv.org/pdf/1606.03657.pdf):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os2wxWdOEKBb",
        "colab_type": "text"
      },
      "source": [
        "# Atividade Prática: Implementando o VAE\n",
        "\n",
        "1.   Defina a arquitetura do VAE. O Encoder da rede será composto de duas camadas precedendo as camadas $\\mu$ e $\\sigma$, de forma que $\\mu$ e $\\sigma$ recebam as mesmas entradas e se combinem como explicado a cima para formar o vetor latente $z$. A dimensionalidade de entrada dos dados ($784$) deve ser diminuída gradativamente até chegar no bottleneck, assim como nosso primeiro exemplo do AE Linear. Ambas camadas $\\mu$ e $\\sigma$ devem receber dados de dimensionalidade alta e codificá-los para uma saída dada pela variável *n_gaus*. Não é preciso criar uma camada explícita para $\\epsilon$, já que ele só representa a amostragem de uma distribuição gaussiana ([torch.randn()](https://pytorch.org/docs/stable/torch.html#torch.randn));\n",
        "2.   Complete a implementação dos métodos *encode()* que encapsula o forward pelo Encoder, *reparameterize()* que amostra $\\epsilon$ e realiza o truque da reparametrização e *decode()* que faz o forward de $z$ pelo Decoder da rede, o qual deve ser simétrico ao encoder, ou seja, receber *n_gaus* features vindos de $z$ e gradativamente aumentar os features para recuperar $784$ features. Dica: na função *reparameterize()*;\n",
        "3.   Defina a loss composta do VAE na função *variational_loss()*. Essa função deve retornar o componente $\\mathcal{L}_{r}$ da loss (já feito usando a BCE) e o componente $\\mathcal{L}_{KL}$. Dica: ver o Apêndice B do paper dos [VAEs](https://arxiv.org/pdf/1312.6114.pdf) para a fórmula de $\\mathcal{L}_{KL}$;\n",
        "4.   Na função *generate_2d()* altere as dimensões da tupla *dim_linspace* até achar um par de dimensões que influencie os novos samples em alto nível semântico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e27D-vk8lFgJ",
        "colab_type": "text"
      },
      "source": [
        "# Definindo a arquitetura"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y7IHYWg2NBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AutoEncoder implementation.\n",
        "class VariationalAutoEncoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_gaus):\n",
        "\n",
        "        super(VariationalAutoEncoder, self).__init__()\n",
        "        \n",
        "        self.n_gaus = n_gaus\n",
        "        \n",
        "        # TO DO: Encoder.\n",
        "        self.enc_1 = # ...\n",
        "        self.enc_2 = # ...\n",
        "        \n",
        "        # TO DO: Layers mu and sigma.\n",
        "        self.enc_mu = # ...\n",
        "        self.enc_sigma = # ...\n",
        "        \n",
        "        # TO DO: Decoder.\n",
        "        self.decoder = # ...\n",
        "        \n",
        "        self.initialize_weights()\n",
        "        \n",
        "    # TO DO: Encoding function.\n",
        "    def encode(self, x):\n",
        "        # ...\n",
        "        \n",
        "    # TO DO: Decoding function.\n",
        "    def decode(self, z):\n",
        "        # ...\n",
        "               \n",
        "    # TO DO: Reparametrization function. \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        \n",
        "        # TO DO: sample eps from gaussian.\n",
        "        eps = # ...\n",
        "        \n",
        "        # TO DO: compute z using mu, eps and std.\n",
        "        z = # ...\n",
        "        \n",
        "        # TO DO: return z.\n",
        "    \n",
        "    # Function for randomly initializing weights.\n",
        "    def initialize_weights(self):\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # TO DO: Call method encode().\n",
        "        mu, logvar = # ...\n",
        "        \n",
        "        # TO DO: Use mu and logvar to compute z in method reparameterize().\n",
        "        z = # ...\n",
        "        \n",
        "        # TO DO: Call method decode().\n",
        "        dec = #...\n",
        "        \n",
        "        return dec, mu, logvar\n",
        "\n",
        "# Instantiating architecture.\n",
        "net = VariationalAutoEncoder(args['num_gauss']).to(args['device'])\n",
        "\n",
        "# Printing architecture.\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS2l_pqAI0F2",
        "colab_type": "text"
      },
      "source": [
        "# Definindo o otimizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_-RN1wH-4bB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(net.parameters(),\n",
        "                       lr=args['lr'],\n",
        "                       weight_decay=args['weight_decay'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVhOWUkWKU4f",
        "colab_type": "text"
      },
      "source": [
        "# Definindo a loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB4XYA1VKIXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO DO: Reconstruction + KL losses summed over all elements and batch.\n",
        "def variational_loss(recon_x, x, mu, logvar):\n",
        "    \n",
        "    # Reconstruction loss using BCE.\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "\n",
        "    # TO DO: KL Divergence loss.\n",
        "    # See Appendix B from VAE paper:\n",
        "    #     https://arxiv.org/pdf/1312.6114.pdf.\n",
        "    # See Pytorch's implementation of VAEs:\n",
        "    #     https://github.com/pytorch/examples/blob/master/vae/main.py.\n",
        "    KLD = # ...\n",
        "    \n",
        "    # TO DO: return BCE and KLD."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXhZakGZK_kU",
        "colab_type": "text"
      },
      "source": [
        "# Criando funções para Treino e Teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCU5Gx9D_6xW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training procedure.\n",
        "def train(train_loader, net, optimizer, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for training mode.\n",
        "    net.train()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    train_loss = []\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "\n",
        "        # Obtaining images and labels for batch.\n",
        "        inps, labs = batch_data\n",
        "        \n",
        "        # Casting to cuda variables and reshaping.\n",
        "        inps = inps.view(inps.size(0), -1).to(args['device'])\n",
        "        \n",
        "        # Clears the gradients of optimizer.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forwarding.\n",
        "        outs, mu, logvar = net(inps)\n",
        "\n",
        "        # TO DO Computing total loss.\n",
        "        loss_bce, loss_kld = variational_loss(outs, inps, mu, logvar)\n",
        "        loss = # ...\n",
        "\n",
        "        # Computing backpropagation.\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Updating lists.\n",
        "        train_loss.append((loss_bce.data.item(),\n",
        "                           args['lambda_var'] * loss_kld.data.item(),\n",
        "                           loss.data.item()))\n",
        "    \n",
        "    toc = time.time()\n",
        "    \n",
        "    train_loss = np.asarray(train_loss)\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('[epoch %d], [train bce loss %.4f +/- %.4f], [train kld loss %.4f +/- %.4f], [training time %.2f]' % (\n",
        "        epoch, train_loss[:,0].mean(), train_loss[:,0].std(), train_loss[:,1].mean(), train_loss[:,1].std(), (toc - tic)))\n",
        "    print('-------------------------------------------------------------------')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD2mQJjjkSgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing procedure.\n",
        "def test(test_loader, net, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for evaluation mode.\n",
        "    net.eval()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    test_loss = []\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(test_loader):\n",
        "\n",
        "        # Obtaining images and labels for batch.\n",
        "        inps, labs = batch_data\n",
        "\n",
        "        # Casting to cuda variables and reshaping.\n",
        "        inps = inps.view(inps.size(0), -1).to(args['device'])\n",
        "\n",
        "        # Forwarding.\n",
        "        outs, mu, logvar = net(inps)\n",
        "\n",
        "        # Computing loss.\n",
        "        loss_bce, loss_kld = variational_loss(outs, inps, mu, logvar)\n",
        "        loss = # ...\n",
        "        \n",
        "        # Updating lists.\n",
        "        test_loss.append((loss_bce.data.item(),\n",
        "                          args['lambda_var'] * loss_kld.data.item(),\n",
        "                          loss.data.item()))\n",
        "        \n",
        "        if i == 0 and epoch % args['print_freq'] == 0:\n",
        "            \n",
        "            fig, ax = plt.subplots(2, 8, figsize=(16, 4))\n",
        "        \n",
        "        if i < 8 and epoch % args['print_freq'] == 0:\n",
        "            \n",
        "            ax[0, i].imshow(inps.view(inps.size(0), 28, 28)[0].detach().cpu().numpy())\n",
        "            ax[0, i].set_yticks([])\n",
        "            ax[0, i].set_xticks([])\n",
        "            ax[0, i].set_title('Image ' + str(i + 1))\n",
        "            \n",
        "            ax[1, i].imshow(outs.view(inps.size(0), 28, 28)[0].detach().cpu().numpy())\n",
        "            ax[1, i].set_yticks([])\n",
        "            ax[1, i].set_xticks([])\n",
        "            ax[1, i].set_title('Reconstructed ' + str(i + 1))\n",
        "            \n",
        "        if i == 8 and epoch % args['print_freq'] == 0:\n",
        "            \n",
        "            plt.show()\n",
        "    \n",
        "    toc = time.time()\n",
        "    \n",
        "    test_loss = np.asarray(test_loss)\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('[epoch %d], [test bce loss %.4f +/- %.4f], [test kld loss %.4f +/- %.4f], [testing time %.2f]' % (\n",
        "        epoch, test_loss[:,0].mean(), test_loss[:,0].std(), test_loss[:,1].mean(), test_loss[:,1].std(), (toc - tic)))\n",
        "    print('-------------------------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF-CeDq7esnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation procedure for sample generation.\n",
        "def evaluate(net, n_samples, n_gauss):\n",
        "\n",
        "    # Setting network for evaluation mode.\n",
        "    net.eval()\n",
        "    \n",
        "    # Plotting new samples generated from VAE.\n",
        "    fig, ax = plt.subplots(1, n_samples, figsize=(n_samples*2, 2))\n",
        "\n",
        "    # Iterating over batches.\n",
        "    for i in range(n_samples):\n",
        "        \n",
        "        # Sampling from Gaussian.\n",
        "        sample = torch.randn(1, n_gauss).to(args['device'])\n",
        "        \n",
        "        # Forwarding through Decoder.\n",
        "        sample = net.decode(sample).detach().cpu().view(28, 28).numpy()\n",
        "        \n",
        "        ax[i].imshow(sample)\n",
        "        ax[i].set_yticks([])\n",
        "        ax[i].set_xticks([])\n",
        "        ax[i].set_title('New Sample ' + str(i + 1))\n",
        "        \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijo07bsTMFMs",
        "colab_type": "text"
      },
      "source": [
        "# Iterando sobre epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU2aYIob_zTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Iterating over epochs.\n",
        "for epoch in range(1, args['epoch_num'] + 1):\n",
        "\n",
        "    # Training function.\n",
        "    train(train_loader, net, optimizer, epoch)\n",
        "\n",
        "    # Computing test loss and metrics.\n",
        "    test(test_loader, net, epoch)\n",
        "    \n",
        "    # Evaluating sample generation in VAE.\n",
        "    evaluate(net, args['num_samples'], args['num_gauss'])\n",
        "    \n",
        "    print('-- End of Epoch ---------------------------------------------------')\n",
        "    print('-------------------------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjZtzJTWzL_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation procedure for sample generation.\n",
        "def generate_2d(net, n_samples, n_gauss):\n",
        "\n",
        "    # Setting network for evaluation mode.\n",
        "    net.eval()\n",
        "    \n",
        "    # Creating linear space to visualize bivariate gaussian.\n",
        "    linspace_gauss = torch.linspace(-2.5, 2.5, n_samples)\n",
        "    \n",
        "    # Select Gaussian dimensions\n",
        "    dim_linspace = (0, 1)\n",
        "    \n",
        "    # Plotting.\n",
        "    fig, ax = plt.subplots(n_samples, n_samples, figsize=(20, 20))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        \n",
        "        for j in range(n_samples):\n",
        "\n",
        "            # Filling batch with size 1 and n_gauss zeros of dimension.\n",
        "            sample = torch.zeros(1, n_gauss).to(args['device'])\n",
        "            \n",
        "            # Replacing zeros in dimensions dim_linspace with values from\n",
        "            # variable linspace_gauss.\n",
        "            sample[0, dim_linspace[0]] = linspace_gauss[j]\n",
        "            sample[0, dim_linspace[1]] = linspace_gauss[i]\n",
        "\n",
        "            # Forwarding through decoder.\n",
        "            sample = net.decode(sample).detach().cpu().view(28, 28).numpy()\n",
        "\n",
        "            # Printing sample.\n",
        "            ax[j, i].imshow(sample)\n",
        "            ax[j, i].set_yticks([])\n",
        "            ax[j, i].set_xticks([])\n",
        "            ax[j, i].set_title('New Sample [' + str(j + 1) + ',' + str(i + 1) + ']')\n",
        "        \n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "generate_2d(net, args['num_samples'], args['num_gauss'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}