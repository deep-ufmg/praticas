{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M04_AP4 - Sparse AutoEncoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG-mVsVuE0if",
        "colab_type": "text"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports, funções, downloads e instalação do Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEHmMCjR4PJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Basic imports.\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from skimage import io\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwhRUUlc4j23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 10,      # Number of epochs.\n",
        "    'n_classes': 10,      # Number of classes.\n",
        "    'lr': 0.0005,         # Learning rate.\n",
        "    'weight_decay': 1e-5, # L2 penalty.\n",
        "    'num_workers': 3,     # Number of workers on data loader.\n",
        "    'batch_size': 100,    # Mini-batch size.\n",
        "    'print_freq': 1,      # Printing frequency.\n",
        "    'lambda_s': 2.0, #0.2,      # Sparsity importance in loss computation.\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20kc9tHQ59ba",
        "colab_type": "text"
      },
      "source": [
        "# Carregando o  MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi3Zh8fQ4X_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Root directory for the dataset (to be downloaded).\n",
        "root = './'\n",
        "\n",
        "# Transformations over the dataset.\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Setting datasets and dataloaders.\n",
        "train_set = datasets.MNIST(root,\n",
        "                           train=True,\n",
        "                           download=True,\n",
        "                           transform=data_transforms)\n",
        "test_set = datasets.MNIST(root,\n",
        "                          train=False,\n",
        "                          download=False,\n",
        "                          transform=data_transforms)\n",
        "\n",
        "# Random subset of training data for small data scenarios.\n",
        "# torch.random.manual_seed(12345)\n",
        "# indices = torch.randperm(len(train_set))[:500]\n",
        "# train_set = data.Subset(train_set, indices)\n",
        "\n",
        "# Setting dataloaders.\n",
        "train_loader = DataLoader(train_set,\n",
        "                          args['batch_size'],\n",
        "                          num_workers=args['num_workers'],\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(test_set,\n",
        "                         args['batch_size'],\n",
        "                         num_workers=args['num_workers'],\n",
        "                         shuffle=False)\n",
        "\n",
        "# Printing training and testing dataset sizes.\n",
        "print('Size of training set: ' + str(len(train_set)) + ' samples')\n",
        "print('Size of test set: ' + str(len(test_set)) + ' samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drOsx-32Ifo1",
        "colab_type": "text"
      },
      "source": [
        "# AutoEncoder Esparso\n",
        "\n",
        "[Regularizações](https://medium.com/datadriveninvestor/l1-l2-regularization-7f1b4fe948f2) são componentes comuns em vários métodos da área de Machine Learning que podem servir como um viés para que o algoritmo dê preferência a soluções mais simples, potencialmente prevenindo overfitting no caso de modelos **overcomplete** e/ou **small data**. A loss de um Sparse AE (SAE) adiciona um termo de regularização $\\mathcal{L}_{s}$ à loss de regressão $\\mathcal{L}_{r}$ de um AE tradicional. Dessa forma, a loss total $\\mathcal{L}_{t}$ é dada por:\n",
        "\n",
        "$\\mathcal{L}_{t}(x, \\hat{x}, z) = \\mathcal{L}_{r}(x, \\hat{x}) + \\lambda_{s} \\mathcal{L}_{s}(z).$\n",
        "\n",
        "Um AE tradicional produz features com ativações consideravelmente densas dos inputs passados a ele, já que, como o objetivo principal é reconstrução, toda informação possível deve ser mantida nas representações latentes da rede.\n",
        "\n",
        "![Dense AE](https://www.dropbox.com/s/nfiix8cfk5g9wue/Sparse_AE_1.png?dl=1)\n",
        "\n",
        "Em contraponto, SAEs produzem representações esparsas dos dados que podem ser utilizadas para realizar [**Sparse Coding**](https://en.wikipedia.org/wiki/Sparse_dictionary_learning), o que tem várias aplicações dentro da área de Machine Learning, incluindo [melhorias na performance de algumas tarefas de classificação](https://arxiv.org/pdf/1312.5663.pdf). A imagem abaixo mostra uma rede com ativações mais esparsas devido à adição de um termo de regularização $\\mathcal{L}_{s}(z)$.\n",
        "\n",
        "![Sparse AE](https://www.dropbox.com/s/rs27590a80srntp/Sparse_AE_2.png?dl=1)\n",
        "\n",
        "Para mais informações sobre **Sparse Coding** (e também outros tópicos interessantes de Machine Learning), um bom material pode ser encontrado nos seguintes vídeos:\n",
        "*   https://www.youtube.com/watch?v=7a0_iEruGoM\n",
        "*   https://www.youtube.com/watch?v=L6qhzWWtqQs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ4zzNo33Loe",
        "colab_type": "text"
      },
      "source": [
        "# Atividade Prática: modificando o AE Convolucional para criar representações esparsas\n",
        "\n",
        "1.   Crie uma função que calcule o componente $\\mathcal{L}_{s}(z)$, recebendo o output do encoder computado pelo AE Convolucional e aplique uma regularização L1 sobre ele: $\\frac{1}{N}\\sum_{i=0}^{N}{|z_{i}|}$;\n",
        "2.   Utilize o fator $\\lambda_{s}$ (*args\\['lambda_s\\']*) para criar uma loss composta $\\mathcal{L}_{t}$ nas funções *train()* e *test()* com a loss de regressão $\\mathcal{L}_{r}$ tradicional do AE, a qual já está implementada. A loss final $\\mathcal{L}_{t}$ é que deve ser usada para computar o backward nas funções de treino e teste;\n",
        "3.   Compare visualmente a densidade de ativações nos feature maps calculados no AE Convolucional denso (implementado na aula passada) e no AE Convolucional esparso.\n",
        "\n",
        "PS.: Compute $\\mathcal{L}_{s}$ usando apenas operações vetoriais, ou seja, sem o uso de um *for*, o que deixaria muito lento o algoritmo."
      ]
    }
  ]
}