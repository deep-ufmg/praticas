{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S04A01_1 - Introduction to Pytorch and Torchvision.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG-mVsVuE0if",
        "colab_type": "text"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports, funções, downloads e instalação do Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ65_lev3RXO",
        "colab_type": "code",
        "outputId": "5d300868-efeb-412c-adba-690384c21a9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "# Reinstalling torch with the right CUDA bindings.\n",
        "!pip3 install -U https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install -U https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.1.0 from https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl (770.7MB)\n",
            "\u001b[K     |████████████████████████████████| 770.7MB 29kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.1.0) (1.16.4)\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.1.0\n",
            "    Uninstalling torch-1.1.0:\n",
            "      Successfully uninstalled torch-1.1.0\n",
            "Successfully installed torch-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting torchvision==0.3.0 from https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (4.3.0)\n",
            "Requirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision==0.3.0) (0.46)\n",
            "Installing collected packages: torchvision\n",
            "  Found existing installation: torchvision 0.3.0\n",
            "    Uninstalling torchvision-0.3.0:\n",
            "      Successfully uninstalled torchvision-0.3.0\n",
            "Successfully installed torchvision-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEHmMCjR4PJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic imports.\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from skimage import io\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20kc9tHQ59ba",
        "colab_type": "text"
      },
      "source": [
        "# Sintaxe básica do Pytorch\n",
        "\n",
        "Assim como o NumPy e o MXNet, o Pytorch é uma biblioteca de processamento vetorial/matricial/tensorial. Operações sobre os tensores do Pytorch possuem sintaxe consideravelmente parecida com operações sobre tensores do NumPy e MXNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQa4-lUw7Rmp",
        "colab_type": "text"
      },
      "source": [
        "## Casting para o dispositivo correto\n",
        "\n",
        "Como usaremos processamento vetorial principalmente em GPUs para aprendizado profundo, primeiramente é possível verificar se há uma GPU disponível com o trecho de código abaixo, armazenando os tensores nos dispositivos apropriados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX0bBEM863sY",
        "colab_type": "code",
        "outputId": "cf8e80df-33cb-4eb5-9716-c98bbbed230c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Checking if GPU/CUDA is available.\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd-pMj8dUe_7",
        "colab_type": "text"
      },
      "source": [
        "## Tensores no Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUGDZvJQWrt7",
        "colab_type": "text"
      },
      "source": [
        "Criando tensores novos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsutJ9TGUeaN",
        "colab_type": "code",
        "outputId": "2d45d81d-9efb-4e82-9990-427d6daa1803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tns = torch.tensor([1, 2, 3, 4, 5, 6])\n",
        "print(tns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3, 4, 5, 6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMHSDZ11WuI9",
        "colab_type": "text"
      },
      "source": [
        "Reorganizando tensores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_ZEyjqfUm_T",
        "colab_type": "code",
        "outputId": "eb506fcd-2de4-4305-f631-e87d3d6aa5e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(tns.view(2, 3))\n",
        "\n",
        "# Function view() with -1 infers the shape according to the remaining elements.\n",
        "print(tns.view(3, -1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.4107, 0.0713, 0.1623],\n",
            "        [0.6770, 0.8527, 0.7865]])\n",
            "tensor([[0.4107, 0.0713],\n",
            "        [0.1623, 0.6770],\n",
            "        [0.8527, 0.7865]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPG9xbsmUtc0",
        "colab_type": "text"
      },
      "source": [
        "Iniciando tensores vazios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4Ho1QAtUwN4",
        "colab_type": "code",
        "outputId": "fab1f15d-a6f1-4fe5-d2c4-c4aec66e8f40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "tns_0 = torch.zeros(2, 3)\n",
        "tns_1 = torch.ones(2, 3)\n",
        "\n",
        "print(tns_0)\n",
        "print(tns_1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laGvfy36U7j5",
        "colab_type": "text"
      },
      "source": [
        "Iniciando tensores com valores aleatórios."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9STHuglU-l4",
        "colab_type": "code",
        "outputId": "0780429a-b984-44b7-8291-1f4b801bc4ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "tns_u = torch.rand(2, 3) # Flat distribution.\n",
        "print(tns_u)\n",
        "\n",
        "tns_n = torch.randn(2, 3) # Normal Distribution.\n",
        "print(tns_n)\n",
        "\n",
        "tns_perm = torch.randperm(6) # Random permutation of the interval [0, 5].\n",
        "print(tns_perm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.6267, 0.4909, 0.9100],\n",
            "        [0.3794, 0.2422, 0.1829]])\n",
            "tensor([[-1.2571,  0.9560, -0.8660],\n",
            "        [ 0.3818, -0.8611, -0.2930]])\n",
            "tensor([3, 2, 1, 4, 0, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTv86tHOVd7j",
        "colab_type": "text"
      },
      "source": [
        "Operações com tensores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeorfO9mVgHF",
        "colab_type": "code",
        "outputId": "3be3b04a-1852-42aa-aba8-1dd545c94fd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(tns_u)\n",
        "print(tns_n)\n",
        "\n",
        "tns_sum = tns_u + tns_n\n",
        "print(tns_sum)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.6267, 0.4909, 0.9100],\n",
            "        [0.3794, 0.2422, 0.1829]])\n",
            "tensor([[-1.2571,  0.9560, -0.8660],\n",
            "        [ 0.3818, -0.8611, -0.2930]])\n",
            "tensor([[-0.6304,  1.4469,  0.0440],\n",
            "        [ 0.7612, -0.6189, -0.1101]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygc7jz5ZV1tc",
        "colab_type": "text"
      },
      "source": [
        "Indexação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7D3DEWHV3sh",
        "colab_type": "code",
        "outputId": "85b35741-3032-4757-a900-810dae1faf0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(tns_sum[1, 1]) # Indexing element.\n",
        "print(tns_sum[0, :]) # Indexing line.\n",
        "print(tns_sum[:, 1]) # Indexing column."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-0.2247)\n",
            "tensor([-0.0041,  1.0504, -1.4629])\n",
            "tensor([ 1.0504, -0.2247])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcLFvWE5WIES",
        "colab_type": "text"
      },
      "source": [
        "Convertendo de tensores do numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv32OH4IWMI2",
        "colab_type": "code",
        "outputId": "9c124fb8-78cc-41ba-e077-eb9fb80e7d94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "np_arr = np.random.randn(2, 3)\n",
        "print(np_arr, np_arr.dtype)\n",
        "\n",
        "torch_tns = torch.from_numpy(np_arr)\n",
        "print(torch_tns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.79914842  0.65067953 -0.52020761]\n",
            " [-1.20479623  0.41412455  0.88032694]] float64\n",
            "tensor([[ 0.7991,  0.6507, -0.5202],\n",
            "        [-1.2048,  0.4141,  0.8803]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_Hu59yTXsf2",
        "colab_type": "text"
      },
      "source": [
        "Concatenando tensores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szn-OZAHXuCc",
        "colab_type": "code",
        "outputId": "d191643b-7287-40c1-8102-0d8f49c2b5ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(tns_u)\n",
        "print(tns_n)\n",
        "\n",
        "tns_cat = torch.cat((tns_u, tns_n), 0)\n",
        "print(tns_cat)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.6267,  0.4909,  0.9100],\n",
            "        [ 0.3794,  0.2422,  0.1829],\n",
            "        [-1.2571,  0.9560, -0.8660],\n",
            "        [ 0.3818, -0.8611, -0.2930]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi1pGoSLYMcd",
        "colab_type": "text"
      },
      "source": [
        "Várias outras operações sobre tensores do Pytorch podem ser vistas nos seguintes tutoriais:\n",
        "1.   https://jhui.github.io/2018/02/09/PyTorch-Basic-operations/\n",
        "2.   https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9LoXL0cUYMT",
        "colab_type": "text"
      },
      "source": [
        "## Treinando uma MLP simples em dados aleatórios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi3Zh8fQ4X_3",
        "colab_type": "code",
        "outputId": "3be72be3-79b1-41d5-f357-9c461a3f9610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold inputs and outputs\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.0697, -0.8655,  0.7422,  ..., -2.1555,  0.5242, -0.8380],\n",
            "        [ 0.3323, -0.4930,  2.7336,  ...,  1.0246,  0.3956, -0.5767],\n",
            "        [ 0.4844, -1.7043, -1.6217,  ...,  0.6703, -1.4023, -1.0271],\n",
            "        ...,\n",
            "        [ 0.6272,  1.1326,  1.6196,  ..., -1.2598, -0.9911, -0.3733],\n",
            "        [-0.8201, -0.4616, -1.4029,  ...,  1.6377, -0.3022, -1.7810],\n",
            "        [-0.4634, -1.0167, -0.3594,  ..., -0.4808,  1.4103, -0.5878]])\n",
            "tensor([[-6.3763e-01, -5.4006e-01, -1.3607e-01,  6.2997e-01,  2.8408e-01,\n",
            "          8.1948e-01, -2.0191e+00,  5.7331e-01, -7.9399e-01, -2.2821e-01],\n",
            "        [ 3.2706e-01, -8.3610e-02,  1.1553e-01,  5.5504e-01,  8.3848e-01,\n",
            "          8.5405e-02, -1.0547e+00,  6.8876e-01,  4.7744e-02, -8.8231e-01],\n",
            "        [-9.0068e-01, -9.6210e-01,  3.3908e-01,  8.5861e-01, -4.4938e-01,\n",
            "          5.4125e-02,  5.8993e-01,  1.8550e+00, -1.8036e-01, -1.3145e+00],\n",
            "        [ 4.7366e-01,  1.3621e+00,  1.6578e+00, -9.8601e-01, -2.4221e-02,\n",
            "         -2.6964e-01,  3.1591e-01, -2.7109e+00,  6.1796e-01,  5.2614e-01],\n",
            "        [-1.8063e-01,  6.6316e-02, -6.2939e-01, -1.1806e+00,  4.4379e-01,\n",
            "          1.7916e-01, -1.0342e+00,  2.1096e+00,  1.2053e+00,  9.1663e-01],\n",
            "        [-1.0611e+00,  1.4309e+00, -4.8923e-02,  2.3242e-03,  2.5271e-01,\n",
            "          1.8256e-01, -2.6144e-01,  1.1734e+00,  6.0494e-01,  5.1900e-01],\n",
            "        [-1.3959e+00,  5.8499e-01,  2.8393e-01, -6.9334e-03, -2.3168e-01,\n",
            "         -8.4442e-01,  6.5656e-01,  5.3749e-03,  1.4889e-01, -9.1308e-01],\n",
            "        [-1.7559e-01,  7.8651e-01, -2.0426e-01,  1.6667e+00, -2.6161e-01,\n",
            "          1.1855e+00, -2.1759e+00,  1.3620e+00, -5.2109e-01, -2.1757e-01],\n",
            "        [ 1.3533e+00, -1.1146e+00,  1.7757e-01, -4.4496e-01, -2.8082e-01,\n",
            "         -9.1812e-01, -1.3253e+00, -1.7770e-01, -2.5234e-02,  2.1840e+00],\n",
            "        [-1.1065e+00, -1.7988e-01,  1.3924e+00,  2.6216e+00,  7.8633e-01,\n",
            "          1.4859e-01,  3.7234e-01,  2.0763e-01,  1.0824e+00,  8.2573e-01],\n",
            "        [ 6.5212e-01, -5.5759e-01, -1.8469e-01, -2.5188e-01, -2.0997e-01,\n",
            "         -1.1845e+00, -4.4500e-01, -2.1695e-01, -1.8414e+00,  1.1869e+00],\n",
            "        [ 5.1011e-01, -9.7753e-03, -1.4301e-01,  1.2473e+00,  6.4790e-01,\n",
            "         -4.7707e-01, -1.4079e+00,  7.7129e-01,  1.2104e+00,  1.9245e-01],\n",
            "        [ 2.2373e+00,  1.3145e-02, -3.4194e-01, -1.6347e-01, -1.7521e+00,\n",
            "          1.6647e+00,  8.0519e-01, -1.1690e+00,  1.0750e+00,  2.4054e+00],\n",
            "        [-1.3893e+00, -1.6889e-02, -1.1115e+00,  1.0258e+00, -7.0670e-01,\n",
            "          9.4807e-01,  5.1240e-01, -4.3401e-01,  1.7417e+00, -9.8039e-01],\n",
            "        [-6.2227e-01,  3.7588e-01,  1.3437e+00,  1.5746e+00,  1.7247e+00,\n",
            "         -2.5112e+00, -1.4232e+00, -7.8712e-01, -1.1799e+00,  4.2979e-01],\n",
            "        [-2.2847e+00, -5.4384e-01, -2.0862e+00,  8.9064e-01,  5.3842e-01,\n",
            "          8.9478e-01,  2.6190e-01,  1.0554e+00, -3.5435e-01,  2.7446e-01],\n",
            "        [ 1.5157e+00,  1.3377e+00, -2.3165e-01, -1.1631e+00, -4.8990e-01,\n",
            "         -4.3023e-01,  3.6156e-02,  1.8335e-01,  1.3364e+00, -9.2480e-01],\n",
            "        [-1.2973e+00, -4.6235e-01, -1.4226e+00,  1.4248e+00, -1.7044e+00,\n",
            "         -4.7037e-01,  9.2715e-01,  1.3595e+00,  3.2512e-01, -5.9123e-01],\n",
            "        [ 3.1121e-02, -1.3208e+00, -1.2963e-01,  2.0566e-01,  9.0888e-01,\n",
            "          1.1163e+00, -5.9693e-01,  3.8427e-01, -1.6760e-01,  7.0749e-01],\n",
            "        [-1.4669e-01,  2.8914e+00,  6.0744e-01, -1.6552e+00, -3.6210e-01,\n",
            "          8.4618e-01,  7.7864e-01, -1.0109e+00,  1.2017e+00,  1.0601e+00],\n",
            "        [-5.4989e-01,  1.5280e+00,  1.7389e-01, -7.6071e-01,  5.2018e-02,\n",
            "         -9.4715e-01, -7.4486e-01, -2.8247e-01, -2.0179e-01, -1.3987e+00],\n",
            "        [ 1.5601e+00, -4.0432e-01, -6.9130e-02,  1.2302e+00,  1.7424e+00,\n",
            "          6.4515e-02,  2.5689e-01,  9.6486e-01,  4.1392e-01,  1.9100e-01],\n",
            "        [-1.3828e+00,  1.5931e+00, -8.7781e-01, -6.2507e-01,  9.7717e-01,\n",
            "         -8.5708e-01, -1.0633e+00,  7.2364e-01, -2.1544e-01,  1.0552e+00],\n",
            "        [ 7.9283e-01, -7.5335e-01,  2.5075e+00,  1.6775e+00, -7.6432e-01,\n",
            "         -5.9673e-01, -4.0859e-01,  7.2176e-01, -2.1494e-01, -9.6649e-01],\n",
            "        [ 2.8844e-01,  1.6459e-02,  1.9320e-01,  4.0658e-01,  2.0277e+00,\n",
            "          1.2315e-01,  3.9584e-01,  2.4842e-01, -1.9921e+00,  7.0230e-01],\n",
            "        [ 2.2176e-01,  1.0851e+00, -2.5058e-01,  6.6704e-01,  2.9775e-01,\n",
            "          2.6440e-01,  1.4826e-01,  8.4621e-01, -5.6675e-01,  2.7453e+00],\n",
            "        [-6.9381e-03,  1.7367e+00,  1.0919e+00,  1.0248e+00, -1.8362e+00,\n",
            "         -2.6682e-01,  4.6293e-01,  1.1937e-01,  2.9564e-01,  7.4860e-01],\n",
            "        [-2.8605e-01, -1.3103e-01,  3.3538e-01,  1.4713e+00,  6.5835e-01,\n",
            "         -1.0539e+00, -1.0478e+00, -4.6538e-01,  2.8893e-01, -5.5436e-01],\n",
            "        [-4.2933e-01,  1.3362e-01,  1.8109e-01, -1.1463e-02, -4.2135e-01,\n",
            "         -3.3496e-01, -3.8935e-01, -3.4477e-01,  3.1833e-01, -4.1436e-01],\n",
            "        [-1.4401e-01,  1.9617e+00, -1.6333e+00, -7.0410e-01,  1.4130e-01,\n",
            "         -6.4982e-01, -1.7893e+00,  3.0628e+00,  1.9840e+00,  6.9526e-01],\n",
            "        [-1.4241e+00,  1.5181e+00,  1.4702e+00, -6.6386e-01,  1.6192e-01,\n",
            "         -5.6496e-01,  5.4738e-02,  1.4896e+00,  7.1629e-01, -1.2607e+00],\n",
            "        [-1.7054e-01,  1.2231e-01, -3.5206e-03,  5.5751e-01, -6.9563e-01,\n",
            "          7.4521e-01,  1.7310e-02, -8.4390e-02, -6.2298e-01, -1.0796e+00],\n",
            "        [ 8.6027e-01,  1.5840e+00,  1.4429e+00,  2.8399e-01,  4.7429e-01,\n",
            "         -7.4857e-01,  7.3342e-02,  2.3326e+00, -1.1126e+00, -8.7374e-01],\n",
            "        [ 7.2909e-01, -1.1042e+00, -1.2841e+00, -1.0054e+00,  7.7851e-01,\n",
            "         -1.3029e+00, -3.2782e-01, -4.4878e-02, -1.2614e+00, -7.3394e-01],\n",
            "        [-1.2227e+00, -9.8827e-01, -5.6966e-02, -1.1504e+00,  1.3841e+00,\n",
            "         -6.2816e-01,  7.1488e-01,  1.0476e+00,  2.7608e+00,  2.3362e-01],\n",
            "        [ 3.7731e-01, -1.0052e+00,  2.0911e-01,  1.7370e+00, -6.7606e-01,\n",
            "          2.2879e+00, -8.3651e-01, -3.5212e-01,  1.0291e+00, -9.3053e-01],\n",
            "        [-2.6897e-01,  5.5481e-01, -4.4483e-01,  8.4030e-01,  4.3415e-01,\n",
            "          5.8062e-01, -8.7640e-01, -1.6401e+00, -3.9073e-01, -5.1884e-01],\n",
            "        [-1.1425e+00,  1.9034e-01,  1.4258e+00, -7.0364e-01, -2.0851e-01,\n",
            "         -1.5152e+00,  6.5339e-01, -7.4684e-01,  1.2505e+00,  6.4159e-02],\n",
            "        [ 2.5575e-01, -4.6173e-01,  5.0005e-01,  2.6393e-01, -1.0464e+00,\n",
            "          1.2650e+00, -9.1969e-01, -1.9694e+00,  8.3617e-02, -1.1805e+00],\n",
            "        [ 1.2127e+00, -7.1259e-02, -1.1850e+00,  7.3812e-01,  1.3751e+00,\n",
            "          1.4280e+00,  2.3364e-01, -2.2176e-02,  1.0361e+00, -2.4116e-01],\n",
            "        [ 1.8860e+00,  6.5381e-02, -1.2442e-01, -2.3531e-01,  8.4005e-01,\n",
            "         -1.8621e+00, -1.3617e+00,  1.3239e+00, -5.6774e-01, -6.2265e-01],\n",
            "        [-4.0046e-01, -1.3829e-01,  8.2694e-01, -3.7163e-01, -8.1446e-01,\n",
            "          1.2932e+00,  3.3478e-01,  2.4793e-01, -3.3524e-02, -8.1620e-01],\n",
            "        [-1.6545e+00, -9.1694e-01, -1.2990e+00,  4.1901e-01,  1.0670e+00,\n",
            "          1.8418e+00,  4.6122e-02, -1.3263e-01,  8.9247e-01,  1.1973e+00],\n",
            "        [-4.4468e-02, -5.4284e-01, -9.0148e-01,  6.0029e-01,  1.1768e-02,\n",
            "         -2.2065e+00, -9.3684e-01,  4.0979e-01,  2.2019e+00, -2.5449e-01],\n",
            "        [ 2.9674e-01,  2.6983e+00,  9.3697e-01, -1.3037e+00,  2.1321e-01,\n",
            "         -2.0378e+00, -7.6862e-01, -3.2534e-01,  5.7311e-01, -6.8012e-01],\n",
            "        [-1.5874e+00, -3.4331e-01,  1.1626e+00, -7.6065e-01,  1.3175e+00,\n",
            "         -2.0701e-01,  6.9760e-01, -5.0462e-01,  2.4101e-01,  1.1618e+00],\n",
            "        [ 2.5035e-01,  8.7399e-01, -8.2109e-01,  8.1520e-01, -1.0504e+00,\n",
            "         -3.1508e-01,  2.2273e+00,  3.2107e-01, -4.6411e-01,  4.3795e-01],\n",
            "        [ 4.4021e-01,  3.8287e-01, -7.0688e-01,  1.0066e+00, -4.9995e-01,\n",
            "          4.3838e-01, -3.6822e-01, -6.2071e-02,  6.2588e-01,  2.0066e-01],\n",
            "        [ 5.3549e-02, -1.3557e+00, -1.2170e+00, -1.5417e+00, -1.9371e+00,\n",
            "          4.4751e-01,  7.8700e-01, -5.7936e-01,  1.3977e+00, -1.7841e+00],\n",
            "        [ 2.4393e-01,  8.4301e-02,  1.1860e+00,  1.5522e+00, -1.6091e+00,\n",
            "          6.2376e-01,  9.3136e-01,  4.0594e-01,  3.3291e-01, -1.8546e+00],\n",
            "        [ 4.7242e-01, -6.2983e-01, -1.4253e+00,  5.1304e-01, -4.5582e-01,\n",
            "          7.5290e-01,  1.4709e-01,  8.5004e-01,  7.0222e-01, -5.1146e-01],\n",
            "        [-5.0610e-02,  2.3791e-01, -1.1804e+00, -1.8809e+00, -3.6329e-01,\n",
            "         -1.6349e-01, -3.1868e-01, -1.6366e+00, -9.1385e-01,  1.2349e+00],\n",
            "        [-6.7123e-01,  7.0642e-01, -4.1029e-02,  1.0810e-01, -6.5906e-01,\n",
            "          2.1265e-01,  1.3929e-01,  4.8833e-01,  4.5893e-01,  1.5516e+00],\n",
            "        [ 7.4117e-01,  3.0112e-01,  6.7195e-02,  1.0510e+00, -4.4007e-01,\n",
            "         -1.8593e-01, -2.3144e+00,  9.4747e-01,  1.4746e+00, -2.4373e-01],\n",
            "        [ 1.5386e+00,  1.0015e+00,  4.5785e-01, -2.4516e-03, -1.3121e-01,\n",
            "         -8.5553e-01, -2.0896e+00,  3.8031e-02,  4.3947e-01,  3.8762e-01],\n",
            "        [-1.3736e+00,  8.4042e-01,  2.3288e-01, -2.3449e+00, -1.9653e+00,\n",
            "         -2.7556e-01, -7.8319e-01,  9.9432e-01,  1.4047e+00, -3.1109e-01],\n",
            "        [ 2.6277e-01,  1.0080e+00, -4.2447e-01,  7.6914e-01, -1.4664e+00,\n",
            "          1.4826e+00,  2.6405e-01, -2.9905e-01, -4.3714e-01,  8.4327e-02],\n",
            "        [-6.3121e-01,  2.6149e-01,  1.4144e-01,  8.7054e-01, -7.0846e-01,\n",
            "          1.0261e+00, -8.5643e-01,  1.1179e+00, -1.2855e+00,  1.0883e+00],\n",
            "        [-2.1337e-02, -2.0395e+00, -4.5344e-01,  1.2789e-01, -5.2137e-01,\n",
            "         -2.5659e+00, -1.1221e+00, -1.9699e-01,  6.5458e-01, -1.1074e-01],\n",
            "        [ 5.1256e-01, -7.0564e-02, -3.1004e+00, -2.6170e+00,  1.1903e+00,\n",
            "         -1.6934e+00, -1.3506e-01,  5.7757e-01,  3.0141e-01,  1.0366e+00],\n",
            "        [-5.6666e-01,  3.5592e-01, -1.1913e+00, -7.4841e-01, -4.0390e-01,\n",
            "          1.1792e+00, -5.2951e-02,  4.7389e-01, -5.0013e-01,  2.1978e-01],\n",
            "        [ 6.1595e-01,  9.8110e-01, -5.5013e-01,  7.5008e-01,  1.1648e-01,\n",
            "         -5.0563e-01, -1.4918e+00,  8.9134e-01, -3.1980e-01,  3.4581e-01],\n",
            "        [-4.4881e-01,  2.3481e+00,  1.4647e-01,  5.5180e-01, -1.5762e+00,\n",
            "          6.6442e-01,  2.4381e+00,  5.5692e-01,  9.4286e-01, -4.9240e-01],\n",
            "        [ 1.0553e+00, -1.2963e+00, -2.1763e-01,  9.4452e-01,  5.6138e-01,\n",
            "          7.4296e-01, -6.5224e-01, -2.4934e-01, -1.8515e+00, -7.0582e-01]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6lK3VO99mSj",
        "colab_type": "code",
        "outputId": "490dd5f9-1a5b-4154-e2ec-427d26f5af24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Casting tensors to the appropriate device.\n",
        "x = x.to(device)\n",
        "y = y.to(device)\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.0697, -0.8655,  0.7422,  ..., -2.1555,  0.5242, -0.8380],\n",
            "        [ 0.3323, -0.4930,  2.7336,  ...,  1.0246,  0.3956, -0.5767],\n",
            "        [ 0.4844, -1.7043, -1.6217,  ...,  0.6703, -1.4023, -1.0271],\n",
            "        ...,\n",
            "        [ 0.6272,  1.1326,  1.6196,  ..., -1.2598, -0.9911, -0.3733],\n",
            "        [-0.8201, -0.4616, -1.4029,  ...,  1.6377, -0.3022, -1.7810],\n",
            "        [-0.4634, -1.0167, -0.3594,  ..., -0.4808,  1.4103, -0.5878]],\n",
            "       device='cuda:0')\n",
            "tensor([[-6.3763e-01, -5.4006e-01, -1.3607e-01,  6.2997e-01,  2.8408e-01,\n",
            "          8.1948e-01, -2.0191e+00,  5.7331e-01, -7.9399e-01, -2.2821e-01],\n",
            "        [ 3.2706e-01, -8.3610e-02,  1.1553e-01,  5.5504e-01,  8.3848e-01,\n",
            "          8.5405e-02, -1.0547e+00,  6.8876e-01,  4.7744e-02, -8.8231e-01],\n",
            "        [-9.0068e-01, -9.6210e-01,  3.3908e-01,  8.5861e-01, -4.4938e-01,\n",
            "          5.4125e-02,  5.8993e-01,  1.8550e+00, -1.8036e-01, -1.3145e+00],\n",
            "        [ 4.7366e-01,  1.3621e+00,  1.6578e+00, -9.8601e-01, -2.4221e-02,\n",
            "         -2.6964e-01,  3.1591e-01, -2.7109e+00,  6.1796e-01,  5.2614e-01],\n",
            "        [-1.8063e-01,  6.6316e-02, -6.2939e-01, -1.1806e+00,  4.4379e-01,\n",
            "          1.7916e-01, -1.0342e+00,  2.1096e+00,  1.2053e+00,  9.1663e-01],\n",
            "        [-1.0611e+00,  1.4309e+00, -4.8923e-02,  2.3242e-03,  2.5271e-01,\n",
            "          1.8256e-01, -2.6144e-01,  1.1734e+00,  6.0494e-01,  5.1900e-01],\n",
            "        [-1.3959e+00,  5.8499e-01,  2.8393e-01, -6.9334e-03, -2.3168e-01,\n",
            "         -8.4442e-01,  6.5656e-01,  5.3749e-03,  1.4889e-01, -9.1308e-01],\n",
            "        [-1.7559e-01,  7.8651e-01, -2.0426e-01,  1.6667e+00, -2.6161e-01,\n",
            "          1.1855e+00, -2.1759e+00,  1.3620e+00, -5.2109e-01, -2.1757e-01],\n",
            "        [ 1.3533e+00, -1.1146e+00,  1.7757e-01, -4.4496e-01, -2.8082e-01,\n",
            "         -9.1812e-01, -1.3253e+00, -1.7770e-01, -2.5234e-02,  2.1840e+00],\n",
            "        [-1.1065e+00, -1.7988e-01,  1.3924e+00,  2.6216e+00,  7.8633e-01,\n",
            "          1.4859e-01,  3.7234e-01,  2.0763e-01,  1.0824e+00,  8.2573e-01],\n",
            "        [ 6.5212e-01, -5.5759e-01, -1.8469e-01, -2.5188e-01, -2.0997e-01,\n",
            "         -1.1845e+00, -4.4500e-01, -2.1695e-01, -1.8414e+00,  1.1869e+00],\n",
            "        [ 5.1011e-01, -9.7753e-03, -1.4301e-01,  1.2473e+00,  6.4790e-01,\n",
            "         -4.7707e-01, -1.4079e+00,  7.7129e-01,  1.2104e+00,  1.9245e-01],\n",
            "        [ 2.2373e+00,  1.3145e-02, -3.4194e-01, -1.6347e-01, -1.7521e+00,\n",
            "          1.6647e+00,  8.0519e-01, -1.1690e+00,  1.0750e+00,  2.4054e+00],\n",
            "        [-1.3893e+00, -1.6889e-02, -1.1115e+00,  1.0258e+00, -7.0670e-01,\n",
            "          9.4807e-01,  5.1240e-01, -4.3401e-01,  1.7417e+00, -9.8039e-01],\n",
            "        [-6.2227e-01,  3.7588e-01,  1.3437e+00,  1.5746e+00,  1.7247e+00,\n",
            "         -2.5112e+00, -1.4232e+00, -7.8712e-01, -1.1799e+00,  4.2979e-01],\n",
            "        [-2.2847e+00, -5.4384e-01, -2.0862e+00,  8.9064e-01,  5.3842e-01,\n",
            "          8.9478e-01,  2.6190e-01,  1.0554e+00, -3.5435e-01,  2.7446e-01],\n",
            "        [ 1.5157e+00,  1.3377e+00, -2.3165e-01, -1.1631e+00, -4.8990e-01,\n",
            "         -4.3023e-01,  3.6156e-02,  1.8335e-01,  1.3364e+00, -9.2480e-01],\n",
            "        [-1.2973e+00, -4.6235e-01, -1.4226e+00,  1.4248e+00, -1.7044e+00,\n",
            "         -4.7037e-01,  9.2715e-01,  1.3595e+00,  3.2512e-01, -5.9123e-01],\n",
            "        [ 3.1121e-02, -1.3208e+00, -1.2963e-01,  2.0566e-01,  9.0888e-01,\n",
            "          1.1163e+00, -5.9693e-01,  3.8427e-01, -1.6760e-01,  7.0749e-01],\n",
            "        [-1.4669e-01,  2.8914e+00,  6.0744e-01, -1.6552e+00, -3.6210e-01,\n",
            "          8.4618e-01,  7.7864e-01, -1.0109e+00,  1.2017e+00,  1.0601e+00],\n",
            "        [-5.4989e-01,  1.5280e+00,  1.7389e-01, -7.6071e-01,  5.2018e-02,\n",
            "         -9.4715e-01, -7.4486e-01, -2.8247e-01, -2.0179e-01, -1.3987e+00],\n",
            "        [ 1.5601e+00, -4.0432e-01, -6.9130e-02,  1.2302e+00,  1.7424e+00,\n",
            "          6.4515e-02,  2.5689e-01,  9.6486e-01,  4.1392e-01,  1.9100e-01],\n",
            "        [-1.3828e+00,  1.5931e+00, -8.7781e-01, -6.2507e-01,  9.7717e-01,\n",
            "         -8.5708e-01, -1.0633e+00,  7.2364e-01, -2.1544e-01,  1.0552e+00],\n",
            "        [ 7.9283e-01, -7.5335e-01,  2.5075e+00,  1.6775e+00, -7.6432e-01,\n",
            "         -5.9673e-01, -4.0859e-01,  7.2176e-01, -2.1494e-01, -9.6649e-01],\n",
            "        [ 2.8844e-01,  1.6459e-02,  1.9320e-01,  4.0658e-01,  2.0277e+00,\n",
            "          1.2315e-01,  3.9584e-01,  2.4842e-01, -1.9921e+00,  7.0230e-01],\n",
            "        [ 2.2176e-01,  1.0851e+00, -2.5058e-01,  6.6704e-01,  2.9775e-01,\n",
            "          2.6440e-01,  1.4826e-01,  8.4621e-01, -5.6675e-01,  2.7453e+00],\n",
            "        [-6.9381e-03,  1.7367e+00,  1.0919e+00,  1.0248e+00, -1.8362e+00,\n",
            "         -2.6682e-01,  4.6293e-01,  1.1937e-01,  2.9564e-01,  7.4860e-01],\n",
            "        [-2.8605e-01, -1.3103e-01,  3.3538e-01,  1.4713e+00,  6.5835e-01,\n",
            "         -1.0539e+00, -1.0478e+00, -4.6538e-01,  2.8893e-01, -5.5436e-01],\n",
            "        [-4.2933e-01,  1.3362e-01,  1.8109e-01, -1.1463e-02, -4.2135e-01,\n",
            "         -3.3496e-01, -3.8935e-01, -3.4477e-01,  3.1833e-01, -4.1436e-01],\n",
            "        [-1.4401e-01,  1.9617e+00, -1.6333e+00, -7.0410e-01,  1.4130e-01,\n",
            "         -6.4982e-01, -1.7893e+00,  3.0628e+00,  1.9840e+00,  6.9526e-01],\n",
            "        [-1.4241e+00,  1.5181e+00,  1.4702e+00, -6.6386e-01,  1.6192e-01,\n",
            "         -5.6496e-01,  5.4738e-02,  1.4896e+00,  7.1629e-01, -1.2607e+00],\n",
            "        [-1.7054e-01,  1.2231e-01, -3.5206e-03,  5.5751e-01, -6.9563e-01,\n",
            "          7.4521e-01,  1.7310e-02, -8.4390e-02, -6.2298e-01, -1.0796e+00],\n",
            "        [ 8.6027e-01,  1.5840e+00,  1.4429e+00,  2.8399e-01,  4.7429e-01,\n",
            "         -7.4857e-01,  7.3342e-02,  2.3326e+00, -1.1126e+00, -8.7374e-01],\n",
            "        [ 7.2909e-01, -1.1042e+00, -1.2841e+00, -1.0054e+00,  7.7851e-01,\n",
            "         -1.3029e+00, -3.2782e-01, -4.4878e-02, -1.2614e+00, -7.3394e-01],\n",
            "        [-1.2227e+00, -9.8827e-01, -5.6966e-02, -1.1504e+00,  1.3841e+00,\n",
            "         -6.2816e-01,  7.1488e-01,  1.0476e+00,  2.7608e+00,  2.3362e-01],\n",
            "        [ 3.7731e-01, -1.0052e+00,  2.0911e-01,  1.7370e+00, -6.7606e-01,\n",
            "          2.2879e+00, -8.3651e-01, -3.5212e-01,  1.0291e+00, -9.3053e-01],\n",
            "        [-2.6897e-01,  5.5481e-01, -4.4483e-01,  8.4030e-01,  4.3415e-01,\n",
            "          5.8062e-01, -8.7640e-01, -1.6401e+00, -3.9073e-01, -5.1884e-01],\n",
            "        [-1.1425e+00,  1.9034e-01,  1.4258e+00, -7.0364e-01, -2.0851e-01,\n",
            "         -1.5152e+00,  6.5339e-01, -7.4684e-01,  1.2505e+00,  6.4159e-02],\n",
            "        [ 2.5575e-01, -4.6173e-01,  5.0005e-01,  2.6393e-01, -1.0464e+00,\n",
            "          1.2650e+00, -9.1969e-01, -1.9694e+00,  8.3617e-02, -1.1805e+00],\n",
            "        [ 1.2127e+00, -7.1259e-02, -1.1850e+00,  7.3812e-01,  1.3751e+00,\n",
            "          1.4280e+00,  2.3364e-01, -2.2176e-02,  1.0361e+00, -2.4116e-01],\n",
            "        [ 1.8860e+00,  6.5381e-02, -1.2442e-01, -2.3531e-01,  8.4005e-01,\n",
            "         -1.8621e+00, -1.3617e+00,  1.3239e+00, -5.6774e-01, -6.2265e-01],\n",
            "        [-4.0046e-01, -1.3829e-01,  8.2694e-01, -3.7163e-01, -8.1446e-01,\n",
            "          1.2932e+00,  3.3478e-01,  2.4793e-01, -3.3524e-02, -8.1620e-01],\n",
            "        [-1.6545e+00, -9.1694e-01, -1.2990e+00,  4.1901e-01,  1.0670e+00,\n",
            "          1.8418e+00,  4.6122e-02, -1.3263e-01,  8.9247e-01,  1.1973e+00],\n",
            "        [-4.4468e-02, -5.4284e-01, -9.0148e-01,  6.0029e-01,  1.1768e-02,\n",
            "         -2.2065e+00, -9.3684e-01,  4.0979e-01,  2.2019e+00, -2.5449e-01],\n",
            "        [ 2.9674e-01,  2.6983e+00,  9.3697e-01, -1.3037e+00,  2.1321e-01,\n",
            "         -2.0378e+00, -7.6862e-01, -3.2534e-01,  5.7311e-01, -6.8012e-01],\n",
            "        [-1.5874e+00, -3.4331e-01,  1.1626e+00, -7.6065e-01,  1.3175e+00,\n",
            "         -2.0701e-01,  6.9760e-01, -5.0462e-01,  2.4101e-01,  1.1618e+00],\n",
            "        [ 2.5035e-01,  8.7399e-01, -8.2109e-01,  8.1520e-01, -1.0504e+00,\n",
            "         -3.1508e-01,  2.2273e+00,  3.2107e-01, -4.6411e-01,  4.3795e-01],\n",
            "        [ 4.4021e-01,  3.8287e-01, -7.0688e-01,  1.0066e+00, -4.9995e-01,\n",
            "          4.3838e-01, -3.6822e-01, -6.2071e-02,  6.2588e-01,  2.0066e-01],\n",
            "        [ 5.3549e-02, -1.3557e+00, -1.2170e+00, -1.5417e+00, -1.9371e+00,\n",
            "          4.4751e-01,  7.8700e-01, -5.7936e-01,  1.3977e+00, -1.7841e+00],\n",
            "        [ 2.4393e-01,  8.4301e-02,  1.1860e+00,  1.5522e+00, -1.6091e+00,\n",
            "          6.2376e-01,  9.3136e-01,  4.0594e-01,  3.3291e-01, -1.8546e+00],\n",
            "        [ 4.7242e-01, -6.2983e-01, -1.4253e+00,  5.1304e-01, -4.5582e-01,\n",
            "          7.5290e-01,  1.4709e-01,  8.5004e-01,  7.0222e-01, -5.1146e-01],\n",
            "        [-5.0610e-02,  2.3791e-01, -1.1804e+00, -1.8809e+00, -3.6329e-01,\n",
            "         -1.6349e-01, -3.1868e-01, -1.6366e+00, -9.1385e-01,  1.2349e+00],\n",
            "        [-6.7123e-01,  7.0642e-01, -4.1029e-02,  1.0810e-01, -6.5906e-01,\n",
            "          2.1265e-01,  1.3929e-01,  4.8833e-01,  4.5893e-01,  1.5516e+00],\n",
            "        [ 7.4117e-01,  3.0112e-01,  6.7195e-02,  1.0510e+00, -4.4007e-01,\n",
            "         -1.8593e-01, -2.3144e+00,  9.4747e-01,  1.4746e+00, -2.4373e-01],\n",
            "        [ 1.5386e+00,  1.0015e+00,  4.5785e-01, -2.4516e-03, -1.3121e-01,\n",
            "         -8.5553e-01, -2.0896e+00,  3.8031e-02,  4.3947e-01,  3.8762e-01],\n",
            "        [-1.3736e+00,  8.4042e-01,  2.3288e-01, -2.3449e+00, -1.9653e+00,\n",
            "         -2.7556e-01, -7.8319e-01,  9.9432e-01,  1.4047e+00, -3.1109e-01],\n",
            "        [ 2.6277e-01,  1.0080e+00, -4.2447e-01,  7.6914e-01, -1.4664e+00,\n",
            "          1.4826e+00,  2.6405e-01, -2.9905e-01, -4.3714e-01,  8.4327e-02],\n",
            "        [-6.3121e-01,  2.6149e-01,  1.4144e-01,  8.7054e-01, -7.0846e-01,\n",
            "          1.0261e+00, -8.5643e-01,  1.1179e+00, -1.2855e+00,  1.0883e+00],\n",
            "        [-2.1337e-02, -2.0395e+00, -4.5344e-01,  1.2789e-01, -5.2137e-01,\n",
            "         -2.5659e+00, -1.1221e+00, -1.9699e-01,  6.5458e-01, -1.1074e-01],\n",
            "        [ 5.1256e-01, -7.0564e-02, -3.1004e+00, -2.6170e+00,  1.1903e+00,\n",
            "         -1.6934e+00, -1.3506e-01,  5.7757e-01,  3.0141e-01,  1.0366e+00],\n",
            "        [-5.6666e-01,  3.5592e-01, -1.1913e+00, -7.4841e-01, -4.0390e-01,\n",
            "          1.1792e+00, -5.2951e-02,  4.7389e-01, -5.0013e-01,  2.1978e-01],\n",
            "        [ 6.1595e-01,  9.8110e-01, -5.5013e-01,  7.5008e-01,  1.1648e-01,\n",
            "         -5.0563e-01, -1.4918e+00,  8.9134e-01, -3.1980e-01,  3.4581e-01],\n",
            "        [-4.4881e-01,  2.3481e+00,  1.4647e-01,  5.5180e-01, -1.5762e+00,\n",
            "          6.6442e-01,  2.4381e+00,  5.5692e-01,  9.4286e-01, -4.9240e-01],\n",
            "        [ 1.0553e+00, -1.2963e+00, -2.1763e-01,  9.4452e-01,  5.6138e-01,\n",
            "          7.4296e-01, -6.5224e-01, -2.4934e-01, -1.8515e+00, -7.0582e-01]],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsSQ00w69h6K",
        "colab_type": "code",
        "outputId": "e2ad01fa-9101-4bca-8010-420e8d417cc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Printing sizes of tensors.\n",
        "print(x.size())\n",
        "print(y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 1000])\n",
            "torch.Size([64, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMDN1viW-0Eg",
        "colab_type": "text"
      },
      "source": [
        "## Definindo arquitetura, loss e otimizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYO7HWC29Ahy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the nn package to define our model.\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(D_in, H),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(H, D_out),\n",
        ").to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T83-DZW-Mu_",
        "colab_type": "code",
        "outputId": "59201576-ba24-4695-e161-672939d6597f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=1000, out_features=100, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oQ2T8jm9BE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the nn package to define our loss function.\n",
        "loss_fn = nn.MSELoss(reduction='sum').to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa5DcYBf82iD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the optim package to define an Optimizer that will update the weights of\n",
        "# the model for us. Here we will use Adam; the optim package contains many other\n",
        "# optimization algoriths. The first argument to the Adam constructor tells the\n",
        "# optimizer which Tensors it should update.\n",
        "learning_rate = 1e-4\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPQtOnNr-kAG",
        "colab_type": "text"
      },
      "source": [
        "## Minimizando o erro entre $f(x)$ e $y$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsMFRIDv80I3",
        "colab_type": "code",
        "outputId": "a613becc-effb-4495-db29-2e3324cefab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "# Creating list of losses for each epoch.\n",
        "loss_list = []\n",
        "\n",
        "# Iterating over epochs.\n",
        "for epoch in range(500):\n",
        "    \n",
        "    # Forward pass: compute predicted y by passing x to the model.\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    \n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print('Epoch ' + str(epoch + 1) + ': loss = ' + str(loss.item()))\n",
        "    \n",
        "    # Updating list of losses for printing.\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "    # Before the backward pass, use the optimizer object to zero all of the\n",
        "    # gradients for the variables it will update (which are the learnable\n",
        "    # weights of the model). This is because by default, gradients are\n",
        "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to model\n",
        "    # parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Calling the step function on an Optimizer makes an update to its\n",
        "    # parameters\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 10: loss = 370.6188049316406\n",
            "Epoch 20: loss = 207.9576873779297\n",
            "Epoch 30: loss = 112.93785858154297\n",
            "Epoch 40: loss = 59.65734100341797\n",
            "Epoch 50: loss = 31.452808380126953\n",
            "Epoch 60: loss = 16.884960174560547\n",
            "Epoch 70: loss = 9.29613208770752\n",
            "Epoch 80: loss = 5.238800048828125\n",
            "Epoch 90: loss = 3.015277862548828\n",
            "Epoch 100: loss = 1.7727394104003906\n",
            "Epoch 110: loss = 1.0639944076538086\n",
            "Epoch 120: loss = 0.6521657705307007\n",
            "Epoch 130: loss = 0.4077293276786804\n",
            "Epoch 140: loss = 0.259786456823349\n",
            "Epoch 150: loss = 0.1682613641023636\n",
            "Epoch 160: loss = 0.110488660633564\n",
            "Epoch 170: loss = 0.07349345088005066\n",
            "Epoch 180: loss = 0.04938848316669464\n",
            "Epoch 190: loss = 0.03349599242210388\n",
            "Epoch 200: loss = 0.022927353158593178\n",
            "Epoch 210: loss = 0.015819432213902473\n",
            "Epoch 220: loss = 0.010989188216626644\n",
            "Epoch 230: loss = 0.0076801045797765255\n",
            "Epoch 240: loss = 0.005396471358835697\n",
            "Epoch 250: loss = 0.003809090470895171\n",
            "Epoch 260: loss = 0.0026997076347470284\n",
            "Epoch 270: loss = 0.001920543727464974\n",
            "Epoch 280: loss = 0.001371354446746409\n",
            "Epoch 290: loss = 0.0009821084095165133\n",
            "Epoch 300: loss = 0.0007052923901937902\n",
            "Epoch 310: loss = 0.0005077112000435591\n",
            "Epoch 320: loss = 0.0003662763047032058\n",
            "Epoch 330: loss = 0.0002647986984811723\n",
            "Epoch 340: loss = 0.00019185432756785303\n",
            "Epoch 350: loss = 0.0001392400881741196\n",
            "Epoch 360: loss = 0.00010120739170815796\n",
            "Epoch 370: loss = 7.366432691924274e-05\n",
            "Epoch 380: loss = 5.368022539187223e-05\n",
            "Epoch 390: loss = 3.9160571759566665e-05\n",
            "Epoch 400: loss = 2.8598606149898842e-05\n",
            "Epoch 410: loss = 2.0906458303215913e-05\n",
            "Epoch 420: loss = 1.52957763930317e-05\n",
            "Epoch 430: loss = 1.1201895176782273e-05\n",
            "Epoch 440: loss = 8.208230610762257e-06\n",
            "Epoch 450: loss = 6.018371095706243e-06\n",
            "Epoch 460: loss = 4.417041964188684e-06\n",
            "Epoch 470: loss = 3.243390892748721e-06\n",
            "Epoch 480: loss = 2.3829697965993546e-06\n",
            "Epoch 490: loss = 1.7517447759018978e-06\n",
            "Epoch 500: loss = 1.2885575415566564e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toQyqq98-68X",
        "colab_type": "code",
        "outputId": "66a1c4bb-94e0-43d0-8653-dfcf1beb085d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        }
      },
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "\n",
        "ax.plot(np.asarray(loss_list))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6gAAAHVCAYAAAD8YtYeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngoAKAABATxBQAQAA6An/PyJdrAZJA65SAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1152x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}