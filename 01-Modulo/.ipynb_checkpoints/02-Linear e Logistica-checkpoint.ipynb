{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado Profundo - UFMG\n",
    "\n",
    "## Preâmbulo\n",
    "\n",
    "O código abaixo consiste dos imports comuns. Além do mais, configuramos as imagens para ficar de um tamanho aceitável e criamos algumas funções auxiliares. No geral, você pode ignorar a próxima célula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mxnet-cu100==1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf8\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['figure.figsize']  = (18, 10)\n",
    "plt.rcParams['axes.labelsize']  = 20\n",
    "plt.rcParams['axes.titlesize']  = 20\n",
    "plt.rcParams['legend.fontsize'] = 20\n",
    "plt.rcParams['xtick.labelsize'] = 20\n",
    "plt.rcParams['ytick.labelsize'] = 20\n",
    "plt.rcParams['lines.linewidth'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams['figure.figsize']  = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar o resultado dos seus algoritmos vamos usar o módulo testing do numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.testing import assert_equal\n",
    "from numpy.testing import assert_almost_equal\n",
    "from numpy.testing import assert_array_almost_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aula 02 - Regressão Linear e Logística from Scratch\n",
    "\n",
    "Um fator importante de salientar é que embora nosso curso está fazendo uso de mxnet nas aulas iniciais, vamos migrar para outros frameworks estilo PyTorch ao longo das aulas. No fim, todos são bem parecidos e aplicam as mesmas ideias de diferenciação automática. Para brincar um pouco mais com essa diferenciação, nesta aula vamos implementar a regressão linear e logística do zero. Vamos fazer duas versões de cada:\n",
    "\n",
    "1. Derivando na mão, não é complicado.\n",
    "2. Derivando com mxnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunto de Problemas 1: Mais Derivadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de entrar na regressão, vamos brincar um pouco de derivadas dentro de funções. Dado dois números `x` and `y`, implemente a função `log_exp` que retorna:\n",
    "\n",
    "$$-\\log\\left(\\frac{e^x}{e^x+e^y}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_exp(x, y):\n",
    "    # implemente\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Abaixo vamos testar o seu código com algumas entradas simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = nd.array([2]), nd.array([3])\n",
    "z = log_exp(x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste. Não apague\n",
    "assert_equal(1.31326175, z.asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Agora implementa uma função para computar $\\partial z/\\partial x$ e $\\partial z/\\partial y$ usando `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O argumento funcao_forward é uma função python. Será a sua log_exp.\n",
    "# A ideia aqui é deixar claro a ideia de forward e backward propagation, depois\n",
    "# de avaliar a função chamamos backward e temos as derivadas.\n",
    "def grad(funcao_forward, x, y):\n",
    "    # implemente\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = nd.array([2]), nd.array([3])\n",
    "dx, dy = grad(log_exp, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_equal(-0.7310586, dx.asnumpy())\n",
    "assert_equal(0.7310586, dy.asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Agora teste com números maiores, algum problema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = nd.array([50]), nd.array([100])\n",
    "grad(log_exp, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Pense um pouco sobre o motivo do erro acima. Usando as propriedade de logaritmos, é possível fazer uma função mais estável? Implemente a mesma abaixo. O problema aqui é que o exponencial \"explode\" quando x ou y são muito grandes. Use o [link](http://www.wolframalpha.com/input/?i=log[e%5Ex+%2F+[e%5Ex+%2B+e%5Ey]]) para lhe ajudar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_log_exp(x, y):\n",
    "    # implemente\n",
    "    pass\n",
    "\n",
    "dx, dy = grad(stable_log_exp, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_log_exp(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste. Não apague\n",
    "assert_equal(-1, dx.asnumpy())\n",
    "assert_equal(1, dy.asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O exemplo acima mostra um pouco de problemas de estabilidade númerica. às vezes é melhor usar versões alternativas de funções. Isto vai ocorrer quando você ver vários nans na sua frente :-) Claro, estamos assumindo que existe uma outra função equivalente que é mais estável para o computador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunto de Problemas 2: Regressão Linear\n",
    "\n",
    "Agora, vamos explorar uma regressão linear. Embora não vamos fazer uso da logexp acima, a ideia de derivar parcialmente dentro de funções pode nos ajudar. Lembrando da regressão linear, inicialmente temos um conjunto observações representadas como tuplas $(\\mathbf{x}_i, y_i)$. Aqui, $\\mathbf{x}_i$ é um vetor de atributos. Vamo forçar $\\mathbf{x}_{i0} = 1$, capturando assim o intercepto. Além do mais, $\\mathbf{x}_{ij}$ quando $j\\neq 0$, são os outros atributos de entrada. $y_i$ uma valor real representando uma resposta. Nossa regressão visa capturar:\n",
    "\n",
    "$$y_i = 1 + \\theta_1 x_{i1} + \\theta_2 x_{i2} + \\cdots + \\theta_k x_{if}$$\n",
    "\n",
    "Lembrando da regressão linear multivariada, podemos representar as equações como uma multiplicação de uma matriz com um vetor\n",
    "\n",
    "![](./figs/linear.png)\n",
    "\n",
    "7. Crie uma função de previsao. A mesma recebe uma matrix $\\mathbf{X}$ e um vetor de parâmetros $\\theta$. Sua função deve retornar um vetor de previsões para cada linha de $\\mathbf{X}$. Não use nenhum laço!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previsao(X, theta):\n",
    "    # implemente\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erros quadrados**. Para aprender os parâmetros ótimos da regressão linear, precisamos fazer uso de um modelo de erros quadrados. Em particular nosso objetico é aprender os parâmetros que minimizam a função:\n",
    "\n",
    "$$L(\\mathbf{\\theta}) = n^{-1} \\sum_i ({\\hat{y}_i - y_i})^2$$\n",
    "\n",
    "Onde $\\hat{y}_i$ é uma previsão (vêm da sua função python acima). $y_i$ é o valor real dos dados.\n",
    "\n",
    "8. Implemente uma função para a média dos os erros quadrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def media_erros_quadrados(X, theta, y):\n",
    "    # implemente\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Agora, crie **DUAS** funções que derivam o erro acima. A primeira deve usar o autograd de mxnet. Lembre-se que temos um vetor de parâmetros $\\theta$. Por sorte, você pode fazer derivadas de tais vetores também."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivada_mxnet(X, theta, y):\n",
    "    # implemente\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. A segunda versão não usa mxnet. Implemente as derivadas do zero. Nos [slides](https://docs.google.com/presentation/d/1bz3G3fEohNtvERKSDtThfGPYOjF83Fzy2yHPdCAlFZw/edit#slide=id.g584f66d2ae_3_74) do link explicamos como fazer tal operação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivada_navera(X, theta, y):\n",
    "    # implemente\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Por fim, otimize sua função usando o algoritmo de gradiente descendente abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(d_fun, loss_fun, X, y, lambda_=0.01, tol=0.00001, max_iter=10000):\n",
    "    '''\n",
    "    Executa Gradiente Descendente. Aqui:\n",
    "    \n",
    "    Parâmetros\n",
    "    ----------\n",
    "    d_fun : é uma função de derivadas\n",
    "    loss_fun : é uma função de perda\n",
    "    X : é um vetor de fatores explanatórios.\n",
    "        Copie seu código de intercepto da primeira aula.\n",
    "        para adicionar o intercepto em X.\n",
    "    y : é a resposta\n",
    "    lambda : é a taxa de aprendizad\n",
    "    tol : é a tolerância, define quando o algoritmo vai parar.\n",
    "    max_ter : é a segunda forma de parada, mesmo sem convergir\n",
    "              paramos depois de max_iter iterações.\n",
    "    '''\n",
    "    theta = nd.random.normal(shape=X.shape[1])\n",
    "    print('Iter {}; theta = '.format(0), theta)\n",
    "    old_err_sq = np.inf\n",
    "    i = 0\n",
    "    while True:\n",
    "        # Computar as derivadas\n",
    "        grad = d_fun(X, theta, y)\n",
    "        # Atualizar\n",
    "        theta_novo = theta - lambda_ * grad\n",
    "        \n",
    "        # Parar quando o erro convergir\n",
    "        err_sq = loss_fun(X, theta, y)\n",
    "        if nd.abs(old_err_sq - err_sq) <= tol:\n",
    "            break\n",
    "        \n",
    "        # Atualizar parâmetros e erro\n",
    "        theta = theta_novo\n",
    "        old_err_sq = err_sq\n",
    "        \n",
    "        # Informação de debug\n",
    "        print('Iter {}; theta = '.format(i+1), theta)\n",
    "        i += 1\n",
    "        if i == max_iter:\n",
    "            break\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para testes, não apague!!!\n",
    "X = nd.zeros(shape=(1000, 2))\n",
    "X[:, 0] = 1\n",
    "X[:, 1] = nd.random.normal(shape=1000)\n",
    "\n",
    "theta_0_real = 7\n",
    "theta_1_real = 9\n",
    "y = theta_0_real + theta_1_real * X[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua função deve retornar algo perto de 7 e 9\n",
    "theta = gd(derivada_mxnet, media_erros_quadrados, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua função deve retornar algo perto de 7 e 9\n",
    "theta = gd(derivada_navera, media_erros_quadrados, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Altere a função de Gradiente Descendente para funcionar com minibatches. Em outras palavras, não compute o erro usando todos os dados de X. Use um `slice` de tamanho do minibatch. Uma ideia é seguir o pseudocódigo abaixo.\n",
    "\n",
    "```python\n",
    "index = np.arange(len(X))\n",
    "while True:\n",
    "    minib = np.random.choice(index, minibatchsize) # aqui estou usando numpy para selection minibatch elementos\n",
    "    X_batch = X[minib]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo abaixo\n",
    "index = np.arange(len(X))\n",
    "mb = np.random.choice(index, 50)\n",
    "print(mb)\n",
    "X[mb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gd(d_fun, loss_fun, X, y, lambda_=0.01, tol=0.00001,\n",
    "                 max_iter=10000, batch_size=10):\n",
    "    '''\n",
    "    Executa Gradiente Descendente. Aqui:\n",
    "    \n",
    "    Parâmetros\n",
    "    ----------\n",
    "    d_fun : é uma função de derivadas\n",
    "    loss_fun : é uma função de perda\n",
    "    X : é um vetor de fatores explanatórios.\n",
    "        Copie seu código de intercepto da primeira aula.\n",
    "        para adicionar o intercepto em X.\n",
    "    y : é a resposta\n",
    "    lambda : é a taxa de aprendizad\n",
    "    tol : é a tolerância, define quando o algoritmo vai parar.\n",
    "    max_ter : é a segunda forma de parada, mesmo sem convergir\n",
    "              paramos depois de max_iter iterações.\n",
    "    batch_size : tamanho do batch\n",
    "    '''\n",
    "    # implemente\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua função deve retornar algo perto de 7 e 9\n",
    "minibatch_gd(derivada_mxnet, media_erros_quadrados, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunto de Problemas 3: Logistic from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Repita o mesmo processo para a Logística. Lembrando que a mesma tem a seguinte forma:\n",
    "\n",
    "$$f(x_i) = \\frac{1}{1 + e^{-(1 + \\theta_1 x_{i1} + \\theta_2 x_{i2} + \\cdots \\theta_k x_{ik})}}$$\n",
    "\n",
    "Implemente a função logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, theta):\n",
    "    # implemente\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testes, não apague!\n",
    "X_teste = nd.random.normal(shape=(1000, 20000))\n",
    "theta = nd.random.normal(shape=(20000))\n",
    "y_hat_teste = logistic(X_teste, theta)\n",
    "assert_equal(True, (y_hat_teste >= 0).asnumpy().all())\n",
    "assert_equal(True, (y_hat_teste <= 1).asnumpy().all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando a logística acima implemente uma função logistica_prever que retorna 0 ou 1. Use o limar dado na função. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistica_prever(X, theta, limiar=0.5):\n",
    "    # implemente\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testes, não apague!\n",
    "X_teste = nd.random.normal(shape=(1000, 20000))\n",
    "theta = nd.random.normal(shape=(20000))\n",
    "y_hat_teste = logistica_prever(X_teste, theta)\n",
    "for yi in y_hat_teste.asnumpy():\n",
    "    assert(yi in {0, 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, implemente uma função de entropia cruzada da logística. A mesma, é proporcional ao inverso da verossimilhança. Para entender a derivação entre as duas faça uso dos [Slides](https://docs.google.com/presentation/d/1yGPETPe8o7PPOP6_CF38LHr3vpxgTEnF5LjP-1pkGIc/edit?usp=sharing). \n",
    "\n",
    "Sendo:\n",
    "\n",
    "$$ll(x_i,y_i~|~\\theta) = y_i \\log f(x_i\\theta) + (1-y_i) \\log (1-f(x_i\\theta))$$\n",
    "\n",
    "A verossimilhança para uma observação. A entropia cruzada é a media da negação do termo para todos os exemplos:\n",
    "\n",
    "$$L(\\theta) = -n^{-1}\\sum_i \\big((1-y_i)\\log (1-f_{\\theta}(x_i)) + y_i\\log (f_{\\theta}(x_i))\\big)$$\n",
    "\n",
    "`dica: use nd.clip(logistic(X, theta), 0.001, 0.999)`. A função remove os 0 e 1s da logistic, evitando assim o valor log(0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_mean(X, theta, y):\n",
    "    # implemente\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testes, não apague!\n",
    "from sklearn import datasets\n",
    "state = np.random.seed(20190187)\n",
    "\n",
    "X, y = datasets.make_blobs(n_samples=200, n_features=2, centers=2)\n",
    "X = nd.array(X)\n",
    "y = nd.array(y)\n",
    "\n",
    "for _ in range(100):\n",
    "    theta = nd.random.normal(shape=(2, ))\n",
    "    assert(cross_entropy_mean(X, theta, y) >= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora implemente a derivada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivada_mxnet_logit(X, theta, y):\n",
    "    # implemente\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir daqui basta executar código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.asnumpy()[:, 0], X.asnumpy()[:, 1], s=80, edgecolors='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Assim como foi feito na primeira aula, abaixo testamos o seu código com um toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use essa função antes de executar o GD\n",
    "def add_intercept(X):\n",
    "    Xn = nd.zeros(shape=(X.shape[0], X.shape[1] + 1))\n",
    "    Xn[:, 0]  = 1\n",
    "    Xn[:, 1:] = X\n",
    "    return Xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xn = add_intercept(X)\n",
    "theta = minibatch_gd(derivada_mxnet_logit, cross_entropy_mean, Xn, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = logistica_prever(Xn, theta)\n",
    "print((y == y_p).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
