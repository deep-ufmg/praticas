{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ed03SC1Jm9Yy"
   },
   "source": [
    "# Aprendizado Profundo - UFMG\n",
    "\n",
    "\n",
    "## Regulizadores\n",
    "\n",
    "Neste código iremos analisar como funcionam os regularizadores e como eles são usados para evitar *overfitting*, fenônemo que faz com o modelo não generalize bem em outros dataset além do usando durante o treino.\n",
    "\n",
    "# *Overfitting*\n",
    "\n",
    "Redes neurais são muito flexíveis porque não se limitam a ver cada atributo a ser aprendido individualmente. Em vez disso, elas podem aprender interações entre os atributos. Por causa disso, mesmo quando temos apenas um pequeno número de atributos, as redes neurais profundas são capazes de chegar ao *overfitting*, um cenário onde o modelo aprende a classificar muito bem (as vezes, perfeitamente) as instâncias de treino, porém não generaliza para outras instâncias não vista (como são os casos de amostras do conjunto de validação ou teste).\n",
    "\n",
    "Para evitar esse cenário, algumas técnicas foram propostas para evitar o *overfitting*.\n",
    "\n",
    "**Como introduzido, nesta aula prática, implementaremos e testaremos duas técnicas para evitar o *overfitting*: *Dropout* e *Weight Decay***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gp6CwWnFnTwb"
   },
   "source": [
    "Esse pequeno bloco de código abaixo é usado somente para instalar o MXNet para CUDA 10. Execute esse bloco somente uma vez e ignore possíveis erros levantados durante a instalação.\n",
    "\n",
    "**ATENÇÃO: a alteração deste bloco pode implicar em problemas na execução dos blocos restantes!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43870,
     "status": "ok",
     "timestamp": 1560449309063,
     "user": {
      "displayName": "Keiller Nogueira",
      "photoUrl": "https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg",
      "userId": "03938009311988397527"
     },
     "user_tz": 180
    },
    "id": "XW-VATPAldgt",
    "outputId": "d1e7472e-cc76-46c8-ffd8-24d3f15b6534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet-cu100\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/91/b5c2692297aa5b8c383e0da18f9208fc6d5519d981c03266abfbde897c41/mxnet_cu100-1.4.1-py2.py3-none-manylinux1_x86_64.whl (488.3MB)\n",
      "\u001b[K     |████████████████████████████████| 488.3MB 29kB/s \n",
      "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1 (from mxnet-cu100)\n",
      "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (2.21.0)\n",
      "Collecting numpy<1.15.0,>=1.8.2 (from mxnet-cu100)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c4/395ebb218053ba44d64935b3729bc88241ec279915e72100c5979db10945/numpy-1.14.6-cp36-cp36m-manylinux1_x86_64.whl (13.8MB)\n",
      "\u001b[K     |████████████████████████████████| 13.8MB 38.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (1.24.3)\n",
      "\u001b[31mERROR: spacy 2.1.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: fastai 1.0.52 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: blis 0.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: graphviz, numpy, mxnet-cu100\n",
      "  Found existing installation: graphviz 0.10.1\n",
      "    Uninstalling graphviz-0.10.1:\n",
      "      Successfully uninstalled graphviz-0.10.1\n",
      "  Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\n",
      "      Successfully uninstalled numpy-1.16.4\n",
      "Successfully installed graphviz-0.8.4 mxnet-cu100-1.4.1 numpy-1.14.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install mxnet-cu100\n",
    "\n",
    "# imports basicos\n",
    "import time, os, sys\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn, utils as gutils, data as gdata\n",
    "\n",
    "# Tenta encontrar GPU\n",
    "def try_gpu():\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.zeros((1,), ctx=ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx\n",
    "\n",
    "ctx = try_gpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Azv2ajIYkIjH"
   },
   "source": [
    "## *Dropout*\n",
    "\n",
    "*Dropout* é uma das formas mais interessantes de regularizar sua rede neural. \n",
    "A ideia do *Droupout* é simples: durante o passo de *Forward*, alguns neurônios são aleatoriamente \"desligados\", ou seja, são zerados, e não são utilizados em nenhum processamento.\n",
    "Em cada passo do *Forward*, neurônios diferentes são \"desligados\" aleatoriamente, de acordo com uma probabilide pré-definida.\n",
    "Lembrem-se que esse processo só acontece durante o treino.\n",
    "Durante o teste, *Dropout* não tem nenhuma ação e todos os neurônios são usados para gerar o resultado fina.\n",
    "\n",
    "Formalmente, suponha um neurônio com ativação $h$ e um *Dropout* com probabilide $p$ (de zerar ou \"desligar\" o neurônio).\n",
    "Logo, essa técnica irá \"desligar\" a ativação desse neurônio com probabilidade $p$ ou reescala-la baseado na probabilidade de essa unidade de processamento permanecer ativa (isto é, $1-p$):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h' =\n",
    "\\begin{cases}\n",
    "    0 & \\text{ com probabilidade } p \\\\\n",
    "    \\frac{h}{1-p} & \\text{ caso contrário}\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Tal método é interessante e chamou a atenção do mundo acadêmico por ser muito simples de implementar e poder impulsar significativamente o desempenho do modelo.\n",
    "\n",
    "### Implementação\n",
    "\n",
    "Em frameworks atuais (como no MxNet, TensorFlow, e PyTorch), para utilizar os benefícios do Dropout basta adicionar a camada homônima (passando como argumento a probabilidade de desligamento dos neurônios) durante a construção da arquitetura.\n",
    "\n",
    "**Um exemplo é mostrado abaixo utilizando o framework MxNet.**\n",
    "\n",
    "Durante o treino, a camada *Dropout* irá \"desligar\" aleatoriamente algumas saídas da camada anterior (ou equivalentemente, as entradas para a camada subsequente) de acordo com a probabilidade especificada.\n",
    "\n",
    "Quando o MXNet não está no modo de treinamento, a camada *Dropout* simplesmente passa os dados sem fazer nenhum \"desligamento\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g9u0pCOtlWLu"
   },
   "outputs": [],
   "source": [
    "## carregando dados\n",
    "\n",
    "# código para carregar o dataset do MNIST\n",
    "# http://yann.lecun.com/exdb/mnist/\n",
    "def load_data_mnist(batch_size, resize=None, root=os.path.join(\n",
    "        '~', '.mxnet', 'datasets', 'mnist')):\n",
    "    \"\"\"Download the MNIST dataset and then load into memory.\"\"\"\n",
    "    root = os.path.expanduser(root)\n",
    "    transformer = []\n",
    "    if resize:\n",
    "        transformer += [gdata.vision.transforms.Resize(resize)]\n",
    "    transformer += [gdata.vision.transforms.ToTensor()]\n",
    "    transformer = gdata.vision.transforms.Compose(transformer)\n",
    "\n",
    "    mnist_train = gdata.vision.MNIST(root=root, train=True)\n",
    "    mnist_test = gdata.vision.MNIST(root=root, train=False)\n",
    "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "    train_iter = gdata.DataLoader(mnist_train.transform_first(transformer),\n",
    "                                  batch_size, shuffle=True,\n",
    "                                  num_workers=num_workers)\n",
    "    test_iter = gdata.DataLoader(mnist_test.transform_first(transformer),\n",
    "                                 batch_size, shuffle=False,\n",
    "                                 num_workers=num_workers)\n",
    "    return train_iter, test_iter\n",
    "\n",
    "# código para carregar o dataset do Fashion-MNIST\n",
    "# https://github.com/zalandoresearch/fashion-mnist\n",
    "def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join(\n",
    "        '~', '.mxnet', 'datasets', 'fashion-mnist')):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load into memory.\"\"\"\n",
    "    root = os.path.expanduser(root)\n",
    "    transformer = []\n",
    "    if resize:\n",
    "        transformer += [gdata.vision.transforms.Resize(resize)]\n",
    "    transformer += [gdata.vision.transforms.ToTensor()]\n",
    "    transformer = gdata.vision.transforms.Compose(transformer)\n",
    "\n",
    "    mnist_train = gdata.vision.FashionMNIST(root=root, train=True)\n",
    "    mnist_test = gdata.vision.FashionMNIST(root=root, train=False)\n",
    "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "    train_iter = gdata.DataLoader(mnist_train.transform_first(transformer),\n",
    "                                  batch_size, shuffle=True,\n",
    "                                  num_workers=num_workers)\n",
    "    test_iter = gdata.DataLoader(mnist_test.transform_first(transformer),\n",
    "                                 batch_size, shuffle=False,\n",
    "                                 num_workers=num_workers)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4487,
     "status": "ok",
     "timestamp": 1560449324584,
     "user": {
      "displayName": "Keiller Nogueira",
      "photoUrl": "https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg",
      "userId": "03938009311988397527"
     },
     "user_tz": 180
    },
    "id": "8oSVf8u1Oi1m",
    "outputId": "08c3ca93-4622-40b4-8957-b3f0a0d0148b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# funções básicas\n",
    "def _get_batch(batch, ctx):\n",
    "    \"\"\"Return features and labels on ctx.\"\"\"\n",
    "    features, labels = batch\n",
    "    if labels.dtype != features.dtype:\n",
    "        labels = labels.astype(features.dtype)\n",
    "    return (gutils.split_and_load(features, ctx),\n",
    "            gutils.split_and_load(labels, ctx), features.shape[0])\n",
    "\n",
    "# Função usada para calcular acurácia\n",
    "def evaluate_accuracy(data_iter, net, loss, ctx=[mx.cpu()]):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    acc_sum, n, l = nd.array([0]), 0, 0\n",
    "    for batch in data_iter:\n",
    "        features, labels, _ = _get_batch(batch, ctx)\n",
    "        for X, y in zip(features, labels):\n",
    "            # X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "            y = y.astype('float32')\n",
    "            y_hat = net(X)\n",
    "            l += loss(y_hat, y).sum()\n",
    "            acc_sum += (y_hat.argmax(axis=1) == y).sum().copyto(mx.cpu())\n",
    "            n += y.size\n",
    "        acc_sum.wait_to_read()\n",
    "    return acc_sum.asscalar() / n, l.asscalar() / n\n",
    "  \n",
    "# Função usada no treinamento e validação da rede\n",
    "def train_validate(net, train_iter, test_iter, batch_size, trainer, loss, ctx,\n",
    "                   num_epochs):\n",
    "    print('training on', ctx)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss, ctx)\n",
    "        print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n",
    "              'test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_loss, \n",
    "                 test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45350,
     "status": "ok",
     "timestamp": 1560351278587,
     "user": {
      "displayName": "Keiller Nogueira",
      "photoUrl": "https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg",
      "userId": "03938009311988397527"
     },
     "user_tz": 180
    },
    "id": "tlBGUroCFfyh",
    "outputId": "126fb0d6-d598-4585-d7fe-7a78c74827cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, train loss 1.1607, train acc 0.553, test loss 0.5975, test acc 0.770, time 3.9 sec\n",
      "epoch 2, train loss 0.5867, train acc 0.784, test loss 0.4665, test acc 0.827, time 4.3 sec\n",
      "epoch 3, train loss 0.4984, train acc 0.819, test loss 0.4051, test acc 0.854, time 5.1 sec\n",
      "epoch 4, train loss 0.4557, train acc 0.835, test loss 0.3840, test acc 0.860, time 4.2 sec\n",
      "epoch 5, train loss 0.4253, train acc 0.843, test loss 0.3873, test acc 0.860, time 4.4 sec\n",
      "epoch 6, train loss 0.4001, train acc 0.854, test loss 0.3579, test acc 0.869, time 4.4 sec\n",
      "epoch 7, train loss 0.3840, train acc 0.860, test loss 0.3409, test acc 0.874, time 4.1 sec\n",
      "epoch 8, train loss 0.3745, train acc 0.863, test loss 0.3623, test acc 0.868, time 4.2 sec\n",
      "epoch 9, train loss 0.3630, train acc 0.867, test loss 0.3451, test acc 0.876, time 4.1 sec\n",
      "epoch 10, train loss 0.3476, train acc 0.872, test loss 0.3368, test acc 0.874, time 4.3 sec\n"
     ]
    }
   ],
   "source": [
    "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), e \n",
    "# tamanho do batch\n",
    "num_epochs, lr, batch_size = 10, 0.5, 256\n",
    "\n",
    "# rede simples somente com perceptrons e camadas densamente conectadas\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation=\"relu\"),  # camada densamente conectada\n",
    "        nn.Dropout(0.2),                   # dropout com 20% de probabilidade de desligar os neurônios\n",
    "        nn.Dense(256, activation=\"relu\"),  # camada densamente conectada\n",
    "        nn.Dropout(0.5),                   # dropout com 50% de probabilidade de desligar os neurônios\n",
    "        nn.Dense(10))                      # camada densamente conectada para classificação\n",
    "net.initialize(init.Normal(sigma=0.01), ctx=ctx)\n",
    "\n",
    "# função de custo (ou loss)\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "# carregamento do dado: fashion mnist\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "\n",
    "# trainer do gluon\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "\n",
    "# treinamento e validação via MXNet\n",
    "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, \n",
    "               ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJTUwjd4QSkF"
   },
   "source": [
    "## *Weight Decay*\n",
    "\n",
    "*Weight Decay* (comumente chamado regularização *L2*), é uma das técnicas mais utilizadas para regularizar modelos paramétricos de aprendizado de máquina.\n",
    "A intuição básica por trás do *Weight Decay* é a noção de que entre todas as funções $f$, a função $f=0$ é a mais simples. Intuitivamente, podemos medir funções pela sua proximidade a zero. Mas quão devemos medir a distância entre uma função e zero? Não há uma resposta correta. De fato, ramos inteiros da matemática são dedicados a responder a esta questão.\n",
    "\n",
    "Para nossos propósitos atuais, uma interpretação muito simples será suficiente: vamos considerar uma função linear $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}$ é simples se o seu vetor de peso $\\mathbf{w}$ for pequeno. Podemos medir isso via norma $||\\mathbf{w}||^2$. Uma maneira de manter o vetor de peso pequeno é adicionar sua norma como um termo de penalidade ao problema de minimizar a função de perda (ou *loss*). Assim, nós substituímos nosso objetivo original, *minimizar o erro de previsão nos rótulos de treinamento*, com novo objetivo, *minimizar o erro de previsão e o termo de penalidade*. Agora, se o vetor de peso se tornar muito grande, nosso algoritmo de aprendizagem vai encontrar mais lucro minimizando a norma $||\\mathbf{w}||^2$ do que minimizando o erro de treinamento. \n",
    "\n",
    "Tecnicamente, para uma função de custo qualquer $\\mathcal{L}$, a adição do novo termo de penalidade (ou *weight decay*) acontece da seguinte forma:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\boldsymbol{w}\\|^2$$\n",
    "\n",
    "Esse parâmetro não negativo $\\lambda \\geq 0$ dita a quantidade de regularização. Para $\\lambda = 0$, recuperamos nossa função de perda original, enquanto para $\\lambda > 0 $ garantimos que os pesos $\\mathbf{w}$ não crescerão demais.\n",
    "\n",
    "### Implementação\n",
    "\n",
    "Em frameworks atuais (como no MxNet, TensorFlow, e PyTorch), *Weight Decay* pode ser facilmente agregado à função de custo durante a construção do modelo.\n",
    "\n",
    "**Um exemplo é mostrado abaixo utilizando o framework MxNet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45387,
     "status": "ok",
     "timestamp": 1560351376154,
     "user": {
      "displayName": "Keiller Nogueira",
      "photoUrl": "https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg",
      "userId": "03938009311988397527"
     },
     "user_tz": 180
    },
    "id": "bB-koQ_vd9ef",
    "outputId": "8a5d8341-5891-48b3-d629-2cef75484b0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, train loss 1.0333, train acc 0.638, test loss 0.2954, test acc 0.912, time 4.3 sec\n",
      "epoch 2, train loss 0.2932, train acc 0.913, test loss 0.1991, test acc 0.944, time 4.3 sec\n",
      "epoch 3, train loss 0.2499, train acc 0.929, test loss 0.1928, test acc 0.947, time 5.3 sec\n",
      "epoch 4, train loss 0.2015, train acc 0.943, test loss 0.1540, test acc 0.956, time 4.2 sec\n",
      "epoch 5, train loss 0.1911, train acc 0.947, test loss 0.1476, test acc 0.959, time 4.2 sec\n",
      "epoch 6, train loss 0.2293, train acc 0.936, test loss 0.1498, test acc 0.959, time 4.3 sec\n",
      "epoch 7, train loss 0.1761, train acc 0.951, test loss 0.1555, test acc 0.957, time 4.2 sec\n",
      "epoch 8, train loss 0.1724, train acc 0.952, test loss 0.2367, test acc 0.931, time 4.2 sec\n",
      "epoch 9, train loss 0.1610, train acc 0.956, test loss 0.1333, test acc 0.963, time 4.1 sec\n",
      "epoch 10, train loss 0.1973, train acc 0.945, test loss 0.1472, test acc 0.961, time 4.2 sec\n"
     ]
    }
   ],
   "source": [
    "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), \n",
    "# tamanho do batch, e valor de weight decay\n",
    "num_epochs, lr, batch_size, weight_decay = 10, 0.5, 256, 0.005\n",
    "\n",
    "# rede simples somente com perceptrons e camadas densamente conectadas\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation=\"relu\"),  # camada densamente conectada\n",
    "        nn.Dense(256, activation=\"relu\"),  # camada densamente conectada\n",
    "        nn.Dense(10))                      # camada densamente conectada para classificação\n",
    "net.initialize(init.Normal(sigma=0.01), ctx=ctx)\n",
    "\n",
    "# função de custo (ou loss)\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "# carregamento do dado: mnist\n",
    "train_iter, test_iter = load_data_mnist(batch_size)\n",
    "\n",
    "# trainer do gluon\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, \n",
    "                                                      'wd': weight_decay})\n",
    "\n",
    "# treinamento e validação via MXNet\n",
    "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, \n",
    "               ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1eXXJN5MoAca"
   },
   "source": [
    "## MNIST - *Overfitting* and regularizadores\n",
    "\n",
    "Agora usaremos um dataset específico juntamente com um rede mais produnda para tentar mostrar o efeito de *overfitting* e entender como as técnicas de regularização aprendidas podem ser usadas para resolver esse problema.\n",
    "\n",
    "O modelo implementado abaixo usa o dataset MNIST treinando uma rede com quatro camadas que não usa nenhum método de regularização.\n",
    "Note a diferença entre o *loss* e a acurácia de treino e teste.\n",
    "Um resultado onde o *loss* do teste é relativamente maior que o treino (caso do exemplo abaixo) indica que um (neste caso, princípio de) *overfitting* está acontecendo.\n",
    "Use as técnicas vistas nesta aula prática para tratar esse problema.\n",
    "\n",
    "Especificamente, implemente:\n",
    "\n",
    "1. Uma versão dessa arquitetura com *Dropout*. Teste diferentes valores de probabilidade de forma a diminuir o *overfitting*.\n",
    "2. Uma versão desse modelo com *Weight Decay*. Teste diferentes valores de $\\lambda$ de forma a diminuir o *overfitting*.\n",
    "3. Uma versão que combina os dois métodos de regularização aprendidos. Talvez seja necessário testar diferentes valores para a probabilidade do *Dropout* e do $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 90815,
     "status": "ok",
     "timestamp": 1560349585111,
     "user": {
      "displayName": "Keiller Nogueira",
      "photoUrl": "https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg",
      "userId": "03938009311988397527"
     },
     "user_tz": 180
    },
    "id": "BblIVW2CoV-P",
    "outputId": "a21e524a-c5cd-4c25-e1b4-c687cd1e3206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, train loss 2.3017, train acc 0.112, test loss 2.3008, test acc 0.114, time 4.1 sec\n",
      "epoch 2, train loss 2.0558, train acc 0.205, test loss 1.0226, test acc 0.567, time 4.3 sec\n",
      "epoch 3, train loss 0.5507, train acc 0.824, test loss 0.1968, test acc 0.945, time 4.3 sec\n",
      "epoch 4, train loss 0.1845, train acc 0.946, test loss 0.1503, test acc 0.953, time 4.5 sec\n",
      "epoch 5, train loss 0.1201, train acc 0.964, test loss 0.1170, test acc 0.966, time 4.4 sec\n",
      "epoch 6, train loss 0.0904, train acc 0.973, test loss 0.1007, test acc 0.971, time 5.4 sec\n",
      "epoch 7, train loss 0.0710, train acc 0.979, test loss 0.0927, test acc 0.972, time 4.4 sec\n",
      "epoch 8, train loss 0.1234, train acc 0.965, test loss 0.1044, test acc 0.970, time 4.2 sec\n",
      "epoch 9, train loss 0.0633, train acc 0.981, test loss 0.0907, test acc 0.973, time 4.2 sec\n",
      "epoch 10, train loss 0.0492, train acc 0.985, test loss 0.0875, test acc 0.976, time 4.3 sec\n",
      "epoch 11, train loss 0.0391, train acc 0.988, test loss 0.0868, test acc 0.976, time 4.4 sec\n",
      "epoch 12, train loss 0.0318, train acc 0.990, test loss 0.0937, test acc 0.975, time 4.3 sec\n",
      "epoch 13, train loss 0.0250, train acc 0.992, test loss 0.0918, test acc 0.977, time 4.5 sec\n",
      "epoch 14, train loss 0.0271, train acc 0.991, test loss 0.0950, test acc 0.977, time 4.5 sec\n",
      "epoch 15, train loss 0.0184, train acc 0.994, test loss 0.0897, test acc 0.979, time 4.7 sec\n",
      "epoch 16, train loss 0.0169, train acc 0.995, test loss 0.1003, test acc 0.978, time 4.4 sec\n",
      "epoch 17, train loss 0.2066, train acc 0.953, test loss 0.1268, test acc 0.964, time 4.5 sec\n",
      "epoch 18, train loss 0.0513, train acc 0.984, test loss 0.1077, test acc 0.971, time 4.4 sec\n",
      "epoch 19, train loss 0.0359, train acc 0.989, test loss 0.1064, test acc 0.973, time 4.5 sec\n",
      "epoch 20, train loss 0.0230, train acc 0.993, test loss 0.1017, test acc 0.976, time 4.4 sec\n"
     ]
    }
   ],
   "source": [
    "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), e \n",
    "# tamanho do batch\n",
    "num_epochs, lr, batch_size = 20, 0.5, 256\n",
    "\n",
    "# rede simples somente com perceptrons e camadas densamente conectadas\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dense(128, activation=\"relu\"),\n",
    "        nn.Dense(64, activation=\"relu\"),\n",
    "        nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01), ctx=ctx)\n",
    "\n",
    "# função de custo (ou loss)\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "# carregamento do dado: mnist\n",
    "train_iter, test_iter = load_data_mnist(batch_size)\n",
    "\n",
    "# trainer do gluon\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "\n",
    "# treinamento e validação via MXNet\n",
    "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, \n",
    "               ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lEu01dDokIlS"
   },
   "source": [
    "## Exercícios\n",
    "\n",
    "1. Qual impacto de alterar a probabilidade da camada de *Dropout*?  Como fica uma rede com probabilidade de 50% de \"desligar\" os neurônios?\n",
    "1. E em relação ao valor de $\\lambda$ para o *Weight Decay*, qual impacto? Teste valores como 0.0005 e 0.0001.\n",
    "1. Qual efeito da quantidade de *epochs* na acurácia final do modelo?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "03.3 - Regularizadores.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
