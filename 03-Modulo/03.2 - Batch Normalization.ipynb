{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02.2 - Batch Normalization.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PvDQHNCWTxgV","colab_type":"text"},"source":["# Batch Normalization\n","\n","Teinar modelos profundos e fazê-los convergir numa quantidade razoável de tempo pode ser   uma tarefa complicada.\n","Nesta prática, descrevemos e usaremos o [*batch normalization*](https://arxiv.org/abs/1502.03167) (BN), uma técnica popular e eficaz capaz de acelerar a convergência de redes profundas que, juntamente com [blocos residuais](https://arxiv.org/abs/1512.03385), nos permitiu treinar redes com mais de 100 (até mesmo 1000) camadas.\n","\n","Primeiro, vamos rever alguns dos desafios práticos ao treinar redes profundas.\n","\n","1. O pré-processamento de dados geralmente se prova crucial modelagem estatística. Como falado anteriormente, num geral, se padroniza a entrada para ter uma média *zero* e variância de *um*. Padronizar os dados de entrada normalmente torna mais fácil treinar modelos profundos, pois os parâmetros estão, *a priori*, em uma escala similar.  \n","1. Para reles MLP ou CNN, enquanto treinamos o modelo, as ativações em camadas intermediárias da rede podem assumir diferentes ordens de magnitude. Os autores de [*batch normalization*](https://arxiv.org/abs/1502.03167) postulou que esta diferença na distribuição das ativações poderia dificultar a convergência da rede. Intuitivamente, poderíamos conjeturar que, se camada tem valores de ativação que são 100x que de outra camada, poderíamos precisa ajustar as taxas de aprendizagem de forma adaptável por camada (ou mesmo para neurônios dentro de uma mesma camada).\n","1. Redes mais profundas são complexas e propensas à *overfitting*. Isso significa que a regularização se torna mais importante. Empiricamente, notamos que mesmo com o *dropout*, os modelos podem cair numa situação de *overfitting*. Neste caso, devemos nos beneficiar de outras heurística de regularização.\n"," \n"]},{"cell_type":"code","metadata":{"attributes":{"classes":[],"id":"","n":"72"},"id":"v6NyEc3oTxgc","colab_type":"code","outputId":"699b6f80-2727-4775-ed64-3b9771172ae3","executionInfo":{"status":"ok","timestamp":1562538820437,"user_tz":180,"elapsed":7290,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":164}},"source":["!pip install mxnet-cu100\n","\n","# imports basicos\n","# imports basicos\n","%matplotlib inline\n","import time, math, os, sys, numpy as np\n","from IPython import display\n","from matplotlib import pyplot as plt\n","\n","import mxnet as mx\n","from mxnet import autograd, gluon, init, nd\n","from mxnet.gluon import loss as gloss, nn, utils as gutils, data as gdata\n","\n","# Tenta encontrar GPU\n","def try_gpu():\n","    try:\n","        ctx = mx.gpu()\n","        _ = nd.zeros((1,), ctx=ctx)\n","    except mx.base.MXNetError:\n","        ctx = mx.cpu()\n","    return ctx\n","\n","ctx = try_gpu()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: mxnet-cu100 in /usr/local/lib/python3.6/dist-packages (1.4.1)\n","Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (2.21.0)\n","Requirement already satisfied: numpy<1.15.0,>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (1.14.6)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (0.8.4)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2019.6.16)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IM_HCWsRzSMM","colab_type":"code","colab":{}},"source":["# código para carregar o dataset do Fashion-MNIST\n","# https://github.com/zalandoresearch/fashion-mnist\n","def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join(\n","        '~', '.mxnet', 'datasets', 'fashion-mnist')):\n","    \"\"\"Download the Fashion-MNIST dataset and then load into memory.\"\"\"\n","    root = os.path.expanduser(root)\n","    transformer = []\n","    if resize:\n","        transformer += [gdata.vision.transforms.Resize(resize)]\n","    transformer += [gdata.vision.transforms.ToTensor()]\n","    transformer = gdata.vision.transforms.Compose(transformer)\n","\n","    mnist_train = gdata.vision.FashionMNIST(root=root, train=True)\n","    mnist_test = gdata.vision.FashionMNIST(root=root, train=False)\n","    num_workers = 0 if sys.platform.startswith('win32') else 4\n","\n","    train_iter = gdata.DataLoader(mnist_train.transform_first(transformer),\n","                                  batch_size, shuffle=True,\n","                                  num_workers=num_workers)\n","    test_iter = gdata.DataLoader(mnist_test.transform_first(transformer),\n","                                 batch_size, shuffle=False,\n","                                 num_workers=num_workers)\n","    return train_iter, test_iter\n","\n","# funções básicas\n","def _get_batch(batch, ctx):\n","    \"\"\"Return features and labels on ctx.\"\"\"\n","    features, labels = batch\n","    if labels.dtype != features.dtype:\n","        labels = labels.astype(features.dtype)\n","    return (gutils.split_and_load(features, ctx),\n","            gutils.split_and_load(labels, ctx), features.shape[0])\n","\n","# Função usada para calcular acurácia\n","def evaluate_accuracy(data_iter, net, loss, ctx=[mx.cpu()]):\n","    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n","    if isinstance(ctx, mx.Context):\n","        ctx = [ctx]\n","    acc_sum, n, l = nd.array([0]), 0, 0\n","    for batch in data_iter:\n","        features, labels, _ = _get_batch(batch, ctx)\n","        for X, y in zip(features, labels):\n","            # X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n","            y = y.astype('float32')\n","            y_hat = net(X)\n","            l += loss(y_hat, y).sum()\n","            acc_sum += (y_hat.argmax(axis=1) == y).sum().copyto(mx.cpu())\n","            n += y.size\n","        acc_sum.wait_to_read()\n","    return acc_sum.asscalar() / n, l.asscalar() / n\n","  \n","# Função usada no treinamento e validação da rede\n","def train_validate(net, train_iter, test_iter, batch_size, trainer, loss, ctx,\n","                   num_epochs):\n","    print('training on', ctx)\n","    for epoch in range(num_epochs):\n","        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n","        for X, y in train_iter:\n","            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n","            with autograd.record():\n","                y_hat = net(X)\n","                l = loss(y_hat, y).sum()\n","            l.backward()\n","            trainer.step(batch_size)\n","            y = y.astype('float32')\n","            train_l_sum += l.asscalar()\n","            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n","            n += y.size\n","        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss, ctx)\n","        print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n","              'test acc %.3f, time %.1f sec'\n","              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_loss, \n","                 test_acc, time.time() - start))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2LJIAO4FkedS","colab_type":"text"},"source":["## Detalhes Técnicos\n","\n","Em 2015, uma heurística inteligente chamada [*batch normalization*](https://arxiv.org/abs/1502.03167) provou ser imensamente útil para melhorar a confiabilidade e a velocidade de convergência no treinamento de modelos profundos. Em cada iteração de treinamento, o  [*batch normalization*](https://arxiv.org/abs/1502.03167) normaliza as ativações de cada neurônio da camada oculta (em cada camada onde é aplicada) subtraindo sua média e dividindo pelo seu desvio padrão, estimando ambos com base no mini-batch atual. Note que se o tamanho do batch fosse $1$, não aprenderíamos nada porque durante o treinamento, todos os nó levaria valor $0$. No entanto, com minibatches grandes o suficiente, a abordagem se prova eficaz e estável.\n","\n","Em poucas palavras, a ideia do [*batch normalization*](https://arxiv.org/abs/1502.03167) é transformar a ativação em uma determinada camada de $\\mathbf{x}$ para:\n","\n","$$\\mathrm{BN}(\\mathbf{x}) = \\mathbf{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\mathbf{\\mu}}}{\\hat\\sigma} + \\mathbf{\\beta}$$\n","\n","Aqui, $\\hat{\\mathbf{\\mu}}$ é a estimativa da média e $\\hat {\\mathbf{\\sigma}}$ é a estimativa da variância. O resultado é que as ativações são aproximadamente reescaladas para uma média zero e uma variância unitária. Como podemos notar, as ativações das camadas intermediárias não pode divergir muito pois estamos ativamente redimensionando-as de volta para uma dada ordem de grandeza através de $\\mathbf{\\mu}$ e $\\sigma$. Entretanto, em alguns casos, as ativações pode precisar diferir dos dados padronizados. Para lidar com isso e dar mais liberdade à essa normalização, definimos um coeficiente de escala de coordenadas $\\mathbf{\\gamma}$ e um offset $\\mathbf{\\beta}$. Intuitivamente, espera-se que essa normalização nos permita ser mais agressivo ao escolher taxas de aprendizado maiores.\n","\n","Em princípio, podemos querer usar todos os nossos dados de treinamento para estimar a média e variância. No entanto, as ativações correspondentes a cada exemplo mudar cada vez que atualizamos nosso modelo. Para remediar este problema, o [*batch normalization*](https://arxiv.org/abs/1502.03167) usa apenas o minibatch atual para estimar $\\hat{\\mathbf {\\mu}} $ e $\\hat \\sigma$. É justamente por esse fato de normalizamos com base apenas no *batch* atual que  o método se chama *batch normalization*. Para indicar qual *minibatch* $\\mathcal {B}$ é usado, nós denotamos essas variáveis como $\\hat{\\mathbf{\\mu}}_\\mathcal {B}$ e $\\hat \\sigma_\\mathcal {B}$.\n","\n","$$\\hat{\\mathbf{\\mu}}_\\mathcal{B} \\leftarrow \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} \\mathbf{x}$$\n","\n","$$\\hat{\\mathbf{\\sigma}}_\\mathcal{B}^2 \\leftarrow \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} (\\mathbf{x} - \\mathbf{\\mu}_{\\mathcal{B}})^2 + \\epsilon$$\n","\n","Observe que adicionamos uma pequena constante $\\epsilon > 0$ à estimativa de variância para garantir que nunca acabemos por dividir por zero, mesmo nos casos em que a estimativa de variância pode desaparecer por acidente.\n","Vamos agora ver, na prática, o uso do [*batch normalization*](https://arxiv.org/abs/1502.03167).\n","\n","## Camadas de *Batch Normalization*\n","\n","O [*batch normalization*](https://arxiv.org/abs/1502.03167) para camadas totalmente conectadas (*fully-connected* ou Densas) e camadas convolucionais são ligeiramente diferentes. Isso se deve à dimensionalidade dos dados gerados pelas camadas convolucionais. Os dois casos são discutidos abaixo. Observe que uma das principais diferenças entre a camada de [*batch normalization*](https://arxiv.org/abs/1502.03167) e outras camadas é que a primeira opera em um *minibatch* completo de cada vez (caso contrário, não é possível calcular os parâmetros de média e variância por *batch*).\n","\n","### Camadas densas\n","\n","Normalmente, aplicamos a camada de [*batch normalization*](https://arxiv.org/abs/1502.03167) entre a transformação e a função de ativação em uma camada densa. A seguir, denotamos por $\\mathbf{u}$ a entrada e por $\\mathbf{x} = \\mathbf{W}\\mathbf{u} + \\mathbf{b}$ a saída da transformada linear. Isso produz a seguinte fórmula:\n","\n","$$\\mathbf{y} = \\phi(\\mathrm{BN}(\\mathbf{x})) =  \\phi(\\mathrm{BN}(\\mathbf{W}\\mathbf{u} + \\mathbf{b}))$$\n","\n","Lembre-se de que a média e a variância são calculadas no **mesmo** *minibatch* $\\mathcal{B}$ no qual a transformação é aplicada. Lembre-se também que o coeficiente $\\mathbf{\\gamma}$ e o offset $\\mathbf{\\beta}$ são parâmetros que precisam ser aprendidos. Eles garantem que o efeito do [*batch normalization*](https://arxiv.org/abs/1502.03167) possa ser neutralizado conforme necessário.\n","\n","### Camadas convolucionais\n","\n","Para camadas convolucionais, o [*batch normalization*](https://arxiv.org/abs/1502.03167) ocorre após o cálculo da convolução e antes da aplicação da função de ativação. Se a computação de convolução gerar múltiplos canais, realizamos o [*batch normalization*](https://arxiv.org/abs/1502.03167) para **cada** uma das saídas desses canais, que tem parâmetros ($\\mathbf{\\gamma}$ e $\\mathbf{\\beta}$) independentes. Suponha que haja exemplos de $m$ no *batch*. Em um único canal, assumimos que a altura e a largura da saída da convolução são $p$ e $q$, respectivamente. Precisamos realizar o [*batch normalization*](https://arxiv.org/abs/1502.03167) para $m \\times p \\times q$ elementos neste canal simultaneamente. Ao executar o cálculo de padronização para esses elementos, usamos a mesma média e variância. Em outras palavras, usamos as médias e as variâncias dos elementos $m \\times p \\times q$ neste canal em vez de um por pixel.\n"]},{"cell_type":"markdown","metadata":{"id":"dc3QamIKTxhU","colab_type":"text"},"source":["## MXNet e o caso de estudo LeNet-5\n","\n","Agora vamos implementar a [LeNet-5](https://ieeexplore.ieee.org/document/726791) usando a camada de [*batch normalization*](https://arxiv.org/abs/1502.03167).\n","\n","Em frameworks modernos, camadas de [*batch normalization*](https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.BatchNorm) já vem implementadas e são fáceis de usar."]},{"cell_type":"code","metadata":{"id":"PGwDyeZ3Txhe","colab_type":"code","outputId":"73f562c7-ca17-4b6e-9c54-bf4cecf69043","colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"status":"ok","timestamp":1562542346901,"user_tz":180,"elapsed":109050,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}}},"source":["# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), \n","# tamanho do batch, e lambda do weight decay\n","num_epochs, lr, batch_size, wd_lambda = 10, 0.01, 128, 0.0001\n","\n","# rede baseada na LeNet-5 \n","net = nn.Sequential()\n","net.add(nn.Conv2D(6, kernel_size=5, strides=1, padding=0),   # entrada: (b, 1, 32, 32) e saida: (b, 6, 28, 28)\n","        nn.BatchNorm(),\n","        nn.Activation('tanh'),\n","        nn.AvgPool2D(pool_size=2, strides=2, padding=0),     # entrada: (b, 6, 28, 28) e saida: (b, 6, 14, 14)\n","        nn.Conv2D(16, kernel_size=5, strides=1, padding=0),  # entrada: (b, 6, 14, 14) e saida: (b, 16, 10, 10)\n","        nn.BatchNorm(),\n","        nn.Activation('tanh'),\n","        nn.AvgPool2D(pool_size=2, strides=2, padding=0),     # entrada: (b, 16, 10, 10) e saida: (b, 16, 5, 5)\n","        nn.Conv2D(120, kernel_size=5, strides=1, padding=0), # entrada: (b, 16, 5, 5) e saida: (b, 120, 1, 1)\n","        nn.BatchNorm(),\n","        nn.Activation('tanh'),\n","        nn.Flatten(),  # lineariza formando um vetor         # entrada: (b, 120, 1, 1) e saida: (b, 120*1*1) = (b, 120)\n","        nn.Dense(84),\n","        nn.BatchNorm(),\n","        nn.Activation('tanh'),\n","        nn.Dense(10))\n","net.initialize(init.Normal(sigma=0.01), ctx=ctx)\n","\n","# função de custo (ou loss)\n","loss = gloss.SoftmaxCrossEntropyLoss()\n","\n","# carregamento do dado: mnist\n","train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=32)\n","\n","# trainer do gluon\n","trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr, 'wd': wd_lambda})\n","\n","# treinamento e validação via MXNet\n","train_validate(net, train_iter, test_iter, batch_size, trainer, loss, \n","               ctx, num_epochs)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["training on gpu(0)\n","epoch 1, train loss 0.5455, train acc 0.797, test loss 0.4089, test acc 0.851, time 10.3 sec\n","epoch 2, train loss 0.3727, train acc 0.863, test loss 0.3609, test acc 0.866, time 10.1 sec\n","epoch 3, train loss 0.3327, train acc 0.878, test loss 0.3802, test acc 0.854, time 10.0 sec\n","epoch 4, train loss 0.3106, train acc 0.887, test loss 0.3783, test acc 0.863, time 10.0 sec\n","epoch 5, train loss 0.2926, train acc 0.894, test loss 0.2965, test acc 0.891, time 10.0 sec\n","epoch 6, train loss 0.2887, train acc 0.895, test loss 0.3855, test acc 0.860, time 10.2 sec\n","epoch 7, train loss 0.2805, train acc 0.897, test loss 0.3206, test acc 0.885, time 10.0 sec\n","epoch 8, train loss 0.2704, train acc 0.901, test loss 0.3617, test acc 0.859, time 12.3 sec\n","epoch 9, train loss 0.2720, train acc 0.901, test loss 0.4826, test acc 0.820, time 13.3 sec\n","epoch 10, train loss 0.2643, train acc 0.904, test loss 0.2854, test acc 0.897, time 10.8 sec\n"],"name":"stdout"}]}]}