{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02.1 - Estrategias-de-treino.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["mA3x5yGAiRDG"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ed03SC1Jm9Yy","colab_type":"text"},"source":["# Estratégias de treino\n","\n","Até agora no curso, vimos somente uma estratégia de treino para redes neurais: o treinamento do zero.\n","Entretanto, há outras formas de se explorar redes neurais.\n","Nessa aula, vamos rever a estratégia treinamento do zero além de apresentar duas novas formas:\n","\n","1.   rede neural como um extrator de características, e\n","2.   *fine-tuning*.\n","\n","Para cada uma dessas estratégias, vamos apresentar sua definição, vantagens e desvantagens.\n","Antes, vamos instalar o MXNet, importar alguns pacotes e definir funções para carregar os dados.\n"]},{"cell_type":"code","metadata":{"id":"Dx_n6rukq1RG","colab_type":"code","outputId":"a15f0d2f-80df-487e-8ff5-8251e9e277cd","executionInfo":{"status":"ok","timestamp":1562634033886,"user_tz":180,"elapsed":67934,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":623}},"source":["!pip install mxnet-cu100\n","\n","# imports basicos\n","import time, os, sys, numpy as np\n","import mxnet as mx\n","from mxnet import autograd, gluon, init, nd\n","from mxnet.gluon import loss as gloss, nn, utils as gutils, data as gdata\n","\n","# Tenta encontrar GPU\n","def try_gpu():\n","    try:\n","        ctx = mx.gpu()\n","        _ = nd.zeros((1,), ctx=ctx)\n","    except mx.base.MXNetError:\n","        ctx = mx.cpu()\n","    return ctx\n","\n","ctx = try_gpu()\n","ctx\n","\n","## carregando dados\n","\n","# código para carregar o dataset do MNIST\n","# http://yann.lecun.com/exdb/mnist/\n","def load_data_mnist(batch_size, resize=None, root=os.path.join(\n","        '~', '.mxnet', 'datasets', 'mnist')):\n","    \"\"\"Download the MNIST dataset and then load into memory.\"\"\"\n","    root = os.path.expanduser(root)\n","    transformer = []\n","    if resize:\n","        transformer += [gdata.vision.transforms.Resize(resize)]\n","    transformer += [gdata.vision.transforms.ToTensor()]\n","    transformer = gdata.vision.transforms.Compose(transformer)\n","\n","    mnist_train = gdata.vision.MNIST(root=root, train=True)\n","    mnist_test = gdata.vision.MNIST(root=root, train=False)\n","    num_workers = 0 if sys.platform.startswith('win32') else 4\n","\n","    train_iter = gdata.DataLoader(mnist_train.transform_first(transformer),\n","                                  batch_size, shuffle=True,\n","                                  num_workers=num_workers)\n","    test_iter = gdata.DataLoader(mnist_test.transform_first(transformer),\n","                                 batch_size, shuffle=False,\n","                                 num_workers=num_workers)\n","    return train_iter, test_iter\n","\n","# código para carregar o dataset do CIFAR 10\n","# https://www.cs.toronto.edu/~kriz/cifar.html\n","def load_data_cifar10(batch_size, resize=None, root=os.path.join(\n","        '~', '.mxnet', 'datasets', 'cifar10')):\n","    \"\"\"Download the MNIST dataset and then load into memory.\"\"\"\n","    root = os.path.expanduser(root)\n","    transformer = []\n","    if resize:\n","        transformer += [gdata.vision.transforms.Resize(resize)]\n","    transformer += [gdata.vision.transforms.ToTensor()]\n","    transformer = gdata.vision.transforms.Compose(transformer)\n","\n","    cifar10_train = gdata.vision.CIFAR10(root=root, train=True)\n","    cifar10_test = gdata.vision.CIFAR10(root=root, train=False)\n","    num_workers = 0 if sys.platform.startswith('win32') else 4\n","\n","    train_iter = gdata.DataLoader(cifar10_train.transform_first(transformer),\n","                                  batch_size, shuffle=True,\n","                                  num_workers=num_workers)\n","    test_iter = gdata.DataLoader(cifar10_test.transform_first(transformer),\n","                                 batch_size, shuffle=False,\n","                                 num_workers=num_workers)\n","    return train_iter, test_iter\n","  \n","# funções básicas\n","def _get_batch(batch, ctx):\n","    \"\"\"Return features and labels on ctx.\"\"\"\n","    features, labels = batch\n","    if labels.dtype != features.dtype:\n","        labels = labels.astype(features.dtype)\n","    return (gutils.split_and_load(features, ctx),\n","            gutils.split_and_load(labels, ctx), features.shape[0])\n","\n","# Função usada para calcular acurácia\n","def evaluate_accuracy(data_iter, net, loss, ctx=[mx.cpu()]):\n","    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n","    if isinstance(ctx, mx.Context):\n","        ctx = [ctx]\n","    acc_sum, n, l = nd.array([0]), 0, 0\n","    for batch in data_iter:\n","        features, labels, _ = _get_batch(batch, ctx)\n","        for X, y in zip(features, labels):\n","            # X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n","            y = y.astype('float32')\n","            y_hat = net(X)\n","            l += loss(y_hat, y).sum()\n","            acc_sum += (y_hat.argmax(axis=1) == y).sum().copyto(mx.cpu())\n","            n += y.size\n","        acc_sum.wait_to_read()\n","    return acc_sum.asscalar() / n, l.asscalar() / n\n","  \n","# Função usada no treinamento e validação da rede\n","def train_validate(net, train_iter, test_iter, batch_size, trainer, loss, ctx,\n","                   num_epochs):\n","    print('training on', ctx)\n","    for epoch in range(num_epochs):\n","        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n","        for X, y in train_iter:\n","            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n","            with autograd.record():\n","                y_hat = net(X)\n","                l = loss(y_hat, y).sum()\n","            l.backward()\n","            trainer.step(batch_size)\n","            y = y.astype('float32')\n","            train_l_sum += l.asscalar()\n","            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n","            n += y.size\n","        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss, ctx)\n","        print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n","              'test acc %.3f, time %.1f sec'\n","              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_loss, \n","                 test_acc, time.time() - start))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting mxnet-cu100\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/91/b5c2692297aa5b8c383e0da18f9208fc6d5519d981c03266abfbde897c41/mxnet_cu100-1.4.1-py2.py3-none-manylinux1_x86_64.whl (488.3MB)\n","\u001b[K     |████████████████████████████████| 488.3MB 34kB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (2.21.0)\n","Collecting numpy<1.15.0,>=1.8.2 (from mxnet-cu100)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c4/395ebb218053ba44d64935b3729bc88241ec279915e72100c5979db10945/numpy-1.14.6-cp36-cp36m-manylinux1_x86_64.whl (13.8MB)\n","\u001b[K     |████████████████████████████████| 13.8MB 35.1MB/s \n","\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1 (from mxnet-cu100)\n","  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2019.6.16)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (1.24.3)\n","\u001b[31mERROR: spacy 2.1.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fastai 1.0.54 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: blis 0.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: numpy, graphviz, mxnet-cu100\n","  Found existing installation: numpy 1.16.4\n","    Uninstalling numpy-1.16.4:\n","      Successfully uninstalled numpy-1.16.4\n","  Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","Successfully installed graphviz-0.8.4 mxnet-cu100-1.4.1 numpy-1.14.6\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"mA3x5yGAiRDG","colab_type":"text"},"source":["## Treinamento do zero\n","\n","Como dito anteriormente, essa foi a única estratégia vista até o momento no curso.\n","Nessa estratégia, uma rede neural é proposta, **inicializada com pesos aleatórios** e treinada até convergir.\n","A **vantagem** dessa estratégia é liberdade para definir como quiser a arquitetura da rede e seus hiper-parâmetros\n","Por outro lado, a **desvantagem** é que essa estratégia requer muitos dados para convergir a rede inicializada aleatoriamente.\n","Logo, se tivermos poucos dados, essa não é a estratégia mais recomendada.\n","Abaixo, uma representação visual dessa estratégia.\n","\n","<p align=\"center\">\n","  <img width=600 src=\"https://drive.google.com/uc?export=view&id=1_bBQjyoDqB3kQMncmVkuJwSxDs3rqUmM\">\n","</p>\n","\n","Apesar de já termos visto essa estratégia na prática, vamos vê-la aqui novamente para efeitos de comparação com as outras técnicas. Para tal, vamos, primeiro, definimos a arquitetura da [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).\n","\n"]},{"cell_type":"code","metadata":{"id":"Fi4WOPu4i0us","colab_type":"code","colab":{}},"source":["class AlexNet(nn.HybridBlock):\n","    r\"\"\"AlexNet model from the `\"One weird trick...\" `_ paper.\n","\n","    Parameters\n","    ----------\n","    classes : int, default 10\n","        Number of classes for the output layer.\n","    \"\"\"\n","    def __init__(self, classes=10, **kwargs):\n","        super(AlexNet, self).__init__(**kwargs)\n","        with self.name_scope():\n","            self.features = nn.HybridSequential(prefix='')\n","            with self.features.name_scope():\n","                self.features.add(nn.Conv2D(64, kernel_size=11, strides=4, padding=0, activation='relu'))  # entrada: (b, 3, 227, 227) e saida: (b, 64, 55, 55)\n","                self.features.add(nn.MaxPool2D(pool_size=3, strides=2))                                    # entrada: (b, 64, 55, 55) e saida: (b, 64, 27, 27)\n","                self.features.add(nn.Conv2D(192, kernel_size=5, padding=2, activation='relu'))             # entrada: (b, 64, 27, 27) e saida: (b, 192, 27, 27)\n","                self.features.add(nn.MaxPool2D(pool_size=3, strides=2))                                    # entrada: (b, 192, 27, 27) e saida: (b, 192, 13, 13)\n","                self.features.add(nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'))             # entrada: (b, 192, 13, 13) e saida: (b, 384, 13, 13)\n","                self.features.add(nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'))             # entrada: (b, 384, 13, 13) e saida: (b, 256, 13, 13)\n","                self.features.add(nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'))             # entrada: (b, 256, 13, 13) e saida: (b, 256, 13, 13)\n","                self.features.add(nn.MaxPool2D(pool_size=3, strides=2))                                    # entrada: (b, 256, 13, 13) e saida: (b, 256, 6, 6)\n","                self.features.add(nn.Flatten())                                                            # entrada: (b, 256, 13, 13) e saida: (b, 256*6*6) = (b, 9216)\n","                self.features.add(nn.Dense(4096, activation='relu'))                                       # entrada: (b, 9216) e saida: (b, 4096)\n","                self.features.add(nn.Dropout(0.5))\n","                self.features.add(nn.Dense(4096, activation='relu'))                                       # entrada: (b, 4096) e saida: (b, 4096)\n","                self.features.add(nn.Dropout(0.5))\n","\n","            self.output = nn.Dense(classes)  # entrada: (b, 4096) e saida: (b, 10)\n","\n","    def hybrid_forward(self, F, x):\n","        x = self.features(x)\n","        x = self.output(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NXy0csxGMer3","colab_type":"text"},"source":["Nesse bloco abaixo, carregamos o dataset [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) para treinar a rede.\n","Notem que a rede é inicializada aleatoriamente na linha `net.initialize(init.Normal(sigma=0.01), ctx=ctx)`.\n","Logo, não temos controle sobre esses pesos que podem assumir qualquer valor."]},{"cell_type":"code","metadata":{"id":"ghmYljpKjIQp","colab_type":"code","outputId":"78e2c062-0db3-4161-d3c4-6adc40af94a6","executionInfo":{"status":"ok","timestamp":1562425713068,"user_tz":180,"elapsed":1345975,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":403}},"source":["NUM_CLASSES = 10\n","\n","num_epochs, lr, batch_size, wd_lambda = 20, 0.001, 100, 0.0001\n","    \n","net = AlexNet(NUM_CLASSES)\n","net.initialize(init.Normal(sigma=0.01), ctx=ctx)\n","\n","# função de custo (ou loss)\n","loss = gloss.SoftmaxCrossEntropyLoss()\n","\n","# carregamento do dado: fashion mnist\n","train_iter, test_iter = load_data_cifar10(batch_size, resize=227)\n","\n","# trainer do gluon\n","trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'wd': wd_lambda, 'momentum': 0.9})\n","\n","# treinamento e validação via MXNet\n","train_validate(net, train_iter, test_iter, batch_size, trainer, loss, \n","               ctx, num_epochs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["training on gpu(0)\n","epoch 1, train loss 2.3025, train acc 0.100, test loss 2.3024, test acc 0.100, time 69.7 sec\n","epoch 2, train loss 2.3023, train acc 0.110, test loss 2.3020, test acc 0.178, time 66.6 sec\n","epoch 3, train loss 2.3019, train acc 0.149, test loss 2.3014, test acc 0.183, time 66.7 sec\n","epoch 4, train loss 2.3011, train acc 0.158, test loss 2.3001, test acc 0.226, time 66.7 sec\n","epoch 5, train loss 2.2992, train acc 0.189, test loss 2.2961, test acc 0.199, time 67.0 sec\n","epoch 6, train loss 2.2820, train acc 0.169, test loss 2.2130, test acc 0.167, time 66.8 sec\n","epoch 7, train loss 2.1272, train acc 0.211, test loss 1.9905, test acc 0.264, time 66.8 sec\n","epoch 8, train loss 1.9652, train acc 0.272, test loss 1.8956, test acc 0.307, time 66.7 sec\n","epoch 9, train loss 1.8751, train acc 0.306, test loss 1.8088, test acc 0.346, time 66.8 sec\n","epoch 10, train loss 1.7789, train acc 0.342, test loss 1.7553, test acc 0.360, time 67.2 sec\n","epoch 11, train loss 1.6997, train acc 0.375, test loss 1.6184, test acc 0.411, time 67.4 sec\n","epoch 12, train loss 1.6134, train acc 0.410, test loss 1.5215, test acc 0.442, time 67.3 sec\n","epoch 13, train loss 1.5514, train acc 0.436, test loss 1.4709, test acc 0.461, time 67.1 sec\n","epoch 14, train loss 1.4845, train acc 0.459, test loss 1.4140, test acc 0.487, time 67.3 sec\n","epoch 15, train loss 1.4206, train acc 0.486, test loss 1.3663, test acc 0.503, time 67.3 sec\n","epoch 16, train loss 1.3696, train acc 0.505, test loss 1.2983, test acc 0.534, time 67.1 sec\n","epoch 17, train loss 1.3244, train acc 0.524, test loss 1.2612, test acc 0.550, time 67.0 sec\n","epoch 18, train loss 1.2732, train acc 0.545, test loss 1.2760, test acc 0.555, time 67.0 sec\n","epoch 19, train loss 1.2247, train acc 0.562, test loss 1.1733, test acc 0.581, time 67.0 sec\n","epoch 20, train loss 1.1734, train acc 0.584, test loss 1.1738, test acc 0.582, time 67.0 sec\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ru1VhhY5ObHN","colab_type":"text"},"source":["É muito comum se usar redes já existentes para aprender características em novos dados.\n","Por isso, muitos frameworks já deixam as arquiteturas mais famosas pré-implementadas para que possam ser usadas.\n","\n","No MXNet, podemos importar uma rede [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) usando o pacote [model_zoo](https://mxnet.incubator.apache.org/api/python/gluon/model_zoo.html) do MXNet.\n","Há várias arquiteturas pré-definidas nessa biblioteca, incluindo várias [DenseNets](https://arxiv.org/pdf/1608.06993.pdf) e [ResNets](https://arxiv.org/abs/1603.05027), [VGGs](https://arxiv.org/abs/1409.1556), [SqueezeNets](https://arxiv.org/abs/1602.07360), etc."]},{"cell_type":"code","metadata":{"id":"Fnfy5Bi6Oam5","colab_type":"code","outputId":"0d471b4e-13cf-4b74-cbf5-72fb60714bbc","executionInfo":{"status":"ok","timestamp":1562427448970,"user_tz":180,"elapsed":1356554,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":403}},"source":["from mxnet.gluon.model_zoo import vision\n","\n","NUM_CLASSES = 10\n","num_epochs, lr, batch_size, wd_lambda = 20, 0.001, 100, 0.0001\n","    \n","net = vision.alexnet(classes=NUM_CLASSES)  # http://mxnet.incubator.apache.org/versions/master/tutorials/gluon/pretrained_models.html\n","net.initialize(init.Normal(sigma=0.01), ctx=ctx)\n","\n","# função de custo (ou loss)\n","loss = gloss.SoftmaxCrossEntropyLoss()\n","\n","# carregamento do dado: fashion mnist\n","train_iter, test_iter = load_data_cifar10(batch_size, resize=227)\n","\n","# trainer do gluon\n","trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'wd': wd_lambda, 'momentum': 0.9})\n","\n","# treinamento e validação via MXNet\n","train_validate(net, train_iter, test_iter, batch_size, trainer, loss, \n","               ctx, num_epochs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["training on gpu(0)\n","epoch 1, train loss 2.3026, train acc 0.101, test loss 2.3024, test acc 0.109, time 70.5 sec\n","epoch 2, train loss 2.3023, train acc 0.121, test loss 2.3021, test acc 0.186, time 67.2 sec\n","epoch 3, train loss 2.3020, train acc 0.134, test loss 2.3016, test acc 0.220, time 67.2 sec\n","epoch 4, train loss 2.3013, train acc 0.161, test loss 2.3004, test acc 0.192, time 67.1 sec\n","epoch 5, train loss 2.2997, train acc 0.189, test loss 2.2973, test acc 0.216, time 67.3 sec\n","epoch 6, train loss 2.2921, train acc 0.178, test loss 2.2704, test acc 0.142, time 67.2 sec\n","epoch 7, train loss 2.1805, train acc 0.189, test loss 2.0912, test acc 0.237, time 67.5 sec\n","epoch 8, train loss 2.0156, train acc 0.259, test loss 1.9899, test acc 0.285, time 67.2 sec\n","epoch 9, train loss 1.9183, train acc 0.292, test loss 1.8192, test acc 0.330, time 67.0 sec\n","epoch 10, train loss 1.8218, train acc 0.323, test loss 1.7433, test acc 0.343, time 67.6 sec\n","epoch 11, train loss 1.7511, train acc 0.352, test loss 1.7014, test acc 0.381, time 67.3 sec\n","epoch 12, train loss 1.6916, train acc 0.377, test loss 1.6069, test acc 0.417, time 67.5 sec\n","epoch 13, train loss 1.6112, train acc 0.410, test loss 1.5174, test acc 0.444, time 67.6 sec\n","epoch 14, train loss 1.5342, train acc 0.443, test loss 1.4882, test acc 0.466, time 67.6 sec\n","epoch 15, train loss 1.4664, train acc 0.468, test loss 1.4015, test acc 0.490, time 67.7 sec\n","epoch 16, train loss 1.4059, train acc 0.493, test loss 1.3811, test acc 0.497, time 68.2 sec\n","epoch 17, train loss 1.3647, train acc 0.509, test loss 1.3339, test acc 0.518, time 68.4 sec\n","epoch 18, train loss 1.3159, train acc 0.529, test loss 1.2461, test acc 0.558, time 68.2 sec\n","epoch 19, train loss 1.2660, train acc 0.546, test loss 1.2268, test acc 0.566, time 67.9 sec\n","epoch 20, train loss 1.2253, train acc 0.563, test loss 1.1634, test acc 0.585, time 66.9 sec\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y4kuJPwaXvRP","colab_type":"text"},"source":["## Extrator de características\n","\n","A terceira e última estratégia, mostrada na figura abaixo, é usar uma rede neural pré-treinada em algum dataset grande para extrair características de um outro dataset. Essa estratégia é preferível quando o dataset que se quer extrair as *features* tem muito poucas amostras, inviabilizando o treinamento ou *fine-tuning* da rede.\n","\n","<p align=\"center\">\n","  <img width=600 src=\"https://drive.google.com/uc?export=view&id=1pWGfQIAeOODIvm-IQ7De4kl60XpRYbb5\">\n","</p>\n","\n","Existem duas formas de se explorar essa estratégia. A primeira consiste em substituir e treinar somente a última camada da rede neural. Nessa primeira forma, todas as outras camadas da rede ficam com *learning rate* 0, ou seja, não aprendem nada, e são somente usadas como codificadores/extratores de características. A segunda forma, *features* das imagens do dataset que se quer classificar são extraídas da penúltima camada da rede pré-treinada (geralmente, a camada antes da camada de classificação). Essas *features* são então usadas para se treinar um agoritmo externo (como um SVM ou *random forest*), que então classifica o dataset."]},{"cell_type":"code","metadata":{"id":"GEYfyf_oNFCB","colab_type":"code","outputId":"2e41c983-4d05-4bbc-da25-933fc8f51162","executionInfo":{"status":"ok","timestamp":1562429483163,"user_tz":180,"elapsed":1377092,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from mxnet.gluon.model_zoo import vision\n","\n","NUM_CLASSES = 10\n","num_epochs, lr, batch_size, wd_lambda = 20, 0.001, 100, 0.0001\n","    \n","net = vision.alexnet(pretrained=True, ctx=ctx)\n","print(net)\n","with net.name_scope():\n","    net.output = gluon.nn.Dense(NUM_CLASSES)  # substitui a ultima camada por uma nova com 10 classes\n","    net.output.initialize(init.Normal(sigma=0.01), ctx=ctx)  # inicializa essa camada aleatoriamente de acordo com uma distribuicao Normal\n","print(net)\n","\n","# este loop abaixo ajusta o multiplicador do learning rate para as camadas\n","for key, value in net.collect_params().items():\n","    print(key, value.lr_mult)\n","    if 'dense3' not in key:  # essa eh a nota camada e nao vamos mudar o multiplicador do seu learning rate\n","      value.lr_mult = 0.0  # so aprenderemos a ultima camada\n","    print(key, value.lr_mult)\n","    \n","# função de custo (ou loss)\n","loss = gloss.SoftmaxCrossEntropyLoss()\n","\n","# carregamento do dado: fashion mnist\n","train_iter, test_iter = load_data_cifar10(batch_size, resize=227)\n","\n","# trainer do gluon\n","trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'wd': wd_lambda, 'momentum': 0.9})\n","\n","# treinamento e validação via MXNet\n","train_validate(net, train_iter, test_iter, batch_size, trainer, loss, \n","               ctx, num_epochs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["AlexNet(\n","  (features): HybridSequential(\n","    (0): Conv2D(3 -> 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2), Activation(relu))\n","    (1): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n","    (2): Conv2D(64 -> 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), Activation(relu))\n","    (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n","    (4): Conv2D(192 -> 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n","    (5): Conv2D(384 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n","    (6): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n","    (7): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n","    (8): Flatten\n","    (9): Dense(9216 -> 4096, Activation(relu))\n","    (10): Dropout(p = 0.5, axes=())\n","    (11): Dense(4096 -> 4096, Activation(relu))\n","    (12): Dropout(p = 0.5, axes=())\n","  )\n","  (output): Dense(4096 -> 1000, linear)\n",")\n","AlexNet(\n","  (features): HybridSequential(\n","    (0): Conv2D(3 -> 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2), Activation(relu))\n","    (1): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n","    (2): Conv2D(64 -> 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), Activation(relu))\n","    (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n","    (4): Conv2D(192 -> 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n","    (5): Conv2D(384 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n","    (6): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n","    (7): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n","    (8): Flatten\n","    (9): Dense(9216 -> 4096, Activation(relu))\n","    (10): Dropout(p = 0.5, axes=())\n","    (11): Dense(4096 -> 4096, Activation(relu))\n","    (12): Dropout(p = 0.5, axes=())\n","  )\n","  (output): Dense(None -> 10, linear)\n",")\n","alexnet14_conv0_weight 1.0\n","alexnet14_conv0_weight 0.0\n","alexnet14_conv0_bias 1.0\n","alexnet14_conv0_bias 0.0\n","alexnet14_conv1_weight 1.0\n","alexnet14_conv1_weight 0.0\n","alexnet14_conv1_bias 1.0\n","alexnet14_conv1_bias 0.0\n","alexnet14_conv2_weight 1.0\n","alexnet14_conv2_weight 0.0\n","alexnet14_conv2_bias 1.0\n","alexnet14_conv2_bias 0.0\n","alexnet14_conv3_weight 1.0\n","alexnet14_conv3_weight 0.0\n","alexnet14_conv3_bias 1.0\n","alexnet14_conv3_bias 0.0\n","alexnet14_conv4_weight 1.0\n","alexnet14_conv4_weight 0.0\n","alexnet14_conv4_bias 1.0\n","alexnet14_conv4_bias 0.0\n","alexnet14_dense0_weight 1.0\n","alexnet14_dense0_weight 0.0\n","alexnet14_dense0_bias 1.0\n","alexnet14_dense0_bias 0.0\n","alexnet14_dense1_weight 1.0\n","alexnet14_dense1_weight 0.0\n","alexnet14_dense1_bias 1.0\n","alexnet14_dense1_bias 0.0\n","alexnet14_dense3_weight 1.0\n","alexnet14_dense3_weight 1.0\n","alexnet14_dense3_bias 1.0\n","alexnet14_dense3_bias 1.0\n","training on gpu(0)\n","epoch 1, train loss 1.4197, train acc 0.493, test loss 1.0944, test acc 0.624, time 68.5 sec\n","epoch 2, train loss 1.2675, train acc 0.551, test loss 1.0562, test acc 0.636, time 67.9 sec\n","epoch 3, train loss 1.2340, train acc 0.563, test loss 1.0494, test acc 0.635, time 68.2 sec\n","epoch 4, train loss 1.2186, train acc 0.567, test loss 1.0418, test acc 0.636, time 68.5 sec\n","epoch 5, train loss 1.2022, train acc 0.572, test loss 1.0019, test acc 0.651, time 68.5 sec\n","epoch 6, train loss 1.1945, train acc 0.576, test loss 1.0115, test acc 0.655, time 68.0 sec\n","epoch 7, train loss 1.1917, train acc 0.577, test loss 1.0073, test acc 0.645, time 68.6 sec\n","epoch 8, train loss 1.1912, train acc 0.577, test loss 0.9889, test acc 0.658, time 68.7 sec\n","epoch 9, train loss 1.1791, train acc 0.583, test loss 1.0317, test acc 0.634, time 68.3 sec\n","epoch 10, train loss 1.1808, train acc 0.581, test loss 0.9915, test acc 0.656, time 68.3 sec\n","epoch 11, train loss 1.1829, train acc 0.581, test loss 0.9969, test acc 0.649, time 69.0 sec\n","epoch 12, train loss 1.1721, train acc 0.585, test loss 0.9822, test acc 0.656, time 69.1 sec\n","epoch 13, train loss 1.1734, train acc 0.583, test loss 0.9768, test acc 0.663, time 68.6 sec\n","epoch 14, train loss 1.1737, train acc 0.584, test loss 0.9888, test acc 0.657, time 68.5 sec\n","epoch 15, train loss 1.1639, train acc 0.586, test loss 0.9821, test acc 0.656, time 69.1 sec\n","epoch 16, train loss 1.1694, train acc 0.587, test loss 0.9658, test acc 0.668, time 69.2 sec\n","epoch 17, train loss 1.1668, train acc 0.588, test loss 0.9901, test acc 0.655, time 68.8 sec\n","epoch 18, train loss 1.1602, train acc 0.589, test loss 0.9849, test acc 0.652, time 69.1 sec\n","epoch 19, train loss 1.1635, train acc 0.589, test loss 0.9851, test acc 0.663, time 69.2 sec\n","epoch 20, train loss 1.1625, train acc 0.588, test loss 0.9920, test acc 0.653, time 68.9 sec\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z72nmLWzHjJ0","colab_type":"code","outputId":"1fb2feaa-5a89-4cc3-91a7-135b47d8acc1","executionInfo":{"status":"ok","timestamp":1562441644703,"user_tz":180,"elapsed":173279,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":92}},"source":["from mxnet.gluon.model_zoo import vision\n","\n","NUM_CLASSES = 10\n","num_epochs, lr, batch_size, wd_lambda = 20, 0.001, 100, 0.0001\n","    \n","net = vision.alexnet(pretrained=True, ctx=ctx)\n","    \n","# carregamento do dado: fashion mnist\n","train_iter, test_iter = load_data_cifar10(batch_size, resize=227)  \n","\n","first = True\n","for batch in train_iter:\n","    features, labels, _ = _get_batch(batch, [ctx])\n","    for X, y in zip(features, labels):\n","        y = y.astype('float32')\n","        features = net.features(X)\n","        if first is True:\n","          train_features = features.asnumpy()\n","          train_labels = y.asnumpy()\n","          first = False\n","        else:\n","          train_features = np.concatenate((train_features, features.asnumpy()))\n","          train_labels = np.concatenate((train_labels, y.asnumpy()))\n","          \n","          \n","first = True\n","for batch in test_iter:\n","    features, labels, _ = _get_batch(batch, [ctx])\n","    for X, y in zip(features, labels):\n","        y = y.astype('float32')\n","        features = net.features(X)\n","        if first is True:\n","          test_features = features.asnumpy()\n","          test_labels = y.asnumpy()\n","          first = False\n","        else:\n","          test_features = np.concatenate((test_features, features.asnumpy()))\n","          test_labels = np.concatenate((test_labels, y.asnumpy()))\n","          \n","print(train_features.shape, test_features.shape)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading /root/.mxnet/models/alexnet-44335d1f.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/alexnet-44335d1f.zip...\n","Downloading /root/.mxnet/datasets/cifar10/cifar-10-binary.tar.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/cifar10/cifar-10-binary.tar.gz...\n","(50000, 4096) (10000, 4096)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qOn9a0bgxMK9","colab_type":"code","outputId":"c78e3fe0-7eb8-4b1f-ed69-98d3817f8d79","executionInfo":{"status":"ok","timestamp":1562442473885,"user_tz":180,"elapsed":731513,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["from sklearn.svm import LinearSVC\n","from sklearn.metrics import accuracy_score\n","\n","clf = LinearSVC()\n","clf.fit(train_features, train_labels)\n","\n","pred = clf.predict(test_features)\n","print(accuracy_score(test_labels, pred))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["0.7043\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"95OcvlZ2NA6Z","colab_type":"text"},"source":["## *Fine-tuning*\n","\n","A segunda estratégia é chamada de *fine-tuning*, e é comumente classificada como um estratégia de *transfer learning*, onde o aprendizado é transferido entre datasets.\n","Especificamente, esta estratégia, representada na figura abaixo, tenta usar um modelo pré-treinado aprendido anteriormente em algum dataset (geralmente muito grande, como o [ImageNet](http://www.image-net.org/)) para classificar outro conjunto de dados diferentes (geralmente com poucas amostras).\n","\n","<p align=\"center\">\n","  <img width=600 src=\"https://drive.google.com/uc?export=view&id=1CoOfpMcQAEl9YAL0lgW11LLYpDcnL4dQ\">\n","</p>\n","\n","Como esses dados podem possuir características diferentes, treinamos a rede usando um *learning rate* pequeno, apenas para fazer pequenos ajustes nos pesos. Entretanto, como esses datasets geralmente tem número e classes diferentes, a última camada não é usada nessa transferência de peso e, geralmente, é inicializada aleatoriamente (e por isso, tem um *learning rate* mais alto que as demais camadas).\n","\n","Por fim, é um [fato conhecido](https://arxiv.org/pdf/1602.01517.pdf) que as redes neurais conseguem aprender características de baixo nível nas camadas iniciais. Geralmente, essas características são comuns à vários datasets. Por isso, uma opção durante o processo de *fine-tuning* é \"congelar\" as camadas iniciais (ou seja, não treiná-las) e treinar somente as demais camadas com taxa de aprendizado bem pequeno (exceto pela camada de classificação).\n","\n","No bloco de código abaixo, importamos a rede pré-treinada [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), que foi treinada no dataset do [ImageNet](http://www.image-net.org/), que tem 1000 classes. Como iremos fazer *fine-tuning* nessa arquitetura para o dataset do [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html), que tem somente 10 classes, removeremos a última camada e criaremos uma nova camada inicializada aleatoriamente."]},{"cell_type":"code","metadata":{"id":"HsUjZMaUleCx","colab_type":"code","outputId":"59a11083-b452-4609-c6ae-04917ac1db1a","executionInfo":{"status":"ok","timestamp":1562424056096,"user_tz":180,"elapsed":1335884,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from mxnet.gluon.model_zoo import vision\n","\n","NUM_CLASSES = 10\n","num_epochs, lr, batch_size, wd_lambda = 20, 0.001, 100, 0.0001\n","    \n","net = vision.alexnet(pretrained=True, ctx=ctx)\n","print(net)\n","with net.name_scope():\n","    net.output = gluon.nn.Dense(NUM_CLASSES)  # substitui a ultima camada por uma nova com 10 classes\n","    net.output.initialize(init.Normal(sigma=0.01), ctx=ctx)  # inicializa essa camada aleatoriamente de acordo com uma distribuicao Normal\n","print(net)\n","\n","# este loop abaixo ajusta o multiplicador do learning rate para as camadas\n","for key, value in net.collect_params().items():\n","    print(key, value.lr_mult)\n","    if 'dense3' not in key:  # essa eh a nota camada e nao vamos mudar o multiplicador do seu learning rate\n","      value.lr_mult = 0.1\n","    print(key, value.lr_mult)\n","    \n","# função de custo (ou loss)\n","loss = gloss.SoftmaxCrossEntropyLoss()\n","\n","# carregamento do dado: fashion mnist\n","train_iter, test_iter = load_data_cifar10(batch_size, resize=227)\n","\n","# trainer do gluon\n","trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'wd': wd_lambda, 'momentum': 0.9})\n","\n","# treinamento e validação via MXNet\n","train_validate(net, train_iter, test_iter, batch_size, trainer, loss, \n","               ctx, num_epochs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["AlexNet(\n","  (features): HybridSequential(\n","    (0): Conv2D(3 -> 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2), Activation(relu))\n","    (1): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n","    (2): Conv2D(64 -> 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), Activation(relu))\n","    (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n","    (4): Conv2D(192 -> 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n","    (5): Conv2D(384 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n","    (6): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n","    (7): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n","    (8): Flatten\n","    (9): Dense(9216 -> 4096, Activation(relu))\n","    (10): Dropout(p = 0.5, axes=())\n","    (11): Dense(4096 -> 4096, Activation(relu))\n","    (12): Dropout(p = 0.5, axes=())\n","  )\n","  (output): Dense(4096 -> 1000, linear)\n",")\n","AlexNet(\n","  (features): HybridSequential(\n","    (0): Conv2D(3 -> 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2), Activation(relu))\n","    (1): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n","    (2): Conv2D(64 -> 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), Activation(relu))\n","    (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n","    (4): Conv2D(192 -> 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n","    (5): Conv2D(384 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n","    (6): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n","    (7): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n","    (8): Flatten\n","    (9): Dense(9216 -> 4096, Activation(relu))\n","    (10): Dropout(p = 0.5, axes=())\n","    (11): Dense(4096 -> 4096, Activation(relu))\n","    (12): Dropout(p = 0.5, axes=())\n","  )\n","  (output): Dense(None -> 10, linear)\n",")\n","alexnet11_conv0_weight 1.0\n","alexnet11_conv0_weight 0.1\n","alexnet11_conv0_bias 1.0\n","alexnet11_conv0_bias 0.1\n","alexnet11_conv1_weight 1.0\n","alexnet11_conv1_weight 0.1\n","alexnet11_conv1_bias 1.0\n","alexnet11_conv1_bias 0.1\n","alexnet11_conv2_weight 1.0\n","alexnet11_conv2_weight 0.1\n","alexnet11_conv2_bias 1.0\n","alexnet11_conv2_bias 0.1\n","alexnet11_conv3_weight 1.0\n","alexnet11_conv3_weight 0.1\n","alexnet11_conv3_bias 1.0\n","alexnet11_conv3_bias 0.1\n","alexnet11_conv4_weight 1.0\n","alexnet11_conv4_weight 0.1\n","alexnet11_conv4_bias 1.0\n","alexnet11_conv4_bias 0.1\n","alexnet11_dense0_weight 1.0\n","alexnet11_dense0_weight 0.1\n","alexnet11_dense0_bias 1.0\n","alexnet11_dense0_bias 0.1\n","alexnet11_dense1_weight 1.0\n","alexnet11_dense1_weight 0.1\n","alexnet11_dense1_bias 1.0\n","alexnet11_dense1_bias 0.1\n","alexnet11_dense3_weight 1.0\n","alexnet11_dense3_weight 1.0\n","alexnet11_dense3_bias 1.0\n","alexnet11_dense3_bias 1.0\n","training on gpu(0)\n","epoch 1, train loss 1.0629, train acc 0.619, test loss 0.6799, test acc 0.763, time 68.2 sec\n","epoch 2, train loss 0.7180, train acc 0.748, test loss 0.5726, test acc 0.800, time 66.7 sec\n","epoch 3, train loss 0.6208, train acc 0.782, test loss 0.5160, test acc 0.825, time 66.5 sec\n","epoch 4, train loss 0.5655, train acc 0.800, test loss 0.4776, test acc 0.839, time 66.4 sec\n","epoch 5, train loss 0.5302, train acc 0.815, test loss 0.4504, test acc 0.845, time 66.4 sec\n","epoch 6, train loss 0.5009, train acc 0.826, test loss 0.4328, test acc 0.852, time 66.3 sec\n","epoch 7, train loss 0.4746, train acc 0.834, test loss 0.4146, test acc 0.858, time 66.4 sec\n","epoch 8, train loss 0.4577, train acc 0.840, test loss 0.4084, test acc 0.858, time 66.5 sec\n","epoch 9, train loss 0.4373, train acc 0.848, test loss 0.3986, test acc 0.859, time 66.4 sec\n","epoch 10, train loss 0.4256, train acc 0.852, test loss 0.3802, test acc 0.868, time 66.4 sec\n","epoch 11, train loss 0.4116, train acc 0.855, test loss 0.3794, test acc 0.868, time 66.5 sec\n","epoch 12, train loss 0.3937, train acc 0.864, test loss 0.3744, test acc 0.871, time 66.6 sec\n","epoch 13, train loss 0.3906, train acc 0.864, test loss 0.3718, test acc 0.865, time 66.6 sec\n","epoch 14, train loss 0.3778, train acc 0.869, test loss 0.3677, test acc 0.873, time 66.5 sec\n","epoch 15, train loss 0.3673, train acc 0.871, test loss 0.3514, test acc 0.878, time 66.5 sec\n","epoch 16, train loss 0.3541, train acc 0.875, test loss 0.3556, test acc 0.878, time 66.4 sec\n","epoch 17, train loss 0.3482, train acc 0.877, test loss 0.3413, test acc 0.880, time 66.5 sec\n","epoch 18, train loss 0.3413, train acc 0.880, test loss 0.3363, test acc 0.880, time 66.7 sec\n","epoch 19, train loss 0.3370, train acc 0.883, test loss 0.3426, test acc 0.879, time 66.6 sec\n","epoch 20, train loss 0.3291, train acc 0.885, test loss 0.3322, test acc 0.886, time 66.8 sec\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"on2i3PU-zlC9","colab_type":"text"},"source":["## Prática\n","\n","1. É possível melhorar o resultado obtido anteriormente?\n","Estude o [model_zoo](https://mxnet.incubator.apache.org/api/python/gluon/model_zoo.html)  e tente usar as estratégias anteriores com diferentes redes neurais para melhorar o resultado.\n","Algumas redes possíveis:\n","\n","- [MobileNets](https://arxiv.org/abs/1801.04381)\n","- [VGGs](https://arxiv.org/abs/1409.1556)\n","- [ResNets](https://arxiv.org/abs/1603.05027)\n","- [DenseNets](https://arxiv.org/pdf/1608.06993.pdf)\n","\n","2. Procure agora congelar algumas camadas para realizar o *fine-tuning*. Essa estratégia é melhor quando se tem poucas imagens para fazer o *fine-tuning*.\n","\n","3. Procura usar outros algoritmos de aprendizado de máquina (como [*random forest*](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) e [SVM-RBF](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)) para classificar *deep features* extraídas de uma rede neural pré-treinada.\n","  1. Procure também extrair e classificar *features* de outras camadas convolucionais.\n","\n","4. Procure usar as diferentes estratégias para melhorar os resultados dos datasets que já usamos, como [MNIST](https://mxnet.incubator.apache.org/api/python/gluon/data.html#module-mxnet.gluon.data.vision) e [Fashion MNIST](https://mxnet.incubator.apache.org/api/python/gluon/data.html#mxnet.gluon.data.vision.datasets.FashionMNIST)."]}]}