{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03.3 - AlexNet-Reduced.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ed03SC1Jm9Yy","colab_type":"text"},"source":["# Prática: AlexNet com recursos limitados\n","\n","Se antes implementamos a [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) sem considerar a questão de quantidade de parâmetros, nesta prática focaremos nesse quesito.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Dx_n6rukq1RG","colab_type":"code","outputId":"f441a1e4-3fc1-4e9c-a1c1-4c2ba7d76df0","executionInfo":{"status":"ok","timestamp":1562764721816,"user_tz":180,"elapsed":7423,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":164}},"source":["!pip install mxnet-cu100\n","\n","# imports basicos\n","import time, os, sys, numpy as np\n","import mxnet as mx\n","from mxnet import autograd, gluon, init, nd\n","from mxnet.gluon import loss as gloss, nn, utils as gutils, data as gdata\n","\n","# Tenta encontrar GPU\n","def try_gpu():\n","    try:\n","        ctx = mx.gpu()\n","        _ = nd.zeros((1,), ctx=ctx)\n","    except mx.base.MXNetError:\n","        ctx = mx.cpu()\n","    return ctx\n","\n","ctx = try_gpu()\n","ctx\n","\n","## carregando dados\n","\n","# código para carregar o dataset do CIFAR 10\n","# https://www.cs.toronto.edu/~kriz/cifar.html\n","def load_data_cifar10(batch_size, resize=None, root=os.path.join(\n","        '~', '.mxnet', 'datasets', 'cifar10')):\n","    \"\"\"Download the MNIST dataset and then load into memory.\"\"\"\n","    root = os.path.expanduser(root)\n","    transformer = []\n","    if resize:\n","        transformer += [gdata.vision.transforms.Resize(resize)]\n","    transformer += [gdata.vision.transforms.ToTensor()]\n","    transformer = gdata.vision.transforms.Compose(transformer)\n","\n","    mnist_train = gdata.vision.CIFAR10(root=root, train=True)\n","    mnist_test = gdata.vision.CIFAR10(root=root, train=False)\n","    num_workers = 0 if sys.platform.startswith('win32') else 4\n","\n","    train_iter = gdata.DataLoader(mnist_train.transform_first(transformer),\n","                                  batch_size, shuffle=True,\n","                                  num_workers=num_workers)\n","    test_iter = gdata.DataLoader(mnist_test.transform_first(transformer),\n","                                 batch_size, shuffle=False,\n","                                 num_workers=num_workers)\n","    return train_iter, test_iter\n","  \n","# funções básicas\n","def _get_batch(batch, ctx):\n","    \"\"\"Return features and labels on ctx.\"\"\"\n","    features, labels = batch\n","    if labels.dtype != features.dtype:\n","        labels = labels.astype(features.dtype)\n","    return (gutils.split_and_load(features, ctx),\n","            gutils.split_and_load(labels, ctx), features.shape[0])\n","\n","# Função usada para calcular acurácia\n","def evaluate_accuracy(data_iter, net, loss, ctx=[mx.cpu()]):\n","    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n","    if isinstance(ctx, mx.Context):\n","        ctx = [ctx]\n","    acc_sum, n, l = nd.array([0]), 0, 0\n","    for batch in data_iter:\n","        features, labels, _ = _get_batch(batch, ctx)\n","        for X, y in zip(features, labels):\n","            # X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n","            y = y.astype('float32')\n","            y_hat = net(X)\n","            l += loss(y_hat, y).sum()\n","            acc_sum += (y_hat.argmax(axis=1) == y).sum().copyto(mx.cpu())\n","            n += y.size\n","        acc_sum.wait_to_read()\n","    return acc_sum.asscalar() / n, l.asscalar() / n\n","  \n","# Função usada no treinamento e validação da rede\n","def train_validate(net, train_iter, test_iter, batch_size, trainer, loss, ctx,\n","                   num_epochs):\n","    print('training on', ctx)\n","    for epoch in range(num_epochs):\n","        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n","        for X, y in train_iter:\n","            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n","            with autograd.record():\n","                y_hat = net(X)\n","                l = loss(y_hat, y).sum()\n","            l.backward()\n","            trainer.step(batch_size)\n","            y = y.astype('float32')\n","            train_l_sum += l.asscalar()\n","            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n","            n += y.size\n","        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss, ctx)\n","        print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n","              'test acc %.3f, time %.1f sec'\n","              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_loss, \n","                 test_acc, time.time() - start))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: mxnet-cu100 in /usr/local/lib/python3.6/dist-packages (1.4.1)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (0.8.4)\n","Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (2.21.0)\n","Requirement already satisfied: numpy<1.15.0,>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (1.14.6)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2019.6.16)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (3.0.4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mA3x5yGAiRDG","colab_type":"text"},"source":["## AlexNet\n","\n","Como vimos, a rede [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) foi uma das arquiteturas mais famosas dessa nova onda de rede neurais.\n","\n","<p align=\"center\">\n","  <img width=700 src=\"https://engmrk.com/wp-content/uploads/2018/10/AlexNet_Summary_Table.jpg\">\n","</p>\n","\n","Entretanto, ela possui muitos parâmetros.\n","Especificamente, essa arquitetura, para classificar 10 classes, tem um total de **58.312.736** de parâmetros como mostrado na tabela abaixo.\n","\n","**Camada** | **Calc Parâmetros** | **Total Parâmetros**\n","--- | ---: | ---:\n","Convolução 1 | 11\\*11\\*3\\*96 | 34.848\n","Convolução 2 | 5\\*5\\*96\\*256 | 614.400\n","Convolução 3 | 3\\*3\\*256\\*384 | 884.736\n","Convolução 4 | 3\\*3\\*384\\*384 | 1.327.104\n","Convolução 5 | 3\\*3\\*384\\*256 | 884.736\n","FC 6 | 9216*4096 | 37.748.736\n","FC 7 | 4096*4096 | 16.777.216\n","FC 8 | 4096*10 | 40.960\n","**Total** | | **58.312.736**\n","\n","**Seu objetivo nessa prática é propor uma nova rede neural, baseada na [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), que possuia MENOS parâmetros e alcance uma acurácia similar ou melhor que a rede original vista na aula passada.**\n","\n","Procure usar [*batch normalization*](https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.BatchNorm), camadas dilatadas e separáveis.\n","Neste caso, desconsidere os parâmetros da camada [*batch normalization*](https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.BatchNorm)."]}]}